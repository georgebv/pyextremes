{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"pyextremes","text":"<p> pyextremes </p> <p> Extreme Value Analysis (EVA) in Python </p> <p> </p> <p>pyextremes is a Python library aimed at performing univariate Extreme Value Analysis (EVA)</p>"},{"location":"#features","title":"Features","text":"<p>pyextremes provides tools necessary to perform a wide range of tasks required to perform univariate EVA, such as:</p> <ul> <li>extraction of extreme events from time series using methods such as   Block Maxima (BM) or Peaks Over Threshold (POT)</li> <li>fitting continuous distributions, such as GEVD, GPD, or user-specified   continous distributions to the extracted extreme events</li> <li>visualization of model inputs, results, and goodness-of-fit statistics</li> <li>estimation of extreme events of given probability or return period   (e.g. 100-year event) and of corresponding confidence intervals</li> <li>tools assisting with model selection and tuning, such as selection of   block size in BM and threshold in POT</li> </ul> <p>Framework provided by the pyextremes library is easy to use and requires minimum user input to get production-ready results. Its default parameters are configured in compliance with best industry standards and underlying models are heavily based in the Extreme Value theory. The largest source of inspiration for this library was the book <code>\"An Introduction to Statistical Modeling of Extreme Values\"</code> by Stuart Coles.</p> <p>In addition to the easy-to-use interface, the library provides interface to underlying tools which can be used to build custom models. All scipy continuous distributions are supported out-of-the-box. Custom distributions can be also provided by subclassing scipy.stats.rv_continuous. Any parameter of a distribution may be frozen to investigate degenerate models (e.g. <code>GEVD -&gt; Gumbel</code> or <code>GPD -&gt; Exponential</code>).</p> <p>Multiple ways of fitting the continuous distributions to the data are supported:</p> <ul> <li><code>MLE</code> (default model) - Maximum Likelihood Estimate, uses SciPy</li> <li><code>Emcee</code> - Markov Chain Monte Calro, see Emcee   package by Dan Foreman-Mackey</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Get latest version from PyPI:</p> <pre><code>pip install pyextremes\n</code></pre> <p>Install with optional dependencies:</p> <pre><code>pip install pyextremes[full]\n</code></pre> <p>Get latest experimental build from GitHub:</p> <pre><code>pip install \"git+https://github.com/georgebv/pyextremes.git#egg=pyextremes\"\n</code></pre> <p>For Anaconda Python distributions:</p> <pre><code>conda install -c conda-forge pyextremes\n</code></pre>"},{"location":"#dependencies","title":"Dependencies","text":"Package Description emcee fit models using the Markov Chain Monte Carlo method matplotlib produce figures numpy perform efficient operations with arrays pandas <code>Series</code> and <code>DataFrame</code> objects for model intput and output scipy statistical models and mathematic functions"},{"location":"#optional-dependencies","title":"Optional Dependencies","text":"<p>Optional dependencies can be installed using the <code>full</code> tag as shown above or manually:</p> Package Description tqdm progress bar for slow processes"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the MIT license.</p>"},{"location":"quickstart/","title":"Quick Start","text":""},{"location":"quickstart/#read-data","title":"Read data","text":"<p>Every <code>pyextremes</code> model starts with a <code>pandas.Series</code> (see pandas documentation) object, which contains timeseries of the data you want to analyze. This example is based on water level data for \"The Battery\" station located in New York.</p> <p>Read data:</p> <pre><code>import pandas as pd\n\nseries = pd.read_csv(\n    \"battery_wl.csv\",\n    index_col=0,\n    parse_dates=True,\n).squeeze()\n</code></pre> <p>Tip</p> <p>The <code>battery_wl.csv</code> file referenced above is used throughout many tutorials and examples for the <code>pyextremes</code> package. If you want to reproduce all steps shown here and get the same results, the file can be downloaded here.</p>"},{"location":"quickstart/#clean-up-data","title":"Clean up data","text":"<p>In order for the analysis results to be meaningful, data needs to be pre-processed by the user. This may include removal of data gaps, detrending, interpolation, removal of outliers, etc. Let's clean up the data:</p> CodeWhen printed <pre><code>series = (\n    series\n    .sort_index(ascending=True)\n    .astype(float)\n    .dropna()\n    .loc[pd.to_datetime(\"1925\"):]\n)\nseries = series - (series.index.array - pd.to_datetime(\"1992\")) / pd.to_timedelta(\"365.2425D\") * 2.87e-3\n</code></pre> <pre><code>print(series.head())\n</code></pre> <pre><code>Date-Time (GMT)\n1926-11-20 05:00:00   -0.411120\n1926-11-20 06:00:00   -0.777120\n1926-11-20 07:00:00   -1.051120\n1926-11-20 08:00:00   -1.051121\n1926-11-20 09:00:00   -0.808121\nName: Water Elevation [m NAVD88], dtype: float64\n</code></pre> Note <p>See this tutorial for more information on why these specific operations were done.</p>"},{"location":"quickstart/#create-model","title":"Create model","text":"<p>The primary interface to the pyextremes library is provided via the <code>EVA</code> class. This class is responsible for all major tasks outlined above and is created using a simple command:</p> <pre><code>from pyextremes import EVA\n\nmodel = EVA(series)\n</code></pre>"},{"location":"quickstart/#extract-extreme-values","title":"Extract extreme values","text":"<p>The first step of extreme value analysis is extraction of extreme values from the timeseries. This is done by using the <code>get_extremes</code> method of the <code>EVA</code> class.</p> <p>In this example extremes will be extracted using the BM <code>method</code> and 1-year <code>block_size</code>, which give us annual maxima series.</p> CodeWhen printed <pre><code>model.get_extremes(method=\"BM\", block_size=\"365.2425D\")\n</code></pre> <pre><code>print(model.extremes.head())\n</code></pre> <pre><code>Date-Time (GMT)\n1927-02-20 16:00:00    1.670154\n1927-12-05 10:00:00    1.432893\n1929-04-16 19:00:00    1.409977\n1930-08-23 01:00:00    1.202101\n1931-03-08 17:00:00    1.529547\nName: Water Elevation [m NAVD88], dtype: float64\n</code></pre>"},{"location":"quickstart/#visualize-extreme-events","title":"Visualize extreme events","text":"<pre><code>model.plot_extremes()\n</code></pre>"},{"location":"quickstart/#fit-a-model","title":"Fit a model","text":"<p>The next step is selecting a model and fitting to the extracted extreme events. What this means practically is that we need to find model parameters (such as shape, location and scale for GEVD or GPD) that maximize or minimize some metric (likelihood) and give us the best fit possible. This is done by calling the <code>fit_model</code> method:</p> <pre><code>model.fit_model()\n</code></pre> <p>Info</p> <p>By default, the <code>fit_model</code> method selects the best model applicable to extracted extremes using the Akaike Information Criterion (AIC).</p>"},{"location":"quickstart/#calculate-return-values","title":"Calculate return values","text":"<p>The final goal of most EVA's is estimation of return values. The simplest way to do this is by using the <code>get_summary</code> method:</p> <pre><code>summary = model.get_summary(\n    return_period=[1, 2, 5, 10, 25, 50, 100, 250, 500, 1000],\n    alpha=0.95,\n    n_samples=1000,\n)\n</code></pre> <p>Note</p> <p>By default return period size is set to one year, which is defined as the mean year from the Gregorian calendar (365.2425 days). This means that a return period of 100 corresponds to a 100-year event.</p> <p>A different return period size can be specified using the <code>return_period_size</code> argument. A value of <code>30D</code> (30 days) would mean that a return period of 12 corresponds to approximately one year.</p> <p>Print the results:</p> <pre><code>print(summary)\n</code></pre> <pre><code>            return value  lower ci  upper ci\nreturn period\n1.0                0.802610 -0.270608  1.024385\n2.0                1.409343  1.370929  1.452727\n5.0                1.622565  1.540408  1.710116\n10.0               1.803499  1.678816  1.955386\n25.0               2.090267  1.851597  2.417670\n50.0               2.354889  1.992022  2.906734\n100.0              2.671313  2.145480  3.568418\n250.0              3.188356  2.346609  4.856107\n500.0              3.671580  2.517831  6.232830\n1000.0             4.252220  2.702800  8.036243\n</code></pre>"},{"location":"quickstart/#investigate-model","title":"Investigate model","text":"<p>After model results are obtained, logical questions naturally arise - how good is the model, are the obtained results meaningful, and how confident can I be with the estimated return values. One way to do that is by visually inspecting the model:</p> <pre><code>model.plot_diagnostic(alpha=0.95)\n</code></pre>"},{"location":"quickstart/#recap","title":"Recap","text":"<p>Following this example you should be able to do the following:</p> <ul> <li>set up an <code>EVA</code> instance</li> <li>extract extreme events</li> <li>fit a model</li> <li>get results</li> </ul> <p>For more in-depth tutorials on features of pyextremes see the User Guide.</p>"},{"location":"api/eva/","title":"EVA","text":""},{"location":"api/eva/#pyextremes.eva.EVA","title":"<code>pyextremes.eva.EVA</code>","text":"<p>Extreme Value Analysis (EVA) class.</p> <p>This class brings together most of the tools available in the pyextremes package bundled together in a pipeline to perform univariate extreme value analysis.</p> <p>A typical workflow using the EVA class would consist of the following:     - extract extreme values (.get_extremes)     - fit a model (.fit_model)     - generate outputs (.get_summary)     - visualize the model (.plot_diagnostic, .plot_return_values)</p> <p>Multiple additional graphical and numerical methods are available within this class to analyze extracted extreme values, visualize them, assess goodness-of-fit of selected model, and to visualize its outputs.</p> Source code in <code>src/pyextremes/eva.py</code> <pre><code>class EVA:\n    \"\"\"\n    Extreme Value Analysis (EVA) class.\n\n    This class brings together most of the tools available in the pyextremes package\n    bundled together in a pipeline to perform univariate extreme value analysis.\n\n    A typical workflow using the EVA class would consist of the following:\n        - extract extreme values (.get_extremes)\n        - fit a model (.fit_model)\n        - generate outputs (.get_summary)\n        - visualize the model (.plot_diagnostic, .plot_return_values)\n\n    Multiple additional graphical and numerical methods are available\n    within this class to analyze extracted extreme values, visualize them,\n    assess goodness-of-fit of selected model, and to visualize its outputs.\n    \"\"\"\n\n    __slots__ = [\n        \"__data\",\n        \"__extremes\",\n        \"__extremes_method\",\n        \"__extremes_type\",\n        \"__extremes_kwargs\",\n        \"__extremes_transformer\",\n        \"__model\",\n    ]\n\n    __data: pd.Series\n    __extremes: typing.Optional[pd.Series]\n    __extremes_method: typing.Optional[typing.Literal[\"BM\", \"POT\"]]\n    __extremes_type: typing.Optional[typing.Literal[\"high\", \"low\"]]\n    __extremes_kwargs: typing.Optional[typing.Dict[str, typing.Any]]\n    __extremes_transformer: typing.Optional[ExtremesTransformer]\n    __model: typing.Optional[typing.Union[MLE, Emcee]]\n\n    def __init__(self, data: pd.Series) -&gt; None:\n        \"\"\"\n        Initialize EVA model.\n\n        Parameters\n        ----------\n        data : pandas.Series\n            Time series to be analyzed.\n            Index must be date-time and values must be numeric.\n\n        \"\"\"\n        # Ensure that `data` is pandas Series\n        if not isinstance(data, pd.Series):\n            raise TypeError(\n                f\"invalid type in '{type(data).__name__}' for the `data` argument, \"\n                f\"must be pandas.Series\"\n            )\n\n        # Copy `data` to ensure the original Series object it is not mutated\n        data = data.copy(deep=True)\n\n        # Ensure that `data` has correct index and value dtypes\n        if not np.issubdtype(data.dtype, np.number):\n            try:\n                message = \"`data` values are not numeric - converting to numeric\"\n                logger.debug(message)\n                warnings.warn(message=message, category=RuntimeWarning)\n                data = data.astype(np.float64)\n            except ValueError as _error:\n                raise TypeError(\n                    f\"invalid dtype in {data.dtype} for the `data` argument, \"\n                    f\"must be numeric (subdtype of numpy.number)\"\n                ) from _error\n        if not isinstance(data.index, pd.DatetimeIndex):\n            raise TypeError(\n                f\"index of `data` must be a sequence of date-time objects, \"\n                f\"not {data.index.inferred_type}\"\n            )\n\n        # Ensure `data` doesn't have duplicate indices\n        if (n_duplicates := len(data) - len(data.index.drop_duplicates())) &gt; 0:\n            message = (\n                f\"{n_duplicates:,d} duplicate indices found in `data` \"\n                \"- removing duplicate entries\"\n            )\n            logger.debug(message)\n            warnings.warn(message=message, category=RuntimeWarning)\n            data = data.groupby(data.index).first()\n\n        # Ensure that `data` is sorted\n        if not data.index.is_monotonic_increasing:\n            message = (\n                \"`data` index is not sorted in ascending order - \"\n                \"sorting `data` by index\"\n            )\n            logger.debug(message)\n            warnings.warn(message=message, category=RuntimeWarning)\n            data = data.sort_index(ascending=True)\n\n        # Ensure that `data` has no invalid entries\n        n_nans = data.isna().sum()\n        if n_nans &gt; 0:\n            message = (\n                f\"{n_nans:,d} Null values found in `data` - removing invalid entries\"\n            )\n            logger.debug(message)\n            warnings.warn(message=message, category=RuntimeWarning)\n            data = data.dropna()\n\n        # Set the `data` attribute\n        self.__data: pd.Series = data\n\n        # Initialize attributes related to extreme value extraction\n        self.__extremes = None\n        self.__extremes_method = None\n        self.__extremes_type = None\n        self.__extremes_kwargs = None\n        self.__extremes_transformer = None\n\n        # Initialize attributes related to model fitting\n        self.__model = None\n\n        logger.info(\"successfully initialized EVA object\")\n\n    @property\n    def data(self) -&gt; pd.Series:\n        return self.__data\n\n    @property\n    def extremes(self) -&gt; pd.Series:\n        if self.__extremes is None:\n            raise AttributeError(\n                \"extreme values must first be extracted \"\n                \"using the '.get_extremes' method\"\n            )\n        return self.__extremes\n\n    @property\n    def extremes_method(self) -&gt; typing.Literal[\"BM\", \"POT\"]:\n        if self.__extremes_method is None:\n            raise AttributeError(\n                \"extreme values must first be extracted \"\n                \"using the '.get_extremes' method\"\n            )\n        return self.__extremes_method\n\n    @property\n    def extremes_type(self) -&gt; typing.Literal[\"high\", \"low\"]:\n        if self.__extremes_type is None:\n            raise AttributeError(\n                \"extreme values must first be extracted \"\n                \"using the '.get_extremes' method\"\n            )\n        return self.__extremes_type\n\n    @property\n    def extremes_kwargs(self) -&gt; typing.Dict[str, typing.Any]:\n        if self.__extremes_kwargs is None:\n            raise AttributeError(\n                \"extreme values must first be extracted \"\n                \"using the '.get_extremes' method\"\n            )\n        return self.__extremes_kwargs\n\n    @property\n    def extremes_transformer(self) -&gt; ExtremesTransformer:\n        if self.__extremes_transformer is None:\n            raise AttributeError(\n                \"extreme values must first be extracted \"\n                \"using the '.get_extremes' method\"\n            )\n        return self.__extremes_transformer\n\n    @property\n    def model(self) -&gt; typing.Union[MLE, Emcee]:\n        if self.__model is None:\n            raise AttributeError(\n                \"model must first be assigned using the '.fit_model' method\"\n            )\n        return self.__model\n\n    @property\n    def distribution(self) -&gt; Distribution:\n        return self.model.distribution\n\n    @property\n    def loglikelihood(self) -&gt; float:\n        return self.model.loglikelihood\n\n    @property\n    def AIC(self) -&gt; float:\n        return self.model.AIC\n\n    def test_ks(self, significance_level: float = 0.05) -&gt; KolmogorovSmirnov:\n        return KolmogorovSmirnov(\n            extremes=self.extremes_transformer.transformed_extremes,\n            distribution=self.distribution.distribution,\n            fit_parameters={\n                **self.model.fit_parameters,\n                **self.model.distribution._fixed_parameters,\n            },\n            significance_level=significance_level,\n        )\n\n    def __repr__(self) -&gt; str:\n        # Width of repr block\n        width = 88\n\n        # Separator used to separate two columns of the repr block\n        sep = \" \" * 6\n\n        # Widths of left and right columns\n        lwidth = (width - len(sep)) // 2\n        rwidth = width - (lwidth + len(sep))\n\n        # Function used to convert label-value pair\n        # into a sequence of lines within a column\n        def align_text(label: str, value: str, position: str) -&gt; typing.List[str]:\n            assert position in [\"left\", \"right\"]\n            if label == \"\":\n                if position == \"left\":\n                    return [f\"{value:&gt;{lwidth}}\"]\n                return [f\"{value:&gt;{rwidth}}\"]\n\n            # Find width available for the value\n            # (+2 stands for colon and space (label: value))\n            label_width = len(label) + 2\n            if position == \"left\":\n                free_width = lwidth - label_width\n            else:\n                free_width = rwidth - label_width\n\n            # Split value into chunks using 'free_width'\n            value_chunks = [\n                value[i : i + free_width] for i in range(0, len(value), free_width)\n            ]\n\n            # Collect text row-by-row using 'value_chunks'\n            aligned_text = [f\"{label}: {value_chunks[0]:&gt;{free_width}}\"]\n            try:\n                for chunk in value_chunks[1:]:\n                    aligned_text.append(\n                        \"\".join(\n                            [\n                                \" \" * label_width,\n                                f\"{chunk:&gt;{free_width}}\",\n                            ]\n                        )\n                    )\n            except IndexError:\n                pass\n            return aligned_text\n\n        # Function used to convert two label-value pairs\n        # into a sequence of rows representing two columns\n        def align_pair(\n            label: typing.Tuple[str, str],\n            value: typing.Tuple[str, str],\n        ) -&gt; str:\n            parts = [\n                align_text(lbl, val, pos)\n                for lbl, val, pos in zip(label, value, (\"left\", \"right\"))\n            ]\n            while len(parts[0]) != len(parts[1]):\n                shorter_part_index = 0 if len(parts[0]) &lt; len(parts[1]) else 1\n                parts[shorter_part_index].append(\n                    \" \" * len(parts[shorter_part_index][0])\n                )\n            return \"\\n\".join([sep.join([left, right]) for left, right in zip(*parts)])\n\n        # Create summary header\n        start_date = (\n            f\"{calendar.month_name[self.data.index[0].month]} \"\n            f\"{self.data.index[0].year}\"\n        )\n        end_date = (\n            f\"{calendar.month_name[self.data.index[-1].month]} \"\n            f\"{self.data.index[-1].year}\"\n        )\n        summary = [\n            \"Univariate Extreme Value Analysis\".center(width),\n            \"=\" * width,\n            \"Source Data\".center(width),\n            \"-\" * width,\n            align_pair(\n                (\"Data label\", \"Size\"),\n                (str(self.data.name), f\"{len(self.data):,d}\"),\n            ),\n            align_pair(\n                (\"Start\", \"End\"),\n                (start_date, end_date),\n            ),\n            \"=\" * width,\n        ]\n\n        # Fill the extremes section\n        summary.extend(\n            [\n                \"Extreme Values\".center(width),\n                \"-\" * width,\n            ]\n        )\n        try:\n            if self.extremes_method == \"BM\":\n                ev_parameters = (\n                    \"Block size\",\n                    str(self.extremes_kwargs[\"block_size\"]),\n                )\n            elif self.extremes_method == \"POT\":\n                ev_parameters = (\n                    \"Threshold\",\n                    str(self.extremes_kwargs[\"threshold\"]),\n                )\n            else:\n                raise AssertionError\n            summary.extend(\n                [\n                    align_pair(\n                        (\"Count\", \"Extraction method\"),\n                        (f\"{len(self.extremes):,d}\", self.extremes_method),\n                    ),\n                    align_pair(\n                        (\"Type\", ev_parameters[0]),\n                        (self.extremes_type, ev_parameters[1]),\n                    ),\n                ]\n            )\n        except AttributeError:\n            summary.append(\"Extreme values have not been extracted\")\n        summary.append(\"=\" * width)\n\n        # Fill the model section\n        summary.extend(\n            [\n                \"Model\".center(width),\n                \"-\" * width,\n            ]\n        )\n        try:\n            summary.append(\n                align_pair(\n                    (\"Model\", \"Distribution\"),\n                    (self.model.name, self.model.distribution.name),\n                )\n            )\n            if self.model.name == \"Emcee\":\n                n_walkers = getattr(self.model, \"n_walkers\")\n                n_samples = getattr(self.model, \"n_samples\")\n                summary.append(\n                    align_pair(\n                        (\"Walkers\", \"Samples per walker\"),\n                        (f\"{n_walkers:,d}\", f\"{n_samples:,d}\"),\n                    )\n                )\n\n            summary.append(\n                align_pair(\n                    (\"Log-likelihood\", \"AIC\"),\n                    (f\"{self.model.loglikelihood:.3f}\", f\"{self.model.AIC:.3f}\"),\n                )\n            )\n\n            summary.append(\"-\" * width)\n\n            free_parameters = [\n                f\"{parameter}={self.model.fit_parameters[parameter]:.3f}\"\n                for parameter in self.model.distribution.free_parameters\n            ]\n            fixed_parameters = [\n                f\"{key}={value:.3f}\"\n                for key, value in self.model.distribution.fixed_parameters.items()\n            ]\n            if len(fixed_parameters) == 0:\n                fixed_parameters = [\"All parameters are free\"]\n            delta_parameters = len(free_parameters) - len(fixed_parameters)\n            if delta_parameters &lt; 0:\n                for _ in range(-delta_parameters):\n                    free_parameters.append(\"\")\n            else:\n                for _ in range(delta_parameters):\n                    fixed_parameters.append(\"\")\n\n            for i, (frp, fip) in enumerate(zip(free_parameters, fixed_parameters)):\n                if i == 0:\n                    summary.append(\n                        align_pair(\n                            (\"Free parameters\", \"Fixed parameters\"),\n                            (frp, fip),\n                        )\n                    )\n                else:\n                    summary.append(\n                        align_pair(\n                            (\"\", \"\"),\n                            (frp, fip),\n                        )\n                    )\n\n        except AttributeError:\n            summary.append(\"Model has not been fit to the extremes\")\n\n        summary.append(\"=\" * width)\n\n        return \"\\n\".join(summary)\n\n    @typing.overload\n    def get_extremes(\n        self,\n        method: typing.Literal[\"BM\"],\n        extremes_type: typing.Literal[\"high\", \"low\"] = \"high\",\n        *,\n        block_size: str = \"365.2425D\",\n        errors: typing.Literal[\"raise\", \"ignore\", \"coerce\"] = \"raise\",\n        min_last_block: typing.Optional[float] = None,\n    ) -&gt; None:\n        ...\n\n    @typing.overload\n    def get_extremes(\n        self,\n        method: typing.Literal[\"POT\"],\n        extremes_type: typing.Literal[\"high\", \"low\"] = \"high\",\n        *,\n        threshold: float,\n        r: typing.Union[pd.Timedelta, typing.Any] = \"24h\",\n    ) -&gt; None:\n        ...\n\n    def get_extremes(\n        self,\n        method: typing.Literal[\"BM\", \"POT\"],\n        extremes_type: typing.Literal[\"high\", \"low\"] = \"high\",\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Get extreme events from time series.\n\n        Extracts extreme values from the 'self.data' attribute.\n        Stores extreme values in the 'self.extremes' attribute.\n\n        Parameters\n        ----------\n        method : str\n            Extreme value extraction method.\n            Supported values:\n                BM - Block Maxima\n                POT - Peaks Over Threshold\n        extremes_type : str, optional\n            high (default) - get extreme high values\n            low - get extreme low values\n        kwargs\n            if method is BM:\n                block_size : str or pandas.Timedelta, optional\n                    Block size (default='365.2425D').\n                    See pandas.to_timedelta for more information.\n                errors : str, optional\n                    raise (default) - raise an exception\n                        when encountering a block with no data\n                    ignore - ignore blocks with no data\n                    coerce - get extreme values for blocks with no data\n                        as mean of all other extreme events in the series\n                        with index being the middle point of corresponding interval\n                min_last_block : float, optional\n                    Minimum data availability ratio (0 to 1) in the last block\n                    for it to be used to extract extreme value from.\n                    This is used to discard last block when it is too short.\n                    If None (default), last block is always used.\n            if method is POT:\n                threshold : float\n                    Threshold used to find exceedances.\n                r : pandas.Timedelta or value convertible to timedelta, optional\n                    Duration of window used to decluster the exceedances.\n                    By default r='24H' (24 hours).\n                    See pandas.to_timedelta for more information.\n\n        \"\"\"\n        message = f\"for method='{method}' and extremes_type='{extremes_type}'\"\n        logger.debug(\"extracting extreme values %s\", message)\n        self.__extremes = get_extremes(\n            method=method,\n            ts=self.data,\n            extremes_type=extremes_type,\n            **kwargs,\n        )\n        self.__extremes_method = method\n        self.__extremes_type = extremes_type\n        logger.info(\"successfully extracted extreme values %s\", message)\n\n        logger.debug(\"collecting extreme value properties\")\n        self.__extremes_kwargs = {}\n        if method == \"BM\":\n            self.__extremes_kwargs[\"block_size\"] = pd.to_timedelta(\n                kwargs.get(\"block_size\", \"365.2425D\")\n            )\n            self.__extremes_kwargs[\"errors\"] = kwargs.get(\"errors\", \"raise\")\n            self.__extremes_kwargs[\"min_last_block\"] = kwargs.get(\n                \"min_last_block\", None\n            )\n        else:\n            self.__extremes_kwargs[\"threshold\"] = kwargs.get(\"threshold\")\n            self.__extremes_kwargs[\"r\"] = pd.to_timedelta(kwargs.get(\"r\", \"24h\"))\n        logger.info(\"successfully collected extreme value properties\")\n\n        logger.debug(\"creating extremes transformer\")\n        self.__extremes_transformer = ExtremesTransformer(\n            extremes=self.__extremes,\n            extremes_type=self.__extremes_type,\n        )\n        logger.info(\"successfully created extremes transformer\")\n\n        logger.info(\"removing any previously declared models\")\n        self.__model = None\n\n    @typing.overload\n    def set_extremes(\n        self,\n        extremes: pd.Series,\n        method: typing.Literal[\"BM\"] = \"BM\",\n        extremes_type: typing.Literal[\"high\", \"low\"] = \"high\",\n        *,\n        block_size: str = \"365.2425D\",\n        errors: typing.Literal[\"raise\", \"ignore\", \"coerce\"] = \"raise\",\n        min_last_block: typing.Optional[float] = None,\n    ) -&gt; None:\n        ...\n\n    @typing.overload\n    def set_extremes(\n        self,\n        extremes: pd.Series,\n        method: typing.Literal[\"POT\"] = \"POT\",\n        extremes_type: typing.Literal[\"high\", \"low\"] = \"high\",\n        *,\n        threshold: float,\n        r: typing.Union[pd.Timedelta, typing.Any] = \"24h\",\n    ) -&gt; None:\n        ...\n\n    def set_extremes(\n        self,\n        extremes: pd.Series,\n        method: typing.Literal[\"BM\", \"POT\"] = \"BM\",\n        extremes_type: typing.Literal[\"high\", \"low\"] = \"high\",\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Set extreme values.\n\n        This method is used to set extreme values onto the model instead\n        of deriving them from data directly using the 'get_extremes' method.\n        This way user can set extremes calculated using a custom methodology.\n\n        Parameters\n        ----------\n        extremes : pd.Series\n            Time series of extreme values to be set onto the model.\n            Must be numeric, have date-time index, and have the same name\n            as self.data.\n        method : str, optional\n            Extreme value extraction method.\n            Supported values:\n                BM (default) - Block Maxima\n                POT - Peaks Over Threshold\n        extremes_type : str, optional\n            high (default) - extreme high values\n            low - extreme low values\n        kwargs:\n            if method is BM:\n                block_size : str or pandas.Timedelta, optional\n                    Block size.\n                    If None (default), then is calculated as median distance\n                    between extreme events.\n                errors : str, optional\n                    raise - raise an exception\n                        when encountering a block with no data\n                    ignore (default) - ignore blocks with no data\n                    coerce - get extreme values for blocks with no data\n                        as mean of all other extreme events in the series\n                        with index being the middle point of corresponding interval\n                min_last_block : float, optional\n                    Minimum data availability ratio (0 to 1) in the last block\n                    for it to be used to extract extreme value from.\n                    This is used to discard last block when it is too short.\n                    If None (default), last block is always used.\n            if method is POT:\n                threshold : float, optional\n                    Threshold used to find exceedances.\n                    By default is taken as smallest value.\n                r : pandas.Timedelta or value convertible to timedelta, optional\n                    Duration of window used to decluster the exceedances.\n                    By default r='24H' (24 hours).\n                    See pandas.to_timedelta for more information.\n\n        \"\"\"\n        # Validate `extremes`\n        if not isinstance(extremes, pd.Series):\n            raise TypeError(\n                f\"invalid type in '{type(extremes).__name__}' for the `extremes` \"\n                f\"argument, must be pandas.Series\"\n            )\n        extremes = extremes.copy(deep=True)\n        if not isinstance(extremes.index, pd.DatetimeIndex):\n            raise TypeError(\"invalid index type for `extremes`, must be date-time\")\n        if not np.issubdtype(extremes.dtype, np.number):\n            raise TypeError(\"`extremes` must have numeric values\")\n        if extremes.name is None:\n            extremes.name = self.data.name\n        else:\n            if extremes.name != self.data.name:\n                raise ValueError(\"`extremes` name doesn't match that of `data`\")\n        if (\n            extremes.index.min() &lt; self.data.index.min()\n            or extremes.index.max() &gt; self.data.index.max()\n        ):\n            raise ValueError(\"`extremes` time range must fit within that of data\")\n\n        # Get `method`\n        if method not in [\"BM\", \"POT\"]:\n            raise ValueError(f\"`method` must be either 'BM' or 'POT', not '{method}'\")\n\n        # Get `extremes_type`\n        if extremes_type not in [\"high\", \"low\"]:\n            raise ValueError(\n                f\"`extremes_type` must be either 'BM' or 'POT', not '{extremes_type}'\"\n            )\n\n        # Get `extremes_kwargs`\n        extremes_kwargs = {}\n        if method == \"BM\":\n            # Get `block_size`\n            extremes_kwargs[\"block_size\"] = pd.to_timedelta(\n                kwargs.pop(\n                    \"block_size\",\n                    pd.to_timedelta(np.quantile(np.diff(extremes.index), 0.5)),\n                )\n            )\n            if extremes_kwargs[\"block_size\"] &lt;= pd.to_timedelta(\"0D\"):\n                raise ValueError(\n                    \"`block_size` must be a positive timedelta, not %s\"\n                    % extremes_kwargs[\"block_size\"]\n                )\n\n            # Get `errors`\n            extremes_kwargs[\"errors\"] = kwargs.pop(\"errors\", \"ignore\")\n            if extremes_kwargs[\"errors\"] not in [\"raise\", \"ignore\", \"coerce\"]:\n                raise ValueError(\n                    f\"invalid value in '{extremes_kwargs['errors']}' \"\n                    f\"for the `errors` argument\"\n                )\n\n            # Get `min_last_block`\n            extremes_kwargs[\"min_last_block\"] = kwargs.pop(\"min_last_block\", None)\n            if extremes_kwargs[\"min_last_block\"] is not None:\n                if not 0 &lt;= extremes_kwargs[\"min_last_block\"] &lt;= 1:\n                    raise ValueError(\n                        \"`min_last_block` must be a number in the [0, 1] range\"\n                    )\n\n        else:\n            # Get `threshold`\n            extremes_kwargs[\"threshold\"] = kwargs.pop(\n                \"threshold\",\n                {\n                    \"high\": extremes.min(),\n                    \"low\": extremes.max(),\n                }[extremes_type],\n            )\n            if (\n                extremes_type == \"high\"\n                and extremes_kwargs[\"threshold\"] &gt; extremes.values.min()\n            ) or (\n                extremes_type == \"low\"\n                and extremes_kwargs[\"threshold\"] &lt; extremes.values.max()\n            ):\n                raise ValueError(\"invalid `threshold` value\")\n\n            # Get `r`\n            extremes_kwargs[\"r\"] = pd.to_timedelta(kwargs.pop(\"r\", \"24h\"))\n            if extremes_kwargs[\"r\"] &lt;= pd.to_timedelta(\"0D\"):\n                raise ValueError(\n                    \"`r` must be a positive timedelta, not %s\" % extremes_kwargs[\"r\"]\n                )\n\n        # Check for unrecognized kwargs\n        if len(kwargs) != 0:\n            raise TypeError(\n                f\"unrecognized arguments passed in: {', '.join(kwargs.keys())}\"\n            )\n\n        # Set attributes\n        self.__extremes = extremes\n        self.__extremes_method = method\n        self.__extremes_type = extremes_type\n        self.__extremes_kwargs = extremes_kwargs\n        self.__extremes_transformer = ExtremesTransformer(\n            extremes=self.__extremes,\n            extremes_type=self.__extremes_type,\n        )\n        self.__model = None\n        logger.info(\"successfully set extremes\")\n\n    @typing.overload\n    @classmethod\n    def from_extremes(\n        cls,\n        extremes: pd.Series,\n        method: typing.Literal[\"BM\"] = \"BM\",\n        extremes_type: typing.Literal[\"high\", \"low\"] = \"high\",\n        *,\n        block_size: str = \"365.2425D\",\n        errors: typing.Literal[\"raise\", \"ignore\", \"coerce\"] = \"raise\",\n        min_last_block: typing.Optional[float] = None,\n    ) -&gt; None:\n        ...\n\n    @typing.overload\n    @classmethod\n    def from_extremes(\n        cls,\n        extremes: pd.Series,\n        method: typing.Literal[\"POT\"] = \"POT\",\n        extremes_type: typing.Literal[\"high\", \"low\"] = \"high\",\n        *,\n        threshold: float,\n        r: typing.Union[pd.Timedelta, typing.Any] = \"24h\",\n    ) -&gt; None:\n        ...\n\n    @classmethod\n    def from_extremes(\n        cls,\n        extremes: pd.Series,\n        method: typing.Literal[\"BM\", \"POT\"] = \"BM\",\n        extremes_type: typing.Literal[\"high\", \"low\"] = \"high\",\n        **kwargs,\n    ) -&gt; EVA:\n        \"\"\"\n        Create an EVA model using pre-defined `extremes`.\n\n        A typical reason to use this method is when full timeseries is not available\n        and only the extracted extremes (i.e. annual maxima) are known.\n\n        Parameters\n        ----------\n        extremes : pd.Series\n            Time series of extreme values.\n        method : str, optional\n            Extreme value extraction method.\n            Supported values:\n                BM (default) - Block Maxima\n                POT - Peaks Over Threshold\n        extremes_type : str, optional\n            high (default) - extreme high values\n            low - extreme low values\n        kwargs:\n            if method is BM:\n                block_size : str or pandas.Timedelta, optional\n                    Block size.\n                    If None (default), then is calculated as median distance\n                    between extreme events.\n                errors : str, optional\n                    raise - raise an exception\n                        when encountering a block with no data\n                    ignore (default) - ignore blocks with no data\n                    coerce - get extreme values for blocks with no data\n                        as mean of all other extreme events in the series\n                        with index being the middle point of corresponding interval\n                min_last_block : float, optional\n                    Minimum data availability ratio (0 to 1) in the last block\n                    for it to be used to extract extreme value from.\n                    This is used to discard last block when it is too short.\n                    If None (default), last block is always used.\n            if method is POT:\n                threshold : float, optional\n                    Threshold used to find exceedances.\n                    By default is taken as smallest value.\n                r : pandas.Timedelta or value convertible to timedelta, optional\n                    Duration of window used to decluster the exceedances.\n                    By default r='24H' (24 hours).\n                    See pandas.to_timedelta for more information.\n\n        Returns\n        -------\n        EVA\n            EVA model initialized with `extremes`.\n\n        \"\"\"\n        model = cls(data=extremes)\n        model.set_extremes(\n            extremes=model.data,\n            method=method,\n            extremes_type=extremes_type,\n            **kwargs,\n        )\n        return model\n\n    def plot_extremes(\n        self,\n        figsize: tuple = (8, 5),\n        ax: typing.Optional[plt.Axes] = None,\n        show_clusters: bool = False,\n    ) -&gt; typing.Tuple[plt.Figure, plt.Axes]:  # pragma: no cover\n        \"\"\"\n        Plot extreme events.\n\n        Parameters\n        ----------\n        figsize : tuple, optional\n            Figure size in inches in format (width, height).\n            By default it is (8, 5).\n        ax : matplotlib.axes._axes.Axes, optional\n            Axes onto which extremes plot is drawn.\n            If None (default), a new figure and axes objects are created.\n        show_clusters : bool, optional\n            If True, show cluster boundaries for POT extremes.\n            Has no effect if extremes were extracted using BM method.\n            May produce wrong cluster boundaries if extremes were set using the\n            `set_extremes` or `from_extremes` methods and threshold and inter-cluster\n            distance (r) arguments were not provided.\n            By default is False.\n\n        Returns\n        -------\n        figure : matplotlib.figure.Figure\n            Figure object.\n        axes : matplotlib.axes._axes.Axes\n            Axes object.\n\n        \"\"\"\n        return plot_extremes(\n            ts=self.data,\n            extremes=self.extremes,\n            extremes_method=self.extremes_method,\n            extremes_type=self.extremes_type,\n            block_size=self.extremes_kwargs.get(\"block_size\", None),\n            threshold=self.extremes_kwargs.get(\"threshold\", None),\n            r=self.extremes_kwargs.get(\"r\", None) if show_clusters else None,\n            figsize=figsize,\n            ax=ax,\n        )\n\n    @typing.overload\n    def fit_model(\n        self,\n        model: typing.Literal[\"MLE\"] = \"MLE\",\n        distribution: typing.Union[str, scipy.stats.rv_continuous] = None,\n        distribution_kwargs: typing.Optional[dict] = None,\n    ) -&gt; None:\n        ...\n\n    @typing.overload\n    def fit_model(\n        self,\n        model: typing.Literal[\"Emcee\"] = \"Emcee\",\n        distribution: typing.Union[str, scipy.stats.rv_continuous] = None,\n        distribution_kwargs: typing.Optional[dict] = None,\n        n_walkers: int = 100,\n        n_samples: int = 500,\n        progress: bool = False,\n    ) -&gt; None:\n        ...\n\n    def fit_model(\n        self,\n        model: typing.Literal[\"MLE\", \"Emcee\"] = \"MLE\",\n        distribution: typing.Union[str, scipy.stats.rv_continuous] = None,\n        distribution_kwargs: typing.Optional[dict] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Fit a model to the extracted extreme values.\n\n        Parameters\n        ----------\n        model : str, optional\n            Name of model. By default it is 'MLE'.\n            Name of model.\n            Supported models:\n                MLE - Maximum Likelihood Estimate (MLE) model.\n                    Based on 'scipy' package (scipy.stats.rv_continuous.fit).\n                Emcee - Markov Chain Monte Carlo (MCMC) model.\n                    Based on 'emcee' package by Daniel Foreman-Mackey.\n        distribution : str or scipy.stats.rv_continuous, optional\n            Distribution name compatible with scipy.stats\n            or a subclass of scipy.stats.rv_continuous.\n            See https://docs.scipy.org/doc/scipy/reference/stats.html\n            By default the distribution is selected automatically\n            as best between 'genextreme' and 'gumbel_r' for 'BM' extremes\n            and 'genpareto' and 'expon' for 'POT' extremes.\n            Best distribution is selected using the AIC metric.\n        distribution_kwargs : dict, optional\n            Special keyword arguments, passed to the `.fit` method of the distribution.\n            These keyword arguments represent parameters to be held fixed.\n            Names of parameters to be fixed must have 'f' prefixes. Valid parameters:\n                - shape(s): 'fc', e.g. fc=0\n                - location: 'floc', e.g. floc=0\n                - scale: 'fscale', e.g. fscale=1\n            See documentation of a specific scipy.stats distribution\n            for names of available parameters.\n            By default, location parameter for 'genpareto' and 'expon' distributions\n            is fixed to threshold (POT) or to minimum extremes (BM) value.\n            Set to empty dictionary (distribution_kwargs={}) to avoid this behaviour.\n        kwargs\n            Keyword arguments passed to a model .fit method.\n            MLE model:\n                MLE model takes no additional arguments.\n            Emcee model:\n                n_walkers : int, optional\n                    The number of walkers in the ensemble (default=100).\n                n_samples : int, optional\n                    The number of steps to run (default=500).\n                progress : bool or str, optional\n                    If True, a progress bar will be shown as the sampler progresses.\n                    If a string, will select a specific tqdm progress bar.\n                    Most notable is 'notebook', which shows a progress bar\n                    suitable for Jupyter notebooks.\n                    If False (default), no progress bar will be shown.\n                    This progress bar is a part of the `emcee` package.\n\n        \"\"\"\n        # Select default distribution\n        if distribution is None:\n            logger.debug(\n                \"selecting default distribution for extremes extracted using the \"\n                \"'%s' method\",\n                self.extremes_method,\n            )\n\n            # Prepare list of candidate distributions\n            if self.extremes_method == \"BM\":\n                candidate_distributions = [\"genextreme\", \"gumbel_r\"]\n                _distribution_kwargs = None\n            elif self.extremes_method == \"POT\":\n                candidate_distributions = [\"genpareto\", \"expon\"]\n                _distribution_kwargs = {\n                    \"floc\": self.extremes_kwargs.get(\n                        \"threshold\",\n                        self.extremes_transformer.transformed_extremes.min(),\n                    )\n                }\n            else:\n                raise AssertionError\n\n            # Fit MLE model for candidate distributions\n            # and select distribution with smallest AIC\n            distribution = None\n            aic = np.inf\n            for distribution_name in candidate_distributions:\n                new_aic = MLE(\n                    extremes=self.extremes_transformer.transformed_extremes,\n                    distribution=distribution_name,\n                    distribution_kwargs=_distribution_kwargs,\n                ).AIC\n                if new_aic &lt; aic:\n                    distribution = distribution_name\n                    aic = new_aic\n            logger.info(\n                \"selected '%s' distribution with AIC score %s\",\n                distribution,\n                aic,\n            )\n\n        # Get distribution name\n        if isinstance(distribution, str):\n            distribution_name = distribution\n        elif isinstance(distribution, scipy.stats.rv_continuous):\n            distribution_name = getattr(distribution, \"name\", None)\n        else:\n            raise TypeError(\n                f\"invalid type in {type(distribution)} \"\n                f\"for the 'distribution' argument, \"\n                f\"must be string or scipy.stats.rv_continuous\"\n            )\n\n        # Checking if distribution is valid per extreme value theory:\n        # Fisher-Tippet-Gnedenko theorem for 'BM'\n        # Pickands\u2013Balkema\u2013de Haan theorem for 'POT'\n        if distribution_name is None:\n            warnings.warn(\n                message=(\n                    \"provided distribution 'name' attribute cannot be resolved \"\n                    \"and distribution validity cannot be verified\"\n                ),\n                category=RuntimeWarning,\n            )\n        else:\n            if self.extremes_method == \"BM\" and distribution_name not in [\n                \"genextreme\",\n                \"gumbel_r\",\n            ]:\n                warnings.warn(\n                    message=(\n                        f\"'{distribution_name}' distribution is not \"\n                        f\"recommended to be used with extremes extracted \"\n                        f\"using the 'BM' method, 'genextreme' or 'gumebel_r' \"\n                        f\"should be used per the Fisher-Tippet-Gnedenko theorem\"\n                    ),\n                    category=RuntimeWarning,\n                )\n            elif self.extremes_method == \"POT\" and distribution_name not in [\n                \"genpareto\",\n                \"expon\",\n            ]:\n                warnings.warn(\n                    message=(\n                        f\"'{distribution_name}' distribution is not \"\n                        f\"recommended to be used with extremes extracted \"\n                        f\"using the 'POT' method, 'genpareto' or 'expon' \"\n                        f\"should be used per the Pickands\u2013Balkema\u2013de Haan theorem\"\n                    ),\n                    category=RuntimeWarning,\n                )\n\n        # Freeze (fix) location parameter for genpareto/expon distributions\n        if distribution_kwargs is None and distribution_name in [\"genpareto\", \"expon\"]:\n            distribution_kwargs = {\n                \"floc\": self.extremes_kwargs.get(\n                    \"threshold\", self.extremes_transformer.transformed_extremes.min()\n                )\n            }\n            logger.debug(\n                \"freezing location parameter (floc) at %s for '%s' distribution\",\n                distribution_kwargs[\"floc\"],\n                distribution_name,\n            )\n\n        # Fit model to transformed extremes\n        self.__model = get_model(\n            model=model,\n            extremes=self.extremes_transformer.transformed_extremes,\n            distribution=distribution,\n            distribution_kwargs=distribution_kwargs,\n            **kwargs,\n        )\n\n    def _get_mcmc_plot_inputs(\n        self,\n        labels: typing.Optional[typing.List[str]] = None,\n    ) -&gt; tuple:  # pragma: no cover\n        try:\n            trace = self.model.trace\n            trace_map = tuple(\n                self.model.fit_parameters[parameter]\n                for parameter in self.model.distribution.free_parameters\n            )\n        except TypeError as _error:\n            raise TypeError(\n                f\"this method is only applicable to MCMC-like models, \"\n                f\"not to '{self.model.name}' model\"\n            ) from _error\n\n        parameter_names = {\n            \"loc\": r\"Location, $\\mu$\",\n            \"scale\": r\"Scale, $\\sigma$\",\n        }\n        if self.model.distribution.name in [\"genextreme\", \"genpareto\"]:\n            parameter_names[\"c\"] = r\"Shape, $\\xi$\"\n        if labels is None:\n            labels = []\n            for parameter in self.model.distribution.free_parameters:\n                try:\n                    labels.append(parameter_names[parameter])\n                except KeyError:\n                    labels.append(f\"Shape parameter '{parameter}'\")\n\n        return trace, trace_map, labels\n\n    def plot_trace(\n        self,\n        burn_in: int = 0,\n        labels: typing.Optional[typing.List[str]] = None,\n        figsize: typing.Optional[typing.Tuple[float, float]] = None,\n    ) -&gt; typing.Tuple[plt.Figure, list]:  # pragma: no cover\n        \"\"\"\n        Plot trace plot for MCMC sampler trace.\n\n        Parameters\n        ----------\n        burn_in : int, optional\n            Burn-in value (number of first steps to discard for each walker).\n            By default it is 0 (no values are discarded).\n        labels : array-like, optional\n            Sequence of strings with parameter names, used to label axes.\n            If None (default), then axes are labeled sequentially.\n        figsize : tuple, optional\n            Figure size in inches.\n            If None (default), then figure size is calculated automatically\n            as 8 by 2 times number of parameters.\n\n        Returns\n        -------\n        figure : matplotlib.figure.Figure\n            Figure object.\n        axes : list\n            List with n_parameters Axes objects.\n\n        \"\"\"\n        trace, trace_map, labels = self._get_mcmc_plot_inputs(labels=labels)\n        return plot_trace(\n            trace=trace,\n            trace_map=trace_map,\n            burn_in=burn_in,\n            labels=labels,\n            figsize=figsize,\n        )\n\n    def plot_corner(\n        self,\n        burn_in: int = 0,\n        labels: typing.Optional[typing.List[str]] = None,\n        levels: typing.Optional[int] = None,\n        figsize: typing.Tuple[float, float] = (8, 8),\n    ) -&gt; typing.Tuple[plt.Figure, list]:  # pragma: no cover\n        \"\"\"\n        Plot corner plot for MCMC sampler trace.\n\n        Parameters\n        ----------\n        burn_in : int, optional\n            Burn-in value (number of first steps to discard for each walker).\n            By default it is 0 (no values are discarded).\n        labels : array-like, optional\n            Sequence of strings with parameter names, used to label axes.\n            If None (default), then axes are labeled sequentially.\n        levels : int, optional\n            Number of Gaussian KDE contours to plot.\n            If None (default), then not shown.\n        figsize : tuple, optional\n            Figure size in inches. By default it is (8, 8).\n\n        Returns\n        -------\n        figure : matplotlib.figure.Figure\n            Figure object.\n        axes : list\n            2D list with Axes objects of size N by N, where N is `trace.shape[2]`.\n            Empty slots are represented by None. Axes are ordered from left to right\n            top to bottom.\n\n        \"\"\"\n        trace, trace_map, labels = self._get_mcmc_plot_inputs(labels=labels)\n        return plot_corner(\n            trace=trace,\n            trace_map=trace_map,\n            burn_in=burn_in,\n            labels=labels,\n            levels=levels,\n            figsize=figsize,\n        )\n\n    def get_return_value(\n        self,\n        return_period,\n        return_period_size: typing.Union[str, pd.Timedelta] = \"365.2425D\",\n        alpha: typing.Optional[float] = None,\n        **kwargs,\n    ) -&gt; tuple:\n        \"\"\"\n        Get return value and confidence interval for given return period(s).\n\n        Parameters\n        ----------\n        return_period : array-like\n            Return period or 1D array of return periods.\n            Given as a multiple of `return_period_size`.\n        return_period_size : str or pandas.Timedelta, optional\n            Size of return periods (default='365.2425D').\n            If set to '30D', then a return period of 12\n            would be roughly equivalent to a 1 year return period (360 days).\n        alpha : float, optional\n            Width of confidence interval (0, 1).\n            If None (default), return None\n            for upper and lower confidence interval bounds.\n        kwargs\n            Model-specific keyword arguments.\n            If alpha is None, keyword arguments are ignored\n            (error still raised for unrecognized arguments).\n            MLE model:\n                n_samples : int, optional\n                    Number of bootstrap samples used to estimate\n                    confidence interval bounds (default=100).\n            Emcee model:\n                burn_in : int\n                    Burn-in value (number of first steps to discard for each walker).\n\n        Returns\n        -------\n        return_value : array-like\n            Return values.\n        ci_lower : array-like\n            Lower confidence interval bounds.\n        ci_upper : array-like\n            Upper confidence interval bounds.\n\n        \"\"\"\n        # Parse the 'return_period_size' argument\n        if not isinstance(return_period_size, pd.Timedelta):\n            if isinstance(return_period_size, str):\n                return_period_size = pd.to_timedelta(return_period_size)\n            else:\n                raise TypeError(\n                    f\"invalid type in {type(return_period_size)} \"\n                    f\"for the 'return_period_size' argument\"\n                )\n\n        # Calculate rate of extreme events\n        # as number of extreme events per `return_period_size`\n        if self.extremes_method == \"BM\":\n            extremes_rate = return_period_size / self.extremes_kwargs[\"block_size\"]\n        elif self.extremes_method == \"POT\":\n            n_periods = (self.data.index[-1] - self.data.index[0]) / return_period_size\n            extremes_rate = len(self.extremes) / n_periods\n        else:\n            raise AssertionError\n\n        # Convert 'return_period' to ndarray\n        return_period = np.asarray(a=return_period, dtype=np.float64).copy()\n        if return_period.ndim == 0:\n            return_period = return_period[np.newaxis]\n        if return_period.ndim != 1:\n            raise ValueError(\n                f\"invalid shape in {return_period.shape} \"\n                f\"for the 'return_period' argument, must be 1D array\"\n            )\n\n        # Calculate exceedance probability\n        exceedance_probability = 1 / return_period / extremes_rate\n\n        # Calculate return values\n        return tuple(\n            self.extremes_transformer.transform(value)\n            for value in self.model.get_return_value(\n                exceedance_probability=exceedance_probability, alpha=alpha, **kwargs\n            )\n        )\n\n    def get_summary(\n        self,\n        return_period,\n        return_period_size: typing.Union[str, pd.Timedelta] = \"365.2425D\",\n        alpha: typing.Optional[float] = None,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Generate a pandas DataFrame with return values and confidence interval bounds.\n\n        Parameters\n        ----------\n        return_period : array-like\n            Return period or 1D array of return periods.\n            Given as a multiple of `return_period_size`.\n        return_period_size : str or pandas.Timedelta, optional\n            Size of return periods (default='365.2425D').\n            If set to '30D', then a return period of 12\n            would be roughly equivalent to a 1 year return period (360 days).\n        alpha : float, optional\n            Width of confidence interval (0, 1).\n            If None (default), return None\n            for upper and lower confidence interval bounds.\n        kwargs\n            Model-specific keyword arguments.\n            If alpha is None, keyword arguments are ignored\n            (error still raised for unrecognized arguments).\n            MLE model:\n                n_samples : int, optional\n                    Number of bootstrap samples used to estimate\n                    confidence interval bounds (default=100).\n            Emcee model:\n                burn_in : int\n                    Burn-in value (number of first steps to discard for each walker).\n\n        Returns\n        -------\n        summary : pandas.DataFrame\n            DataFrame with return values and confidence interval bounds.\n\n        \"\"\"\n        # Convert 'return_period' to ndarray\n        return_period = np.asarray(a=return_period, dtype=np.float64).copy()\n        if return_period.ndim == 0:\n            return_period = return_period[np.newaxis]\n        if return_period.ndim != 1:\n            raise ValueError(\n                f\"invalid shape in {return_period.shape} \"\n                f\"for the 'return_period' argument, must be 1D array\"\n            )\n\n        # Calculate return values\n        rv = self.get_return_value(\n            return_period=return_period,\n            return_period_size=return_period_size,\n            alpha=alpha,\n            **kwargs,\n        )\n        return_values = []\n        for value in rv:\n            value = np.asarray(a=value, dtype=np.float64)\n            if value.ndim == 0:\n                value = value[np.newaxis]\n            return_values.append(value)\n\n        return pd.DataFrame(\n            data=np.transpose(return_values),\n            index=pd.Index(data=return_period, name=\"return period\"),\n            columns=[\"return value\", \"lower ci\", \"upper ci\"],\n        )\n\n    def plot_return_values(\n        self,\n        return_period=None,\n        return_period_size: typing.Union[str, pd.Timedelta] = \"365.2425D\",\n        alpha: typing.Optional[float] = None,\n        plotting_position: typing.Literal[\n            \"ecdf\",\n            \"hazen\",\n            \"weibull\",\n            \"tukey\",\n            \"blom\",\n            \"median\",\n            \"cunnane\",\n            \"gringorten\",\n            \"beard\",\n        ] = \"weibull\",\n        ax: typing.Optional[plt.Axes] = None,\n        figsize: typing.Tuple[float, float] = (8, 5),\n        **kwargs,\n    ) -&gt; tuple:  # pragma: no cover\n        \"\"\"\n        Plot return values and confidence intervals for given return periods.\n\n        Parameters\n        ----------\n        return_period : array-like, optional\n            Return period or 1D array of return periods.\n            Given as a multiple of `return_period_size`.\n            If None (default), calculates as 100 values uniformly spaced\n            within the range of return periods of the extracted extreme values.\n        return_period_size : str or pandas.Timedelta, optional\n            Size of return periods (default='365.2425D').\n            If set to '30D', then a return period of 12\n            would be roughly equivalent to a 1 year return period (360 days).\n        alpha : float, optional\n            Width of confidence interval (0, 1).\n            If None (default), confidence interval bounds are not plotted.\n        plotting_position : str, optional\n            Plotting position name (default='weibull'), not case-sensitive.\n            Supported plotting positions:\n                ecdf, hazen, weibull, tukey, blom, median, cunnane, gringorten, beard\n        ax : matplotlib.axes._axes.Axes, optional\n            Axes onto which the return value plot is drawn.\n            If None (default), a new figure and axes objects are created.\n        figsize : tuple, optional\n            Figure size in inches in format (width, height).\n            By default it is (8, 5).\n        kwargs\n            Model-specific keyword arguments.\n            If alpha is None, keyword arguments are ignored\n            (error still raised for unrecognized arguments).\n            MLE model:\n                n_samples : int, optional\n                    Number of bootstrap samples used to estimate\n                    confidence interval bounds (default=100).\n            Emcee model:\n                burn_in : int\n                    Burn-in value (number of first steps to discard for each walker).\n\n        Returns\n        -------\n        figure : matplotlib.figure.Figure\n            Figure object.\n        axes : matplotlib.axes._axes.Axes\n            Axes object.\n\n        \"\"\"\n        # Get observed return values\n        observed_return_values = get_return_periods(\n            ts=self.data,\n            extremes=self.extremes,\n            extremes_method=self.extremes_method,\n            extremes_type=self.extremes_type,\n            block_size=self.extremes_kwargs.get(\"block_size\", None),\n            return_period_size=return_period_size,\n            plotting_position=plotting_position,\n        )\n\n        # Parse the 'return_period' argument\n        if return_period is None:\n            return_period = np.linspace(\n                observed_return_values.loc[:, \"return period\"].min(),\n                observed_return_values.loc[:, \"return period\"].max(),\n                100,\n            )\n        else:\n            # Convert 'return_period' to ndarray\n            return_period = np.asarray(a=return_period, dtype=np.float64).copy()\n            if return_period.ndim == 0:\n                return_period = return_period[np.newaxis]\n            if return_period.ndim != 1:\n                raise ValueError(\n                    f\"invalid shape in {return_period.shape} \"\n                    f\"for the 'return_period' argument, must be 1D array\"\n                )\n            if len(return_period) &lt; 2:\n                raise ValueError(\n                    f\"'return_period' must have at least 2 return periods, \"\n                    f\"{len(return_period)} was given\"\n                )\n\n        # Get modeled return values\n        modeled_return_values = self.get_summary(\n            return_period=return_period,\n            return_period_size=return_period_size,\n            alpha=alpha,\n            **kwargs,\n        )\n\n        # Plot return values\n        return plot_return_values(\n            observed_return_values=observed_return_values,\n            modeled_return_values=modeled_return_values,\n            ax=ax,\n            figsize=figsize,\n        )\n\n    def plot_probability(\n        self,\n        plot_type: str,\n        return_period_size: typing.Union[str, pd.Timedelta] = \"365.2425D\",\n        plotting_position: typing.Literal[\n            \"ecdf\",\n            \"hazen\",\n            \"weibull\",\n            \"tukey\",\n            \"blom\",\n            \"median\",\n            \"cunnane\",\n            \"gringorten\",\n            \"beard\",\n        ] = \"weibull\",\n        ax: typing.Optional[plt.Axes] = None,\n        figsize: typing.Tuple[float, float] = (8, 8),\n    ) -&gt; tuple:  # pragma: no cover\n        \"\"\"\n        Plot a probability plot (QQ or PP).\n\n        Parameters\n        ----------\n        plot_type : str\n            Probability plot type.\n            Supported values:\n                PP - probability plot\n                QQ - quantile plot\n        return_period_size : str or pandas.Timedelta, optional\n            Size of return periods (default='365.2425D').\n            If set to '30D', then a return period of 12\n            would be roughly equivalent to a 1 year return period (360 days).\n        plotting_position : str, optional\n            Plotting position name (default='weibull'), not case-sensitive.\n            Supported plotting positions:\n                ecdf, hazen, weibull, tukey, blom, median, cunnane, gringorten, beard\n        ax : matplotlib.axes._axes.Axes, optional\n            Axes onto which the probability plot is drawn.\n            If None (default), a new figure and axes objects are created.\n        figsize : tuple, optional\n            Figure size in inches in format (width, height).\n            By default it is (8, 8).\n\n        Returns\n        -------\n        figure : matplotlib.figure.Figure\n            Figure object.\n        axes : matplotlib.axes._axes.Axes\n            Axes object.\n\n        \"\"\"\n        # Get observed return values\n        observed_return_values = get_return_periods(\n            ts=self.data,\n            extremes=self.extremes,\n            extremes_method=self.extremes_method,\n            extremes_type=self.extremes_type,\n            block_size=self.extremes_kwargs.get(\"block_size\", None),\n            return_period_size=return_period_size,\n            plotting_position=plotting_position,\n        )\n\n        # Get observed and theoretical values\n        # depending on 'plot_type'\n        if plot_type == \"PP\":\n            observed = (\n                1 - observed_return_values.loc[:, \"exceedance probability\"].values\n            )\n            theoretical = self.model.cdf(\n                self.extremes_transformer.transform(\n                    observed_return_values.loc[:, self.extremes.name].values\n                )\n            )\n        elif plot_type == \"QQ\":\n            observed = observed_return_values.loc[:, self.extremes.name].values\n            theoretical = self.extremes_transformer.transform(\n                self.model.isf(\n                    observed_return_values.loc[:, \"exceedance probability\"].values\n                )\n            )\n        else:\n            raise ValueError(\n                f\"invalid value in '{plot_type}' for the 'plot_type' argument, \"\n                f\"available values: PP, QQ\"\n            )\n\n        # Plot the probability plot\n        return plot_probability(\n            observed=observed,\n            theoretical=theoretical,\n            ax=ax,\n            figsize=figsize,\n        )\n\n    def plot_diagnostic(\n        self,\n        return_period=None,\n        return_period_size: typing.Union[str, pd.Timedelta] = \"365.2425D\",\n        alpha: typing.Optional[float] = None,\n        plotting_position: typing.Literal[\n            \"ecdf\",\n            \"hazen\",\n            \"weibull\",\n            \"tukey\",\n            \"blom\",\n            \"median\",\n            \"cunnane\",\n            \"gringorten\",\n            \"beard\",\n        ] = \"weibull\",\n        figsize: typing.Tuple[float, float] = (8, 8),\n        **kwargs,\n    ):  # pragma: no cover\n        \"\"\"\n        Plot a diagnostic plot.\n\n        This plot shows four key plots characterizing the EVA model:\n            - top left : return values plot\n            - top right : probability density (PDF) plot\n            - bottom left : quantile (Q-Q) plot\n            - bottom right : probability (P-P) plot\n\n        Parameters\n        ----------\n        return_period : array-like, optional\n            Return period or 1D array of return periods.\n            Given as a multiple of `return_period_size`.\n            If None (default), calculates as 100 values uniformly spaced\n            within the range of return periods of the extracted extreme values.\n        return_period_size : str or pandas.Timedelta, optional\n            Size of return periods (default='365.2425D').\n            If set to '30D', then a return period of 12\n            would be roughly equivalent to a 1 year return period (360 days).\n        alpha : float, optional\n            Width of confidence interval (0, 1).\n            If None (default), confidence interval bounds are not plotted.\n        plotting_position : str, optional\n            Plotting position name (default='weibull'), not case-sensitive.\n            Supported plotting positions:\n                ecdf, hazen, weibull, tukey, blom, median, cunnane, gringorten, beard\n        figsize : tuple, optional\n            Figure size in inches in format (width, height).\n            By default it is (8, 8).\n        kwargs\n            Model-specific keyword arguments.\n            If alpha is None, keyword arguments are ignored\n            (error still raised for unrecognized arguments).\n            MLE model:\n                n_samples : int, optional\n                    Number of bootstrap samples used to estimate\n                    confidence interval bounds (default=100).\n            Emcee model:\n                burn_in : int\n                    Burn-in value (number of first steps to discard for each walker).\n\n        Returns\n        -------\n        figure : matplotlib.figure.Figure\n            Figure object.\n        axes : tuple\n            Tuple with four Axes objects: return values, pdf, qq, pp\n\n        \"\"\"\n        with plt.rc_context(rc=pyextremes_rc):\n            # Create figure\n            fig = plt.figure(figsize=figsize, dpi=96)\n\n            # Create gridspec\n            gs = matplotlib.gridspec.GridSpec(\n                nrows=2,\n                ncols=2,\n                wspace=0.3,\n                hspace=0.3,\n                width_ratios=[1, 1],\n                height_ratios=[1, 1],\n            )\n\n            # Create axes\n            ax_rv = fig.add_subplot(gs[0, 0])\n            ax_pdf = fig.add_subplot(gs[0, 1])\n            ax_qq = fig.add_subplot(gs[1, 0])\n            ax_pp = fig.add_subplot(gs[1, 1])\n\n            # Plot return values\n            self.plot_return_values(\n                return_period=return_period,\n                return_period_size=return_period_size,\n                alpha=alpha,\n                plotting_position=plotting_position,\n                ax=ax_rv,\n                **kwargs,\n            )\n            ax_rv.set_title(\"Return value plot\")\n            ax_rv.grid(False, which=\"both\")\n\n            # Plot PDF\n            pdf_support = np.linspace(self.extremes.min(), self.extremes.max(), 100)\n            pdf = self.model.pdf(self.extremes_transformer.transform(pdf_support))\n            ax_pdf.grid(False)\n            ax_pdf.set_title(\"Probability density plot\")\n            ax_pdf.set_ylabel(\"Probability density\")\n            ax_pdf.set_xlabel(self.data.name)\n            ax_pdf.hist(\n                self.extremes.values,\n                bins=np.histogram_bin_edges(a=self.extremes.values, bins=\"auto\"),\n                density=True,\n                rwidth=0.8,\n                facecolor=\"#5199FF\",\n                edgecolor=\"None\",\n                lw=0,\n                alpha=0.25,\n                zorder=5,\n            )\n            ax_pdf.hist(\n                self.extremes.values,\n                bins=np.histogram_bin_edges(a=self.extremes.values, bins=\"auto\"),\n                density=True,\n                rwidth=0.8,\n                facecolor=\"None\",\n                edgecolor=\"#5199FF\",\n                lw=1,\n                ls=\"--\",\n                zorder=10,\n            )\n            ax_pdf.plot(pdf_support, pdf, color=\"#F85C50\", lw=2, ls=\"-\", zorder=15)\n            ax_pdf.scatter(\n                self.extremes.values,\n                np.full(shape=len(self.extremes), fill_value=0),\n                marker=\"|\",\n                s=40,\n                color=\"k\",\n                lw=0.5,\n                zorder=15,\n            )\n            ax_pdf.set_ylim(0, ax_pdf.get_ylim()[1])\n\n            # Plot Q-Q plot\n            self.plot_probability(\n                plot_type=\"QQ\",\n                return_period_size=return_period_size,\n                plotting_position=plotting_position,\n                ax=ax_qq,\n            )\n            ax_qq.set_title(\"Q-Q plot\")\n\n            # Plot P-P plot\n            self.plot_probability(\n                plot_type=\"PP\",\n                return_period_size=return_period_size,\n                plotting_position=plotting_position,\n                ax=ax_pp,\n            )\n            ax_pp.set_title(\"P-P plot\")\n\n            return fig, (ax_rv, ax_pdf, ax_qq, ax_pp)\n</code></pre>"},{"location":"api/eva/#pyextremes.eva.EVA.__init__","title":"<code>__init__(data)</code>","text":"<p>Initialize EVA model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series</code> <p>Time series to be analyzed. Index must be date-time and values must be numeric.</p> required Source code in <code>src/pyextremes/eva.py</code> <pre><code>def __init__(self, data: pd.Series) -&gt; None:\n    \"\"\"\n    Initialize EVA model.\n\n    Parameters\n    ----------\n    data : pandas.Series\n        Time series to be analyzed.\n        Index must be date-time and values must be numeric.\n\n    \"\"\"\n    # Ensure that `data` is pandas Series\n    if not isinstance(data, pd.Series):\n        raise TypeError(\n            f\"invalid type in '{type(data).__name__}' for the `data` argument, \"\n            f\"must be pandas.Series\"\n        )\n\n    # Copy `data` to ensure the original Series object it is not mutated\n    data = data.copy(deep=True)\n\n    # Ensure that `data` has correct index and value dtypes\n    if not np.issubdtype(data.dtype, np.number):\n        try:\n            message = \"`data` values are not numeric - converting to numeric\"\n            logger.debug(message)\n            warnings.warn(message=message, category=RuntimeWarning)\n            data = data.astype(np.float64)\n        except ValueError as _error:\n            raise TypeError(\n                f\"invalid dtype in {data.dtype} for the `data` argument, \"\n                f\"must be numeric (subdtype of numpy.number)\"\n            ) from _error\n    if not isinstance(data.index, pd.DatetimeIndex):\n        raise TypeError(\n            f\"index of `data` must be a sequence of date-time objects, \"\n            f\"not {data.index.inferred_type}\"\n        )\n\n    # Ensure `data` doesn't have duplicate indices\n    if (n_duplicates := len(data) - len(data.index.drop_duplicates())) &gt; 0:\n        message = (\n            f\"{n_duplicates:,d} duplicate indices found in `data` \"\n            \"- removing duplicate entries\"\n        )\n        logger.debug(message)\n        warnings.warn(message=message, category=RuntimeWarning)\n        data = data.groupby(data.index).first()\n\n    # Ensure that `data` is sorted\n    if not data.index.is_monotonic_increasing:\n        message = (\n            \"`data` index is not sorted in ascending order - \"\n            \"sorting `data` by index\"\n        )\n        logger.debug(message)\n        warnings.warn(message=message, category=RuntimeWarning)\n        data = data.sort_index(ascending=True)\n\n    # Ensure that `data` has no invalid entries\n    n_nans = data.isna().sum()\n    if n_nans &gt; 0:\n        message = (\n            f\"{n_nans:,d} Null values found in `data` - removing invalid entries\"\n        )\n        logger.debug(message)\n        warnings.warn(message=message, category=RuntimeWarning)\n        data = data.dropna()\n\n    # Set the `data` attribute\n    self.__data: pd.Series = data\n\n    # Initialize attributes related to extreme value extraction\n    self.__extremes = None\n    self.__extremes_method = None\n    self.__extremes_type = None\n    self.__extremes_kwargs = None\n    self.__extremes_transformer = None\n\n    # Initialize attributes related to model fitting\n    self.__model = None\n\n    logger.info(\"successfully initialized EVA object\")\n</code></pre>"},{"location":"api/eva/#pyextremes.eva.EVA.fit_model","title":"<code>fit_model(model='MLE', distribution=None, distribution_kwargs=None, **kwargs)</code>","text":"<p>Fit a model to the extracted extreme values.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name of model. By default it is 'MLE'. Name of model. Supported models:     MLE - Maximum Likelihood Estimate (MLE) model.         Based on 'scipy' package (scipy.stats.rv_continuous.fit).     Emcee - Markov Chain Monte Carlo (MCMC) model.         Based on 'emcee' package by Daniel Foreman-Mackey.</p> <code>'MLE'</code> <code>distribution</code> <code>str or rv_continuous</code> <p>Distribution name compatible with scipy.stats or a subclass of scipy.stats.rv_continuous. See https://docs.scipy.org/doc/scipy/reference/stats.html By default the distribution is selected automatically as best between 'genextreme' and 'gumbel_r' for 'BM' extremes and 'genpareto' and 'expon' for 'POT' extremes. Best distribution is selected using the AIC metric.</p> <code>None</code> <code>distribution_kwargs</code> <code>dict</code> <p>Special keyword arguments, passed to the <code>.fit</code> method of the distribution. These keyword arguments represent parameters to be held fixed. Names of parameters to be fixed must have 'f' prefixes. Valid parameters:     - shape(s): 'fc', e.g. fc=0     - location: 'floc', e.g. floc=0     - scale: 'fscale', e.g. fscale=1 See documentation of a specific scipy.stats distribution for names of available parameters. By default, location parameter for 'genpareto' and 'expon' distributions is fixed to threshold (POT) or to minimum extremes (BM) value. Set to empty dictionary (distribution_kwargs={}) to avoid this behaviour.</p> <code>None</code> <code>kwargs</code> <p>Keyword arguments passed to a model .fit method. MLE model:     MLE model takes no additional arguments. Emcee model:     n_walkers : int, optional         The number of walkers in the ensemble (default=100).     n_samples : int, optional         The number of steps to run (default=500).     progress : bool or str, optional         If True, a progress bar will be shown as the sampler progresses.         If a string, will select a specific tqdm progress bar.         Most notable is 'notebook', which shows a progress bar         suitable for Jupyter notebooks.         If False (default), no progress bar will be shown.         This progress bar is a part of the <code>emcee</code> package.</p> <code>{}</code> Source code in <code>src/pyextremes/eva.py</code> <pre><code>def fit_model(\n    self,\n    model: typing.Literal[\"MLE\", \"Emcee\"] = \"MLE\",\n    distribution: typing.Union[str, scipy.stats.rv_continuous] = None,\n    distribution_kwargs: typing.Optional[dict] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Fit a model to the extracted extreme values.\n\n    Parameters\n    ----------\n    model : str, optional\n        Name of model. By default it is 'MLE'.\n        Name of model.\n        Supported models:\n            MLE - Maximum Likelihood Estimate (MLE) model.\n                Based on 'scipy' package (scipy.stats.rv_continuous.fit).\n            Emcee - Markov Chain Monte Carlo (MCMC) model.\n                Based on 'emcee' package by Daniel Foreman-Mackey.\n    distribution : str or scipy.stats.rv_continuous, optional\n        Distribution name compatible with scipy.stats\n        or a subclass of scipy.stats.rv_continuous.\n        See https://docs.scipy.org/doc/scipy/reference/stats.html\n        By default the distribution is selected automatically\n        as best between 'genextreme' and 'gumbel_r' for 'BM' extremes\n        and 'genpareto' and 'expon' for 'POT' extremes.\n        Best distribution is selected using the AIC metric.\n    distribution_kwargs : dict, optional\n        Special keyword arguments, passed to the `.fit` method of the distribution.\n        These keyword arguments represent parameters to be held fixed.\n        Names of parameters to be fixed must have 'f' prefixes. Valid parameters:\n            - shape(s): 'fc', e.g. fc=0\n            - location: 'floc', e.g. floc=0\n            - scale: 'fscale', e.g. fscale=1\n        See documentation of a specific scipy.stats distribution\n        for names of available parameters.\n        By default, location parameter for 'genpareto' and 'expon' distributions\n        is fixed to threshold (POT) or to minimum extremes (BM) value.\n        Set to empty dictionary (distribution_kwargs={}) to avoid this behaviour.\n    kwargs\n        Keyword arguments passed to a model .fit method.\n        MLE model:\n            MLE model takes no additional arguments.\n        Emcee model:\n            n_walkers : int, optional\n                The number of walkers in the ensemble (default=100).\n            n_samples : int, optional\n                The number of steps to run (default=500).\n            progress : bool or str, optional\n                If True, a progress bar will be shown as the sampler progresses.\n                If a string, will select a specific tqdm progress bar.\n                Most notable is 'notebook', which shows a progress bar\n                suitable for Jupyter notebooks.\n                If False (default), no progress bar will be shown.\n                This progress bar is a part of the `emcee` package.\n\n    \"\"\"\n    # Select default distribution\n    if distribution is None:\n        logger.debug(\n            \"selecting default distribution for extremes extracted using the \"\n            \"'%s' method\",\n            self.extremes_method,\n        )\n\n        # Prepare list of candidate distributions\n        if self.extremes_method == \"BM\":\n            candidate_distributions = [\"genextreme\", \"gumbel_r\"]\n            _distribution_kwargs = None\n        elif self.extremes_method == \"POT\":\n            candidate_distributions = [\"genpareto\", \"expon\"]\n            _distribution_kwargs = {\n                \"floc\": self.extremes_kwargs.get(\n                    \"threshold\",\n                    self.extremes_transformer.transformed_extremes.min(),\n                )\n            }\n        else:\n            raise AssertionError\n\n        # Fit MLE model for candidate distributions\n        # and select distribution with smallest AIC\n        distribution = None\n        aic = np.inf\n        for distribution_name in candidate_distributions:\n            new_aic = MLE(\n                extremes=self.extremes_transformer.transformed_extremes,\n                distribution=distribution_name,\n                distribution_kwargs=_distribution_kwargs,\n            ).AIC\n            if new_aic &lt; aic:\n                distribution = distribution_name\n                aic = new_aic\n        logger.info(\n            \"selected '%s' distribution with AIC score %s\",\n            distribution,\n            aic,\n        )\n\n    # Get distribution name\n    if isinstance(distribution, str):\n        distribution_name = distribution\n    elif isinstance(distribution, scipy.stats.rv_continuous):\n        distribution_name = getattr(distribution, \"name\", None)\n    else:\n        raise TypeError(\n            f\"invalid type in {type(distribution)} \"\n            f\"for the 'distribution' argument, \"\n            f\"must be string or scipy.stats.rv_continuous\"\n        )\n\n    # Checking if distribution is valid per extreme value theory:\n    # Fisher-Tippet-Gnedenko theorem for 'BM'\n    # Pickands\u2013Balkema\u2013de Haan theorem for 'POT'\n    if distribution_name is None:\n        warnings.warn(\n            message=(\n                \"provided distribution 'name' attribute cannot be resolved \"\n                \"and distribution validity cannot be verified\"\n            ),\n            category=RuntimeWarning,\n        )\n    else:\n        if self.extremes_method == \"BM\" and distribution_name not in [\n            \"genextreme\",\n            \"gumbel_r\",\n        ]:\n            warnings.warn(\n                message=(\n                    f\"'{distribution_name}' distribution is not \"\n                    f\"recommended to be used with extremes extracted \"\n                    f\"using the 'BM' method, 'genextreme' or 'gumebel_r' \"\n                    f\"should be used per the Fisher-Tippet-Gnedenko theorem\"\n                ),\n                category=RuntimeWarning,\n            )\n        elif self.extremes_method == \"POT\" and distribution_name not in [\n            \"genpareto\",\n            \"expon\",\n        ]:\n            warnings.warn(\n                message=(\n                    f\"'{distribution_name}' distribution is not \"\n                    f\"recommended to be used with extremes extracted \"\n                    f\"using the 'POT' method, 'genpareto' or 'expon' \"\n                    f\"should be used per the Pickands\u2013Balkema\u2013de Haan theorem\"\n                ),\n                category=RuntimeWarning,\n            )\n\n    # Freeze (fix) location parameter for genpareto/expon distributions\n    if distribution_kwargs is None and distribution_name in [\"genpareto\", \"expon\"]:\n        distribution_kwargs = {\n            \"floc\": self.extremes_kwargs.get(\n                \"threshold\", self.extremes_transformer.transformed_extremes.min()\n            )\n        }\n        logger.debug(\n            \"freezing location parameter (floc) at %s for '%s' distribution\",\n            distribution_kwargs[\"floc\"],\n            distribution_name,\n        )\n\n    # Fit model to transformed extremes\n    self.__model = get_model(\n        model=model,\n        extremes=self.extremes_transformer.transformed_extremes,\n        distribution=distribution,\n        distribution_kwargs=distribution_kwargs,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/eva/#pyextremes.eva.EVA.from_extremes","title":"<code>from_extremes(extremes, method='BM', extremes_type='high', **kwargs)</code>  <code>classmethod</code>","text":"<p>Create an EVA model using pre-defined <code>extremes</code>.</p> <p>A typical reason to use this method is when full timeseries is not available and only the extracted extremes (i.e. annual maxima) are known.</p> <p>Parameters:</p> Name Type Description Default <code>extremes</code> <code>Series</code> <p>Time series of extreme values.</p> required <code>method</code> <code>str</code> <p>Extreme value extraction method. Supported values:     BM (default) - Block Maxima     POT - Peaks Over Threshold</p> <code>'BM'</code> <code>extremes_type</code> <code>str</code> <p>high (default) - extreme high values low - extreme low values</p> <code>'high'</code> <code>kwargs</code> <p>if method is BM:     block_size : str or pandas.Timedelta, optional         Block size.         If None (default), then is calculated as median distance         between extreme events.     errors : str, optional         raise - raise an exception             when encountering a block with no data         ignore (default) - ignore blocks with no data         coerce - get extreme values for blocks with no data             as mean of all other extreme events in the series             with index being the middle point of corresponding interval     min_last_block : float, optional         Minimum data availability ratio (0 to 1) in the last block         for it to be used to extract extreme value from.         This is used to discard last block when it is too short.         If None (default), last block is always used. if method is POT:     threshold : float, optional         Threshold used to find exceedances.         By default is taken as smallest value.     r : pandas.Timedelta or value convertible to timedelta, optional         Duration of window used to decluster the exceedances.         By default r='24H' (24 hours).         See pandas.to_timedelta for more information.</p> <code>{}</code> <p>Returns:</p> Type Description <code>EVA</code> <p>EVA model initialized with <code>extremes</code>.</p> Source code in <code>src/pyextremes/eva.py</code> <pre><code>@classmethod\ndef from_extremes(\n    cls,\n    extremes: pd.Series,\n    method: typing.Literal[\"BM\", \"POT\"] = \"BM\",\n    extremes_type: typing.Literal[\"high\", \"low\"] = \"high\",\n    **kwargs,\n) -&gt; EVA:\n    \"\"\"\n    Create an EVA model using pre-defined `extremes`.\n\n    A typical reason to use this method is when full timeseries is not available\n    and only the extracted extremes (i.e. annual maxima) are known.\n\n    Parameters\n    ----------\n    extremes : pd.Series\n        Time series of extreme values.\n    method : str, optional\n        Extreme value extraction method.\n        Supported values:\n            BM (default) - Block Maxima\n            POT - Peaks Over Threshold\n    extremes_type : str, optional\n        high (default) - extreme high values\n        low - extreme low values\n    kwargs:\n        if method is BM:\n            block_size : str or pandas.Timedelta, optional\n                Block size.\n                If None (default), then is calculated as median distance\n                between extreme events.\n            errors : str, optional\n                raise - raise an exception\n                    when encountering a block with no data\n                ignore (default) - ignore blocks with no data\n                coerce - get extreme values for blocks with no data\n                    as mean of all other extreme events in the series\n                    with index being the middle point of corresponding interval\n            min_last_block : float, optional\n                Minimum data availability ratio (0 to 1) in the last block\n                for it to be used to extract extreme value from.\n                This is used to discard last block when it is too short.\n                If None (default), last block is always used.\n        if method is POT:\n            threshold : float, optional\n                Threshold used to find exceedances.\n                By default is taken as smallest value.\n            r : pandas.Timedelta or value convertible to timedelta, optional\n                Duration of window used to decluster the exceedances.\n                By default r='24H' (24 hours).\n                See pandas.to_timedelta for more information.\n\n    Returns\n    -------\n    EVA\n        EVA model initialized with `extremes`.\n\n    \"\"\"\n    model = cls(data=extremes)\n    model.set_extremes(\n        extremes=model.data,\n        method=method,\n        extremes_type=extremes_type,\n        **kwargs,\n    )\n    return model\n</code></pre>"},{"location":"api/eva/#pyextremes.eva.EVA.get_extremes","title":"<code>get_extremes(method, extremes_type='high', **kwargs)</code>","text":"<p>Get extreme events from time series.</p> <p>Extracts extreme values from the 'self.data' attribute. Stores extreme values in the 'self.extremes' attribute.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Extreme value extraction method. Supported values:     BM - Block Maxima     POT - Peaks Over Threshold</p> required <code>extremes_type</code> <code>str</code> <p>high (default) - get extreme high values low - get extreme low values</p> <code>'high'</code> <code>kwargs</code> <p>if method is BM:     block_size : str or pandas.Timedelta, optional         Block size (default='365.2425D').         See pandas.to_timedelta for more information.     errors : str, optional         raise (default) - raise an exception             when encountering a block with no data         ignore - ignore blocks with no data         coerce - get extreme values for blocks with no data             as mean of all other extreme events in the series             with index being the middle point of corresponding interval     min_last_block : float, optional         Minimum data availability ratio (0 to 1) in the last block         for it to be used to extract extreme value from.         This is used to discard last block when it is too short.         If None (default), last block is always used. if method is POT:     threshold : float         Threshold used to find exceedances.     r : pandas.Timedelta or value convertible to timedelta, optional         Duration of window used to decluster the exceedances.         By default r='24H' (24 hours).         See pandas.to_timedelta for more information.</p> <code>{}</code> Source code in <code>src/pyextremes/eva.py</code> <pre><code>def get_extremes(\n    self,\n    method: typing.Literal[\"BM\", \"POT\"],\n    extremes_type: typing.Literal[\"high\", \"low\"] = \"high\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Get extreme events from time series.\n\n    Extracts extreme values from the 'self.data' attribute.\n    Stores extreme values in the 'self.extremes' attribute.\n\n    Parameters\n    ----------\n    method : str\n        Extreme value extraction method.\n        Supported values:\n            BM - Block Maxima\n            POT - Peaks Over Threshold\n    extremes_type : str, optional\n        high (default) - get extreme high values\n        low - get extreme low values\n    kwargs\n        if method is BM:\n            block_size : str or pandas.Timedelta, optional\n                Block size (default='365.2425D').\n                See pandas.to_timedelta for more information.\n            errors : str, optional\n                raise (default) - raise an exception\n                    when encountering a block with no data\n                ignore - ignore blocks with no data\n                coerce - get extreme values for blocks with no data\n                    as mean of all other extreme events in the series\n                    with index being the middle point of corresponding interval\n            min_last_block : float, optional\n                Minimum data availability ratio (0 to 1) in the last block\n                for it to be used to extract extreme value from.\n                This is used to discard last block when it is too short.\n                If None (default), last block is always used.\n        if method is POT:\n            threshold : float\n                Threshold used to find exceedances.\n            r : pandas.Timedelta or value convertible to timedelta, optional\n                Duration of window used to decluster the exceedances.\n                By default r='24H' (24 hours).\n                See pandas.to_timedelta for more information.\n\n    \"\"\"\n    message = f\"for method='{method}' and extremes_type='{extremes_type}'\"\n    logger.debug(\"extracting extreme values %s\", message)\n    self.__extremes = get_extremes(\n        method=method,\n        ts=self.data,\n        extremes_type=extremes_type,\n        **kwargs,\n    )\n    self.__extremes_method = method\n    self.__extremes_type = extremes_type\n    logger.info(\"successfully extracted extreme values %s\", message)\n\n    logger.debug(\"collecting extreme value properties\")\n    self.__extremes_kwargs = {}\n    if method == \"BM\":\n        self.__extremes_kwargs[\"block_size\"] = pd.to_timedelta(\n            kwargs.get(\"block_size\", \"365.2425D\")\n        )\n        self.__extremes_kwargs[\"errors\"] = kwargs.get(\"errors\", \"raise\")\n        self.__extremes_kwargs[\"min_last_block\"] = kwargs.get(\n            \"min_last_block\", None\n        )\n    else:\n        self.__extremes_kwargs[\"threshold\"] = kwargs.get(\"threshold\")\n        self.__extremes_kwargs[\"r\"] = pd.to_timedelta(kwargs.get(\"r\", \"24h\"))\n    logger.info(\"successfully collected extreme value properties\")\n\n    logger.debug(\"creating extremes transformer\")\n    self.__extremes_transformer = ExtremesTransformer(\n        extremes=self.__extremes,\n        extremes_type=self.__extremes_type,\n    )\n    logger.info(\"successfully created extremes transformer\")\n\n    logger.info(\"removing any previously declared models\")\n    self.__model = None\n</code></pre>"},{"location":"api/eva/#pyextremes.eva.EVA.get_return_value","title":"<code>get_return_value(return_period, return_period_size='365.2425D', alpha=None, **kwargs)</code>","text":"<p>Get return value and confidence interval for given return period(s).</p> <p>Parameters:</p> Name Type Description Default <code>return_period</code> <code>array - like</code> <p>Return period or 1D array of return periods. Given as a multiple of <code>return_period_size</code>.</p> required <code>return_period_size</code> <code>str or Timedelta</code> <p>Size of return periods (default='365.2425D'). If set to '30D', then a return period of 12 would be roughly equivalent to a 1 year return period (360 days).</p> <code>'365.2425D'</code> <code>alpha</code> <code>float</code> <p>Width of confidence interval (0, 1). If None (default), return None for upper and lower confidence interval bounds.</p> <code>None</code> <code>kwargs</code> <p>Model-specific keyword arguments. If alpha is None, keyword arguments are ignored (error still raised for unrecognized arguments). MLE model:     n_samples : int, optional         Number of bootstrap samples used to estimate         confidence interval bounds (default=100). Emcee model:     burn_in : int         Burn-in value (number of first steps to discard for each walker).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>return_value</code> <code>array - like</code> <p>Return values.</p> <code>ci_lower</code> <code>array - like</code> <p>Lower confidence interval bounds.</p> <code>ci_upper</code> <code>array - like</code> <p>Upper confidence interval bounds.</p> Source code in <code>src/pyextremes/eva.py</code> <pre><code>def get_return_value(\n    self,\n    return_period,\n    return_period_size: typing.Union[str, pd.Timedelta] = \"365.2425D\",\n    alpha: typing.Optional[float] = None,\n    **kwargs,\n) -&gt; tuple:\n    \"\"\"\n    Get return value and confidence interval for given return period(s).\n\n    Parameters\n    ----------\n    return_period : array-like\n        Return period or 1D array of return periods.\n        Given as a multiple of `return_period_size`.\n    return_period_size : str or pandas.Timedelta, optional\n        Size of return periods (default='365.2425D').\n        If set to '30D', then a return period of 12\n        would be roughly equivalent to a 1 year return period (360 days).\n    alpha : float, optional\n        Width of confidence interval (0, 1).\n        If None (default), return None\n        for upper and lower confidence interval bounds.\n    kwargs\n        Model-specific keyword arguments.\n        If alpha is None, keyword arguments are ignored\n        (error still raised for unrecognized arguments).\n        MLE model:\n            n_samples : int, optional\n                Number of bootstrap samples used to estimate\n                confidence interval bounds (default=100).\n        Emcee model:\n            burn_in : int\n                Burn-in value (number of first steps to discard for each walker).\n\n    Returns\n    -------\n    return_value : array-like\n        Return values.\n    ci_lower : array-like\n        Lower confidence interval bounds.\n    ci_upper : array-like\n        Upper confidence interval bounds.\n\n    \"\"\"\n    # Parse the 'return_period_size' argument\n    if not isinstance(return_period_size, pd.Timedelta):\n        if isinstance(return_period_size, str):\n            return_period_size = pd.to_timedelta(return_period_size)\n        else:\n            raise TypeError(\n                f\"invalid type in {type(return_period_size)} \"\n                f\"for the 'return_period_size' argument\"\n            )\n\n    # Calculate rate of extreme events\n    # as number of extreme events per `return_period_size`\n    if self.extremes_method == \"BM\":\n        extremes_rate = return_period_size / self.extremes_kwargs[\"block_size\"]\n    elif self.extremes_method == \"POT\":\n        n_periods = (self.data.index[-1] - self.data.index[0]) / return_period_size\n        extremes_rate = len(self.extremes) / n_periods\n    else:\n        raise AssertionError\n\n    # Convert 'return_period' to ndarray\n    return_period = np.asarray(a=return_period, dtype=np.float64).copy()\n    if return_period.ndim == 0:\n        return_period = return_period[np.newaxis]\n    if return_period.ndim != 1:\n        raise ValueError(\n            f\"invalid shape in {return_period.shape} \"\n            f\"for the 'return_period' argument, must be 1D array\"\n        )\n\n    # Calculate exceedance probability\n    exceedance_probability = 1 / return_period / extremes_rate\n\n    # Calculate return values\n    return tuple(\n        self.extremes_transformer.transform(value)\n        for value in self.model.get_return_value(\n            exceedance_probability=exceedance_probability, alpha=alpha, **kwargs\n        )\n    )\n</code></pre>"},{"location":"api/eva/#pyextremes.eva.EVA.get_summary","title":"<code>get_summary(return_period, return_period_size='365.2425D', alpha=None, **kwargs)</code>","text":"<p>Generate a pandas DataFrame with return values and confidence interval bounds.</p> <p>Parameters:</p> Name Type Description Default <code>return_period</code> <code>array - like</code> <p>Return period or 1D array of return periods. Given as a multiple of <code>return_period_size</code>.</p> required <code>return_period_size</code> <code>str or Timedelta</code> <p>Size of return periods (default='365.2425D'). If set to '30D', then a return period of 12 would be roughly equivalent to a 1 year return period (360 days).</p> <code>'365.2425D'</code> <code>alpha</code> <code>float</code> <p>Width of confidence interval (0, 1). If None (default), return None for upper and lower confidence interval bounds.</p> <code>None</code> <code>kwargs</code> <p>Model-specific keyword arguments. If alpha is None, keyword arguments are ignored (error still raised for unrecognized arguments). MLE model:     n_samples : int, optional         Number of bootstrap samples used to estimate         confidence interval bounds (default=100). Emcee model:     burn_in : int         Burn-in value (number of first steps to discard for each walker).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>summary</code> <code>DataFrame</code> <p>DataFrame with return values and confidence interval bounds.</p> Source code in <code>src/pyextremes/eva.py</code> <pre><code>def get_summary(\n    self,\n    return_period,\n    return_period_size: typing.Union[str, pd.Timedelta] = \"365.2425D\",\n    alpha: typing.Optional[float] = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate a pandas DataFrame with return values and confidence interval bounds.\n\n    Parameters\n    ----------\n    return_period : array-like\n        Return period or 1D array of return periods.\n        Given as a multiple of `return_period_size`.\n    return_period_size : str or pandas.Timedelta, optional\n        Size of return periods (default='365.2425D').\n        If set to '30D', then a return period of 12\n        would be roughly equivalent to a 1 year return period (360 days).\n    alpha : float, optional\n        Width of confidence interval (0, 1).\n        If None (default), return None\n        for upper and lower confidence interval bounds.\n    kwargs\n        Model-specific keyword arguments.\n        If alpha is None, keyword arguments are ignored\n        (error still raised for unrecognized arguments).\n        MLE model:\n            n_samples : int, optional\n                Number of bootstrap samples used to estimate\n                confidence interval bounds (default=100).\n        Emcee model:\n            burn_in : int\n                Burn-in value (number of first steps to discard for each walker).\n\n    Returns\n    -------\n    summary : pandas.DataFrame\n        DataFrame with return values and confidence interval bounds.\n\n    \"\"\"\n    # Convert 'return_period' to ndarray\n    return_period = np.asarray(a=return_period, dtype=np.float64).copy()\n    if return_period.ndim == 0:\n        return_period = return_period[np.newaxis]\n    if return_period.ndim != 1:\n        raise ValueError(\n            f\"invalid shape in {return_period.shape} \"\n            f\"for the 'return_period' argument, must be 1D array\"\n        )\n\n    # Calculate return values\n    rv = self.get_return_value(\n        return_period=return_period,\n        return_period_size=return_period_size,\n        alpha=alpha,\n        **kwargs,\n    )\n    return_values = []\n    for value in rv:\n        value = np.asarray(a=value, dtype=np.float64)\n        if value.ndim == 0:\n            value = value[np.newaxis]\n        return_values.append(value)\n\n    return pd.DataFrame(\n        data=np.transpose(return_values),\n        index=pd.Index(data=return_period, name=\"return period\"),\n        columns=[\"return value\", \"lower ci\", \"upper ci\"],\n    )\n</code></pre>"},{"location":"api/eva/#pyextremes.eva.EVA.plot_corner","title":"<code>plot_corner(burn_in=0, labels=None, levels=None, figsize=(8, 8))</code>","text":"<p>Plot corner plot for MCMC sampler trace.</p> <p>Parameters:</p> Name Type Description Default <code>burn_in</code> <code>int</code> <p>Burn-in value (number of first steps to discard for each walker). By default it is 0 (no values are discarded).</p> <code>0</code> <code>labels</code> <code>array - like</code> <p>Sequence of strings with parameter names, used to label axes. If None (default), then axes are labeled sequentially.</p> <code>None</code> <code>levels</code> <code>int</code> <p>Number of Gaussian KDE contours to plot. If None (default), then not shown.</p> <code>None</code> <code>figsize</code> <code>tuple</code> <p>Figure size in inches. By default it is (8, 8).</p> <code>(8, 8)</code> <p>Returns:</p> Name Type Description <code>figure</code> <code>Figure</code> <p>Figure object.</p> <code>axes</code> <code>list</code> <p>2D list with Axes objects of size N by N, where N is <code>trace.shape[2]</code>. Empty slots are represented by None. Axes are ordered from left to right top to bottom.</p> Source code in <code>src/pyextremes/eva.py</code> <pre><code>def plot_corner(\n    self,\n    burn_in: int = 0,\n    labels: typing.Optional[typing.List[str]] = None,\n    levels: typing.Optional[int] = None,\n    figsize: typing.Tuple[float, float] = (8, 8),\n) -&gt; typing.Tuple[plt.Figure, list]:  # pragma: no cover\n    \"\"\"\n    Plot corner plot for MCMC sampler trace.\n\n    Parameters\n    ----------\n    burn_in : int, optional\n        Burn-in value (number of first steps to discard for each walker).\n        By default it is 0 (no values are discarded).\n    labels : array-like, optional\n        Sequence of strings with parameter names, used to label axes.\n        If None (default), then axes are labeled sequentially.\n    levels : int, optional\n        Number of Gaussian KDE contours to plot.\n        If None (default), then not shown.\n    figsize : tuple, optional\n        Figure size in inches. By default it is (8, 8).\n\n    Returns\n    -------\n    figure : matplotlib.figure.Figure\n        Figure object.\n    axes : list\n        2D list with Axes objects of size N by N, where N is `trace.shape[2]`.\n        Empty slots are represented by None. Axes are ordered from left to right\n        top to bottom.\n\n    \"\"\"\n    trace, trace_map, labels = self._get_mcmc_plot_inputs(labels=labels)\n    return plot_corner(\n        trace=trace,\n        trace_map=trace_map,\n        burn_in=burn_in,\n        labels=labels,\n        levels=levels,\n        figsize=figsize,\n    )\n</code></pre>"},{"location":"api/eva/#pyextremes.eva.EVA.plot_diagnostic","title":"<code>plot_diagnostic(return_period=None, return_period_size='365.2425D', alpha=None, plotting_position='weibull', figsize=(8, 8), **kwargs)</code>","text":"<p>Plot a diagnostic plot.</p> <p>This plot shows four key plots characterizing the EVA model:     - top left : return values plot     - top right : probability density (PDF) plot     - bottom left : quantile (Q-Q) plot     - bottom right : probability (P-P) plot</p> <p>Parameters:</p> Name Type Description Default <code>return_period</code> <code>array - like</code> <p>Return period or 1D array of return periods. Given as a multiple of <code>return_period_size</code>. If None (default), calculates as 100 values uniformly spaced within the range of return periods of the extracted extreme values.</p> <code>None</code> <code>return_period_size</code> <code>str or Timedelta</code> <p>Size of return periods (default='365.2425D'). If set to '30D', then a return period of 12 would be roughly equivalent to a 1 year return period (360 days).</p> <code>'365.2425D'</code> <code>alpha</code> <code>float</code> <p>Width of confidence interval (0, 1). If None (default), confidence interval bounds are not plotted.</p> <code>None</code> <code>plotting_position</code> <code>str</code> <p>Plotting position name (default='weibull'), not case-sensitive. Supported plotting positions:     ecdf, hazen, weibull, tukey, blom, median, cunnane, gringorten, beard</p> <code>'weibull'</code> <code>figsize</code> <code>tuple</code> <p>Figure size in inches in format (width, height). By default it is (8, 8).</p> <code>(8, 8)</code> <code>kwargs</code> <p>Model-specific keyword arguments. If alpha is None, keyword arguments are ignored (error still raised for unrecognized arguments). MLE model:     n_samples : int, optional         Number of bootstrap samples used to estimate         confidence interval bounds (default=100). Emcee model:     burn_in : int         Burn-in value (number of first steps to discard for each walker).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>figure</code> <code>Figure</code> <p>Figure object.</p> <code>axes</code> <code>tuple</code> <p>Tuple with four Axes objects: return values, pdf, qq, pp</p> Source code in <code>src/pyextremes/eva.py</code> <pre><code>def plot_diagnostic(\n    self,\n    return_period=None,\n    return_period_size: typing.Union[str, pd.Timedelta] = \"365.2425D\",\n    alpha: typing.Optional[float] = None,\n    plotting_position: typing.Literal[\n        \"ecdf\",\n        \"hazen\",\n        \"weibull\",\n        \"tukey\",\n        \"blom\",\n        \"median\",\n        \"cunnane\",\n        \"gringorten\",\n        \"beard\",\n    ] = \"weibull\",\n    figsize: typing.Tuple[float, float] = (8, 8),\n    **kwargs,\n):  # pragma: no cover\n    \"\"\"\n    Plot a diagnostic plot.\n\n    This plot shows four key plots characterizing the EVA model:\n        - top left : return values plot\n        - top right : probability density (PDF) plot\n        - bottom left : quantile (Q-Q) plot\n        - bottom right : probability (P-P) plot\n\n    Parameters\n    ----------\n    return_period : array-like, optional\n        Return period or 1D array of return periods.\n        Given as a multiple of `return_period_size`.\n        If None (default), calculates as 100 values uniformly spaced\n        within the range of return periods of the extracted extreme values.\n    return_period_size : str or pandas.Timedelta, optional\n        Size of return periods (default='365.2425D').\n        If set to '30D', then a return period of 12\n        would be roughly equivalent to a 1 year return period (360 days).\n    alpha : float, optional\n        Width of confidence interval (0, 1).\n        If None (default), confidence interval bounds are not plotted.\n    plotting_position : str, optional\n        Plotting position name (default='weibull'), not case-sensitive.\n        Supported plotting positions:\n            ecdf, hazen, weibull, tukey, blom, median, cunnane, gringorten, beard\n    figsize : tuple, optional\n        Figure size in inches in format (width, height).\n        By default it is (8, 8).\n    kwargs\n        Model-specific keyword arguments.\n        If alpha is None, keyword arguments are ignored\n        (error still raised for unrecognized arguments).\n        MLE model:\n            n_samples : int, optional\n                Number of bootstrap samples used to estimate\n                confidence interval bounds (default=100).\n        Emcee model:\n            burn_in : int\n                Burn-in value (number of first steps to discard for each walker).\n\n    Returns\n    -------\n    figure : matplotlib.figure.Figure\n        Figure object.\n    axes : tuple\n        Tuple with four Axes objects: return values, pdf, qq, pp\n\n    \"\"\"\n    with plt.rc_context(rc=pyextremes_rc):\n        # Create figure\n        fig = plt.figure(figsize=figsize, dpi=96)\n\n        # Create gridspec\n        gs = matplotlib.gridspec.GridSpec(\n            nrows=2,\n            ncols=2,\n            wspace=0.3,\n            hspace=0.3,\n            width_ratios=[1, 1],\n            height_ratios=[1, 1],\n        )\n\n        # Create axes\n        ax_rv = fig.add_subplot(gs[0, 0])\n        ax_pdf = fig.add_subplot(gs[0, 1])\n        ax_qq = fig.add_subplot(gs[1, 0])\n        ax_pp = fig.add_subplot(gs[1, 1])\n\n        # Plot return values\n        self.plot_return_values(\n            return_period=return_period,\n            return_period_size=return_period_size,\n            alpha=alpha,\n            plotting_position=plotting_position,\n            ax=ax_rv,\n            **kwargs,\n        )\n        ax_rv.set_title(\"Return value plot\")\n        ax_rv.grid(False, which=\"both\")\n\n        # Plot PDF\n        pdf_support = np.linspace(self.extremes.min(), self.extremes.max(), 100)\n        pdf = self.model.pdf(self.extremes_transformer.transform(pdf_support))\n        ax_pdf.grid(False)\n        ax_pdf.set_title(\"Probability density plot\")\n        ax_pdf.set_ylabel(\"Probability density\")\n        ax_pdf.set_xlabel(self.data.name)\n        ax_pdf.hist(\n            self.extremes.values,\n            bins=np.histogram_bin_edges(a=self.extremes.values, bins=\"auto\"),\n            density=True,\n            rwidth=0.8,\n            facecolor=\"#5199FF\",\n            edgecolor=\"None\",\n            lw=0,\n            alpha=0.25,\n            zorder=5,\n        )\n        ax_pdf.hist(\n            self.extremes.values,\n            bins=np.histogram_bin_edges(a=self.extremes.values, bins=\"auto\"),\n            density=True,\n            rwidth=0.8,\n            facecolor=\"None\",\n            edgecolor=\"#5199FF\",\n            lw=1,\n            ls=\"--\",\n            zorder=10,\n        )\n        ax_pdf.plot(pdf_support, pdf, color=\"#F85C50\", lw=2, ls=\"-\", zorder=15)\n        ax_pdf.scatter(\n            self.extremes.values,\n            np.full(shape=len(self.extremes), fill_value=0),\n            marker=\"|\",\n            s=40,\n            color=\"k\",\n            lw=0.5,\n            zorder=15,\n        )\n        ax_pdf.set_ylim(0, ax_pdf.get_ylim()[1])\n\n        # Plot Q-Q plot\n        self.plot_probability(\n            plot_type=\"QQ\",\n            return_period_size=return_period_size,\n            plotting_position=plotting_position,\n            ax=ax_qq,\n        )\n        ax_qq.set_title(\"Q-Q plot\")\n\n        # Plot P-P plot\n        self.plot_probability(\n            plot_type=\"PP\",\n            return_period_size=return_period_size,\n            plotting_position=plotting_position,\n            ax=ax_pp,\n        )\n        ax_pp.set_title(\"P-P plot\")\n\n        return fig, (ax_rv, ax_pdf, ax_qq, ax_pp)\n</code></pre>"},{"location":"api/eva/#pyextremes.eva.EVA.plot_extremes","title":"<code>plot_extremes(figsize=(8, 5), ax=None, show_clusters=False)</code>","text":"<p>Plot extreme events.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>Figure size in inches in format (width, height). By default it is (8, 5).</p> <code>(8, 5)</code> <code>ax</code> <code>Axes</code> <p>Axes onto which extremes plot is drawn. If None (default), a new figure and axes objects are created.</p> <code>None</code> <code>show_clusters</code> <code>bool</code> <p>If True, show cluster boundaries for POT extremes. Has no effect if extremes were extracted using BM method. May produce wrong cluster boundaries if extremes were set using the <code>set_extremes</code> or <code>from_extremes</code> methods and threshold and inter-cluster distance (r) arguments were not provided. By default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>figure</code> <code>Figure</code> <p>Figure object.</p> <code>axes</code> <code>Axes</code> <p>Axes object.</p> Source code in <code>src/pyextremes/eva.py</code> <pre><code>def plot_extremes(\n    self,\n    figsize: tuple = (8, 5),\n    ax: typing.Optional[plt.Axes] = None,\n    show_clusters: bool = False,\n) -&gt; typing.Tuple[plt.Figure, plt.Axes]:  # pragma: no cover\n    \"\"\"\n    Plot extreme events.\n\n    Parameters\n    ----------\n    figsize : tuple, optional\n        Figure size in inches in format (width, height).\n        By default it is (8, 5).\n    ax : matplotlib.axes._axes.Axes, optional\n        Axes onto which extremes plot is drawn.\n        If None (default), a new figure and axes objects are created.\n    show_clusters : bool, optional\n        If True, show cluster boundaries for POT extremes.\n        Has no effect if extremes were extracted using BM method.\n        May produce wrong cluster boundaries if extremes were set using the\n        `set_extremes` or `from_extremes` methods and threshold and inter-cluster\n        distance (r) arguments were not provided.\n        By default is False.\n\n    Returns\n    -------\n    figure : matplotlib.figure.Figure\n        Figure object.\n    axes : matplotlib.axes._axes.Axes\n        Axes object.\n\n    \"\"\"\n    return plot_extremes(\n        ts=self.data,\n        extremes=self.extremes,\n        extremes_method=self.extremes_method,\n        extremes_type=self.extremes_type,\n        block_size=self.extremes_kwargs.get(\"block_size\", None),\n        threshold=self.extremes_kwargs.get(\"threshold\", None),\n        r=self.extremes_kwargs.get(\"r\", None) if show_clusters else None,\n        figsize=figsize,\n        ax=ax,\n    )\n</code></pre>"},{"location":"api/eva/#pyextremes.eva.EVA.plot_probability","title":"<code>plot_probability(plot_type, return_period_size='365.2425D', plotting_position='weibull', ax=None, figsize=(8, 8))</code>","text":"<p>Plot a probability plot (QQ or PP).</p> <p>Parameters:</p> Name Type Description Default <code>plot_type</code> <code>str</code> <p>Probability plot type. Supported values:     PP - probability plot     QQ - quantile plot</p> required <code>return_period_size</code> <code>str or Timedelta</code> <p>Size of return periods (default='365.2425D'). If set to '30D', then a return period of 12 would be roughly equivalent to a 1 year return period (360 days).</p> <code>'365.2425D'</code> <code>plotting_position</code> <code>str</code> <p>Plotting position name (default='weibull'), not case-sensitive. Supported plotting positions:     ecdf, hazen, weibull, tukey, blom, median, cunnane, gringorten, beard</p> <code>'weibull'</code> <code>ax</code> <code>Axes</code> <p>Axes onto which the probability plot is drawn. If None (default), a new figure and axes objects are created.</p> <code>None</code> <code>figsize</code> <code>tuple</code> <p>Figure size in inches in format (width, height). By default it is (8, 8).</p> <code>(8, 8)</code> <p>Returns:</p> Name Type Description <code>figure</code> <code>Figure</code> <p>Figure object.</p> <code>axes</code> <code>Axes</code> <p>Axes object.</p> Source code in <code>src/pyextremes/eva.py</code> <pre><code>def plot_probability(\n    self,\n    plot_type: str,\n    return_period_size: typing.Union[str, pd.Timedelta] = \"365.2425D\",\n    plotting_position: typing.Literal[\n        \"ecdf\",\n        \"hazen\",\n        \"weibull\",\n        \"tukey\",\n        \"blom\",\n        \"median\",\n        \"cunnane\",\n        \"gringorten\",\n        \"beard\",\n    ] = \"weibull\",\n    ax: typing.Optional[plt.Axes] = None,\n    figsize: typing.Tuple[float, float] = (8, 8),\n) -&gt; tuple:  # pragma: no cover\n    \"\"\"\n    Plot a probability plot (QQ or PP).\n\n    Parameters\n    ----------\n    plot_type : str\n        Probability plot type.\n        Supported values:\n            PP - probability plot\n            QQ - quantile plot\n    return_period_size : str or pandas.Timedelta, optional\n        Size of return periods (default='365.2425D').\n        If set to '30D', then a return period of 12\n        would be roughly equivalent to a 1 year return period (360 days).\n    plotting_position : str, optional\n        Plotting position name (default='weibull'), not case-sensitive.\n        Supported plotting positions:\n            ecdf, hazen, weibull, tukey, blom, median, cunnane, gringorten, beard\n    ax : matplotlib.axes._axes.Axes, optional\n        Axes onto which the probability plot is drawn.\n        If None (default), a new figure and axes objects are created.\n    figsize : tuple, optional\n        Figure size in inches in format (width, height).\n        By default it is (8, 8).\n\n    Returns\n    -------\n    figure : matplotlib.figure.Figure\n        Figure object.\n    axes : matplotlib.axes._axes.Axes\n        Axes object.\n\n    \"\"\"\n    # Get observed return values\n    observed_return_values = get_return_periods(\n        ts=self.data,\n        extremes=self.extremes,\n        extremes_method=self.extremes_method,\n        extremes_type=self.extremes_type,\n        block_size=self.extremes_kwargs.get(\"block_size\", None),\n        return_period_size=return_period_size,\n        plotting_position=plotting_position,\n    )\n\n    # Get observed and theoretical values\n    # depending on 'plot_type'\n    if plot_type == \"PP\":\n        observed = (\n            1 - observed_return_values.loc[:, \"exceedance probability\"].values\n        )\n        theoretical = self.model.cdf(\n            self.extremes_transformer.transform(\n                observed_return_values.loc[:, self.extremes.name].values\n            )\n        )\n    elif plot_type == \"QQ\":\n        observed = observed_return_values.loc[:, self.extremes.name].values\n        theoretical = self.extremes_transformer.transform(\n            self.model.isf(\n                observed_return_values.loc[:, \"exceedance probability\"].values\n            )\n        )\n    else:\n        raise ValueError(\n            f\"invalid value in '{plot_type}' for the 'plot_type' argument, \"\n            f\"available values: PP, QQ\"\n        )\n\n    # Plot the probability plot\n    return plot_probability(\n        observed=observed,\n        theoretical=theoretical,\n        ax=ax,\n        figsize=figsize,\n    )\n</code></pre>"},{"location":"api/eva/#pyextremes.eva.EVA.plot_return_values","title":"<code>plot_return_values(return_period=None, return_period_size='365.2425D', alpha=None, plotting_position='weibull', ax=None, figsize=(8, 5), **kwargs)</code>","text":"<p>Plot return values and confidence intervals for given return periods.</p> <p>Parameters:</p> Name Type Description Default <code>return_period</code> <code>array - like</code> <p>Return period or 1D array of return periods. Given as a multiple of <code>return_period_size</code>. If None (default), calculates as 100 values uniformly spaced within the range of return periods of the extracted extreme values.</p> <code>None</code> <code>return_period_size</code> <code>str or Timedelta</code> <p>Size of return periods (default='365.2425D'). If set to '30D', then a return period of 12 would be roughly equivalent to a 1 year return period (360 days).</p> <code>'365.2425D'</code> <code>alpha</code> <code>float</code> <p>Width of confidence interval (0, 1). If None (default), confidence interval bounds are not plotted.</p> <code>None</code> <code>plotting_position</code> <code>str</code> <p>Plotting position name (default='weibull'), not case-sensitive. Supported plotting positions:     ecdf, hazen, weibull, tukey, blom, median, cunnane, gringorten, beard</p> <code>'weibull'</code> <code>ax</code> <code>Axes</code> <p>Axes onto which the return value plot is drawn. If None (default), a new figure and axes objects are created.</p> <code>None</code> <code>figsize</code> <code>tuple</code> <p>Figure size in inches in format (width, height). By default it is (8, 5).</p> <code>(8, 5)</code> <code>kwargs</code> <p>Model-specific keyword arguments. If alpha is None, keyword arguments are ignored (error still raised for unrecognized arguments). MLE model:     n_samples : int, optional         Number of bootstrap samples used to estimate         confidence interval bounds (default=100). Emcee model:     burn_in : int         Burn-in value (number of first steps to discard for each walker).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>figure</code> <code>Figure</code> <p>Figure object.</p> <code>axes</code> <code>Axes</code> <p>Axes object.</p> Source code in <code>src/pyextremes/eva.py</code> <pre><code>def plot_return_values(\n    self,\n    return_period=None,\n    return_period_size: typing.Union[str, pd.Timedelta] = \"365.2425D\",\n    alpha: typing.Optional[float] = None,\n    plotting_position: typing.Literal[\n        \"ecdf\",\n        \"hazen\",\n        \"weibull\",\n        \"tukey\",\n        \"blom\",\n        \"median\",\n        \"cunnane\",\n        \"gringorten\",\n        \"beard\",\n    ] = \"weibull\",\n    ax: typing.Optional[plt.Axes] = None,\n    figsize: typing.Tuple[float, float] = (8, 5),\n    **kwargs,\n) -&gt; tuple:  # pragma: no cover\n    \"\"\"\n    Plot return values and confidence intervals for given return periods.\n\n    Parameters\n    ----------\n    return_period : array-like, optional\n        Return period or 1D array of return periods.\n        Given as a multiple of `return_period_size`.\n        If None (default), calculates as 100 values uniformly spaced\n        within the range of return periods of the extracted extreme values.\n    return_period_size : str or pandas.Timedelta, optional\n        Size of return periods (default='365.2425D').\n        If set to '30D', then a return period of 12\n        would be roughly equivalent to a 1 year return period (360 days).\n    alpha : float, optional\n        Width of confidence interval (0, 1).\n        If None (default), confidence interval bounds are not plotted.\n    plotting_position : str, optional\n        Plotting position name (default='weibull'), not case-sensitive.\n        Supported plotting positions:\n            ecdf, hazen, weibull, tukey, blom, median, cunnane, gringorten, beard\n    ax : matplotlib.axes._axes.Axes, optional\n        Axes onto which the return value plot is drawn.\n        If None (default), a new figure and axes objects are created.\n    figsize : tuple, optional\n        Figure size in inches in format (width, height).\n        By default it is (8, 5).\n    kwargs\n        Model-specific keyword arguments.\n        If alpha is None, keyword arguments are ignored\n        (error still raised for unrecognized arguments).\n        MLE model:\n            n_samples : int, optional\n                Number of bootstrap samples used to estimate\n                confidence interval bounds (default=100).\n        Emcee model:\n            burn_in : int\n                Burn-in value (number of first steps to discard for each walker).\n\n    Returns\n    -------\n    figure : matplotlib.figure.Figure\n        Figure object.\n    axes : matplotlib.axes._axes.Axes\n        Axes object.\n\n    \"\"\"\n    # Get observed return values\n    observed_return_values = get_return_periods(\n        ts=self.data,\n        extremes=self.extremes,\n        extremes_method=self.extremes_method,\n        extremes_type=self.extremes_type,\n        block_size=self.extremes_kwargs.get(\"block_size\", None),\n        return_period_size=return_period_size,\n        plotting_position=plotting_position,\n    )\n\n    # Parse the 'return_period' argument\n    if return_period is None:\n        return_period = np.linspace(\n            observed_return_values.loc[:, \"return period\"].min(),\n            observed_return_values.loc[:, \"return period\"].max(),\n            100,\n        )\n    else:\n        # Convert 'return_period' to ndarray\n        return_period = np.asarray(a=return_period, dtype=np.float64).copy()\n        if return_period.ndim == 0:\n            return_period = return_period[np.newaxis]\n        if return_period.ndim != 1:\n            raise ValueError(\n                f\"invalid shape in {return_period.shape} \"\n                f\"for the 'return_period' argument, must be 1D array\"\n            )\n        if len(return_period) &lt; 2:\n            raise ValueError(\n                f\"'return_period' must have at least 2 return periods, \"\n                f\"{len(return_period)} was given\"\n            )\n\n    # Get modeled return values\n    modeled_return_values = self.get_summary(\n        return_period=return_period,\n        return_period_size=return_period_size,\n        alpha=alpha,\n        **kwargs,\n    )\n\n    # Plot return values\n    return plot_return_values(\n        observed_return_values=observed_return_values,\n        modeled_return_values=modeled_return_values,\n        ax=ax,\n        figsize=figsize,\n    )\n</code></pre>"},{"location":"api/eva/#pyextremes.eva.EVA.plot_trace","title":"<code>plot_trace(burn_in=0, labels=None, figsize=None)</code>","text":"<p>Plot trace plot for MCMC sampler trace.</p> <p>Parameters:</p> Name Type Description Default <code>burn_in</code> <code>int</code> <p>Burn-in value (number of first steps to discard for each walker). By default it is 0 (no values are discarded).</p> <code>0</code> <code>labels</code> <code>array - like</code> <p>Sequence of strings with parameter names, used to label axes. If None (default), then axes are labeled sequentially.</p> <code>None</code> <code>figsize</code> <code>tuple</code> <p>Figure size in inches. If None (default), then figure size is calculated automatically as 8 by 2 times number of parameters.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>figure</code> <code>Figure</code> <p>Figure object.</p> <code>axes</code> <code>list</code> <p>List with n_parameters Axes objects.</p> Source code in <code>src/pyextremes/eva.py</code> <pre><code>def plot_trace(\n    self,\n    burn_in: int = 0,\n    labels: typing.Optional[typing.List[str]] = None,\n    figsize: typing.Optional[typing.Tuple[float, float]] = None,\n) -&gt; typing.Tuple[plt.Figure, list]:  # pragma: no cover\n    \"\"\"\n    Plot trace plot for MCMC sampler trace.\n\n    Parameters\n    ----------\n    burn_in : int, optional\n        Burn-in value (number of first steps to discard for each walker).\n        By default it is 0 (no values are discarded).\n    labels : array-like, optional\n        Sequence of strings with parameter names, used to label axes.\n        If None (default), then axes are labeled sequentially.\n    figsize : tuple, optional\n        Figure size in inches.\n        If None (default), then figure size is calculated automatically\n        as 8 by 2 times number of parameters.\n\n    Returns\n    -------\n    figure : matplotlib.figure.Figure\n        Figure object.\n    axes : list\n        List with n_parameters Axes objects.\n\n    \"\"\"\n    trace, trace_map, labels = self._get_mcmc_plot_inputs(labels=labels)\n    return plot_trace(\n        trace=trace,\n        trace_map=trace_map,\n        burn_in=burn_in,\n        labels=labels,\n        figsize=figsize,\n    )\n</code></pre>"},{"location":"api/eva/#pyextremes.eva.EVA.set_extremes","title":"<code>set_extremes(extremes, method='BM', extremes_type='high', **kwargs)</code>","text":"<p>Set extreme values.</p> <p>This method is used to set extreme values onto the model instead of deriving them from data directly using the 'get_extremes' method. This way user can set extremes calculated using a custom methodology.</p> <p>Parameters:</p> Name Type Description Default <code>extremes</code> <code>Series</code> <p>Time series of extreme values to be set onto the model. Must be numeric, have date-time index, and have the same name as self.data.</p> required <code>method</code> <code>str</code> <p>Extreme value extraction method. Supported values:     BM (default) - Block Maxima     POT - Peaks Over Threshold</p> <code>'BM'</code> <code>extremes_type</code> <code>str</code> <p>high (default) - extreme high values low - extreme low values</p> <code>'high'</code> <code>kwargs</code> <p>if method is BM:     block_size : str or pandas.Timedelta, optional         Block size.         If None (default), then is calculated as median distance         between extreme events.     errors : str, optional         raise - raise an exception             when encountering a block with no data         ignore (default) - ignore blocks with no data         coerce - get extreme values for blocks with no data             as mean of all other extreme events in the series             with index being the middle point of corresponding interval     min_last_block : float, optional         Minimum data availability ratio (0 to 1) in the last block         for it to be used to extract extreme value from.         This is used to discard last block when it is too short.         If None (default), last block is always used. if method is POT:     threshold : float, optional         Threshold used to find exceedances.         By default is taken as smallest value.     r : pandas.Timedelta or value convertible to timedelta, optional         Duration of window used to decluster the exceedances.         By default r='24H' (24 hours).         See pandas.to_timedelta for more information.</p> <code>{}</code> Source code in <code>src/pyextremes/eva.py</code> <pre><code>def set_extremes(\n    self,\n    extremes: pd.Series,\n    method: typing.Literal[\"BM\", \"POT\"] = \"BM\",\n    extremes_type: typing.Literal[\"high\", \"low\"] = \"high\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Set extreme values.\n\n    This method is used to set extreme values onto the model instead\n    of deriving them from data directly using the 'get_extremes' method.\n    This way user can set extremes calculated using a custom methodology.\n\n    Parameters\n    ----------\n    extremes : pd.Series\n        Time series of extreme values to be set onto the model.\n        Must be numeric, have date-time index, and have the same name\n        as self.data.\n    method : str, optional\n        Extreme value extraction method.\n        Supported values:\n            BM (default) - Block Maxima\n            POT - Peaks Over Threshold\n    extremes_type : str, optional\n        high (default) - extreme high values\n        low - extreme low values\n    kwargs:\n        if method is BM:\n            block_size : str or pandas.Timedelta, optional\n                Block size.\n                If None (default), then is calculated as median distance\n                between extreme events.\n            errors : str, optional\n                raise - raise an exception\n                    when encountering a block with no data\n                ignore (default) - ignore blocks with no data\n                coerce - get extreme values for blocks with no data\n                    as mean of all other extreme events in the series\n                    with index being the middle point of corresponding interval\n            min_last_block : float, optional\n                Minimum data availability ratio (0 to 1) in the last block\n                for it to be used to extract extreme value from.\n                This is used to discard last block when it is too short.\n                If None (default), last block is always used.\n        if method is POT:\n            threshold : float, optional\n                Threshold used to find exceedances.\n                By default is taken as smallest value.\n            r : pandas.Timedelta or value convertible to timedelta, optional\n                Duration of window used to decluster the exceedances.\n                By default r='24H' (24 hours).\n                See pandas.to_timedelta for more information.\n\n    \"\"\"\n    # Validate `extremes`\n    if not isinstance(extremes, pd.Series):\n        raise TypeError(\n            f\"invalid type in '{type(extremes).__name__}' for the `extremes` \"\n            f\"argument, must be pandas.Series\"\n        )\n    extremes = extremes.copy(deep=True)\n    if not isinstance(extremes.index, pd.DatetimeIndex):\n        raise TypeError(\"invalid index type for `extremes`, must be date-time\")\n    if not np.issubdtype(extremes.dtype, np.number):\n        raise TypeError(\"`extremes` must have numeric values\")\n    if extremes.name is None:\n        extremes.name = self.data.name\n    else:\n        if extremes.name != self.data.name:\n            raise ValueError(\"`extremes` name doesn't match that of `data`\")\n    if (\n        extremes.index.min() &lt; self.data.index.min()\n        or extremes.index.max() &gt; self.data.index.max()\n    ):\n        raise ValueError(\"`extremes` time range must fit within that of data\")\n\n    # Get `method`\n    if method not in [\"BM\", \"POT\"]:\n        raise ValueError(f\"`method` must be either 'BM' or 'POT', not '{method}'\")\n\n    # Get `extremes_type`\n    if extremes_type not in [\"high\", \"low\"]:\n        raise ValueError(\n            f\"`extremes_type` must be either 'BM' or 'POT', not '{extremes_type}'\"\n        )\n\n    # Get `extremes_kwargs`\n    extremes_kwargs = {}\n    if method == \"BM\":\n        # Get `block_size`\n        extremes_kwargs[\"block_size\"] = pd.to_timedelta(\n            kwargs.pop(\n                \"block_size\",\n                pd.to_timedelta(np.quantile(np.diff(extremes.index), 0.5)),\n            )\n        )\n        if extremes_kwargs[\"block_size\"] &lt;= pd.to_timedelta(\"0D\"):\n            raise ValueError(\n                \"`block_size` must be a positive timedelta, not %s\"\n                % extremes_kwargs[\"block_size\"]\n            )\n\n        # Get `errors`\n        extremes_kwargs[\"errors\"] = kwargs.pop(\"errors\", \"ignore\")\n        if extremes_kwargs[\"errors\"] not in [\"raise\", \"ignore\", \"coerce\"]:\n            raise ValueError(\n                f\"invalid value in '{extremes_kwargs['errors']}' \"\n                f\"for the `errors` argument\"\n            )\n\n        # Get `min_last_block`\n        extremes_kwargs[\"min_last_block\"] = kwargs.pop(\"min_last_block\", None)\n        if extremes_kwargs[\"min_last_block\"] is not None:\n            if not 0 &lt;= extremes_kwargs[\"min_last_block\"] &lt;= 1:\n                raise ValueError(\n                    \"`min_last_block` must be a number in the [0, 1] range\"\n                )\n\n    else:\n        # Get `threshold`\n        extremes_kwargs[\"threshold\"] = kwargs.pop(\n            \"threshold\",\n            {\n                \"high\": extremes.min(),\n                \"low\": extremes.max(),\n            }[extremes_type],\n        )\n        if (\n            extremes_type == \"high\"\n            and extremes_kwargs[\"threshold\"] &gt; extremes.values.min()\n        ) or (\n            extremes_type == \"low\"\n            and extremes_kwargs[\"threshold\"] &lt; extremes.values.max()\n        ):\n            raise ValueError(\"invalid `threshold` value\")\n\n        # Get `r`\n        extremes_kwargs[\"r\"] = pd.to_timedelta(kwargs.pop(\"r\", \"24h\"))\n        if extremes_kwargs[\"r\"] &lt;= pd.to_timedelta(\"0D\"):\n            raise ValueError(\n                \"`r` must be a positive timedelta, not %s\" % extremes_kwargs[\"r\"]\n            )\n\n    # Check for unrecognized kwargs\n    if len(kwargs) != 0:\n        raise TypeError(\n            f\"unrecognized arguments passed in: {', '.join(kwargs.keys())}\"\n        )\n\n    # Set attributes\n    self.__extremes = extremes\n    self.__extremes_method = method\n    self.__extremes_type = extremes_type\n    self.__extremes_kwargs = extremes_kwargs\n    self.__extremes_transformer = ExtremesTransformer(\n        extremes=self.__extremes,\n        extremes_type=self.__extremes_type,\n    )\n    self.__model = None\n    logger.info(\"successfully set extremes\")\n</code></pre>"},{"location":"api/extremes/","title":"Extremes","text":""},{"location":"api/extremes/#pyextremes.extremes.get_extremes","title":"<code>pyextremes.extremes.get_extremes(ts, method, extremes_type='high', **kwargs)</code>","text":"<p>Get extreme events from time series.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Series</code> <p>Time series of the signal.</p> required <code>method</code> <code>str</code> <p>Extreme value extraction method. Supported values:     BM - Block Maxima     POT - Peaks Over Threshold</p> required <code>extremes_type</code> <code>str</code> <p>high (default) - get extreme high values low - get extreme low values</p> <code>'high'</code> <code>kwargs</code> <p>if method is BM:     block_size : str or pandas.Timedelta, optional         Block size (default='365.2425D').     errors : str, optional         raise (default) - raise an exception             when encountering a block with no data         ignore - ignore blocks with no data         coerce - get extreme values for blocks with no data             as mean of all other extreme events in the series             with index being the middle point of corresponding interval     min_last_block : float, optional         Minimum data availability ratio (0 to 1) in the last block         for it to be used to extract extreme value from.         This is used to discard last block when it is too short.         If None (default), last block is always used. if method is POT:     threshold : float         Threshold used to find exceedances.     r : pandas.Timedelta or value convertible to timedelta, optional         Duration of window used to decluster the exceedances.         By default r='24H' (24 hours).         See pandas.to_timedelta for more information.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>extremes</code> <code>Series</code> <p>Time series of extreme events.</p> Source code in <code>src/pyextremes/extremes/extremes.py</code> <pre><code>def get_extremes(\n    ts: pd.Series,\n    method: Literal[\"BM\", \"POT\"],\n    extremes_type: Literal[\"high\", \"low\"] = \"high\",\n    **kwargs,\n) -&gt; pd.Series:\n    \"\"\"\n    Get extreme events from time series.\n\n    Parameters\n    ----------\n    ts : pandas.Series\n        Time series of the signal.\n    method : str\n        Extreme value extraction method.\n        Supported values:\n            BM - Block Maxima\n            POT - Peaks Over Threshold\n    extremes_type : str, optional\n        high (default) - get extreme high values\n        low - get extreme low values\n    kwargs\n        if method is BM:\n            block_size : str or pandas.Timedelta, optional\n                Block size (default='365.2425D').\n            errors : str, optional\n                raise (default) - raise an exception\n                    when encountering a block with no data\n                ignore - ignore blocks with no data\n                coerce - get extreme values for blocks with no data\n                    as mean of all other extreme events in the series\n                    with index being the middle point of corresponding interval\n            min_last_block : float, optional\n                Minimum data availability ratio (0 to 1) in the last block\n                for it to be used to extract extreme value from.\n                This is used to discard last block when it is too short.\n                If None (default), last block is always used.\n        if method is POT:\n            threshold : float\n                Threshold used to find exceedances.\n            r : pandas.Timedelta or value convertible to timedelta, optional\n                Duration of window used to decluster the exceedances.\n                By default r='24H' (24 hours).\n                See pandas.to_timedelta for more information.\n\n    Returns\n    -------\n    extremes : pandas.Series\n        Time series of extreme events.\n\n    \"\"\"\n    if method == \"BM\":\n        return get_extremes_block_maxima(\n            ts=ts,\n            extremes_type=extremes_type,\n            **kwargs,\n        )\n    if method == \"POT\":\n        return get_extremes_peaks_over_threshold(\n            ts=ts,\n            extremes_type=extremes_type,\n            **kwargs,\n        )\n    raise ValueError(\n        f\"invalid value in '{method}' for the 'method' argument, \"\n        f\"available methods: 'BM', 'POT'\"\n    )\n</code></pre>"},{"location":"api/extremes/#pyextremes.extremes.get_return_periods","title":"<code>pyextremes.extremes.get_return_periods(ts, extremes, extremes_method, extremes_type, block_size=None, return_period_size='365.2425D', plotting_position='weibull')</code>","text":"<p>Calculate return periods for given extreme values using given plotting position.</p> <p>Return periods are multiples of <code>return_period_size</code>. Plotting positions were taken from https://matplotlib.org/mpl-probscale/tutorial/closer_look_at_plot_pos.html</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Series</code> <p>Time series of the signal.</p> required <code>extremes</code> <code>Series</code> <p>Time series of extreme events.</p> required <code>extremes_method</code> <code>str</code> <p>Extreme value extraction method. Supported values:     BM - Block Maxima     POT - Peaks Over Threshold</p> required <code>extremes_type</code> <code>str</code> <p>high - provided extreme values are extreme high values low - provided extreme values are extreme low values</p> required <code>block_size</code> <code>str or Timedelta</code> <p>Block size in the 'BM' <code>extremes_method</code> (default=None). If None, then is calculated as median distance between extreme events.</p> <code>None</code> <code>return_period_size</code> <code>str or Timedelta</code> <p>Size of return periods (default='365.2425D'). If set to '30D', then a return period of 12 would be roughly equivalent to a 1 year return period (360 days).</p> <code>'365.2425D'</code> <code>plotting_position</code> <code>str</code> <p>Plotting position name (default='weibull'), not case-sensitive. Supported plotting positions:     ecdf, hazen, weibull, tukey, blom, median, cunnane, gringorten, beard</p> <code>'weibull'</code> <p>Returns:</p> Name Type Description <code>extreme_events</code> <code>DataFrame</code> <p>A DataFrame with extreme values, exceedance probabilities, and return periods as multiples of <code>return_period_size</code>.</p> Source code in <code>src/pyextremes/extremes/return_periods.py</code> <pre><code>def get_return_periods(\n    ts: pd.Series,\n    extremes: pd.Series,\n    extremes_method: Literal[\"BM\", \"POT\"],\n    extremes_type: Literal[\"high\", \"low\"],\n    block_size: Optional[Union[str, pd.Timedelta]] = None,\n    return_period_size: Union[str, pd.Timedelta] = \"365.2425D\",\n    plotting_position: Literal[\n        \"ecdf\",\n        \"hazen\",\n        \"weibull\",\n        \"tukey\",\n        \"blom\",\n        \"median\",\n        \"cunnane\",\n        \"gringorten\",\n        \"beard\",\n    ] = \"weibull\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate return periods for given extreme values using given plotting position.\n\n    Return periods are multiples of `return_period_size`.\n    Plotting positions were taken from\n    https://matplotlib.org/mpl-probscale/tutorial/closer_look_at_plot_pos.html\n\n    Parameters\n    ----------\n    ts : pandas.Series\n        Time series of the signal.\n    extremes : pandas.Series\n        Time series of extreme events.\n    extremes_method : str\n        Extreme value extraction method.\n        Supported values:\n            BM - Block Maxima\n            POT - Peaks Over Threshold\n    extremes_type : str\n        high - provided extreme values are extreme high values\n        low - provided extreme values are extreme low values\n    block_size : str or pandas.Timedelta, optional\n        Block size in the 'BM' `extremes_method` (default=None).\n        If None, then is calculated as median distance between extreme events.\n    return_period_size : str or pandas.Timedelta, optional\n        Size of return periods (default='365.2425D').\n        If set to '30D', then a return period of 12\n        would be roughly equivalent to a 1 year return period (360 days).\n    plotting_position : str, optional\n        Plotting position name (default='weibull'), not case-sensitive.\n        Supported plotting positions:\n            ecdf, hazen, weibull, tukey, blom, median, cunnane, gringorten, beard\n\n    Returns\n    -------\n    extreme_events : pandas.DataFrame\n        A DataFrame with extreme values, exceedance probabilities,\n        and return periods as multiples of `return_period_size`.\n\n    \"\"\"\n    if extremes_method == \"BM\":\n        # Parse 'block_size' argument\n        if block_size is None:\n            # Calculate 'block_size' as median distance between extremes\n            block_size = pd.to_timedelta(np.quantile(np.diff(extremes.index), 0.5))\n        else:\n            if not isinstance(block_size, pd.Timedelta):\n                if isinstance(block_size, str):\n                    block_size = pd.to_timedelta(block_size)\n                else:\n                    raise TypeError(\n                        f\"invalid type in {type(block_size)} \"\n                        f\"for the 'block_size' argument\"\n                    )\n    else:\n        if block_size is not None:\n            raise ValueError(\n                f\"'block_size' value is used only if 'extremes_method' is 'BM', \"\n                f\"provided 'extremes_method' is {extremes_method}\"\n            )\n\n    # Parse the 'return_period_size' argument\n    if not isinstance(return_period_size, pd.Timedelta):\n        if isinstance(return_period_size, str):\n            return_period_size = pd.to_timedelta(return_period_size)\n        else:\n            raise TypeError(\n                f\"invalid type in {type(return_period_size)} \"\n                f\"for the 'return_period_size' argument\"\n            )\n\n    # Calculate rate of extreme events as number of events per one return period\n    if extremes_method == \"BM\":\n        extremes_rate = return_period_size / block_size\n    elif extremes_method == \"POT\":\n        n_periods = (ts.index.max() - ts.index.min()) / return_period_size\n        extremes_rate = len(extremes) / n_periods\n    else:\n        raise ValueError(\n            f\"invalid value in '{extremes_method}' for the 'extremes_method' argument\"\n        )\n\n    # Rank extreme values from most extreme (1) to least extreme (len(extremes))\n    if extremes_type == \"high\":\n        ranks = (\n            len(extremes) + 1 - scipy.stats.rankdata(extremes.values, method=\"average\")\n        )\n    elif extremes_type == \"low\":\n        ranks = scipy.stats.rankdata(extremes.values, method=\"average\")\n    else:\n        raise ValueError(\n            f\"invalid value in '{extremes_type}' for the 'extremes_type' argument\"\n        )\n\n    # Get plotting position parameters\n    try:\n        alpha, beta = plotting_positions[plotting_position.lower()]\n    except KeyError as _error:\n        raise ValueError(\n            f\"invalid value in '{plotting_position}' \"\n            f\"for the 'plotting_position' argument\"\n        ) from _error\n\n    # Calculate exceedance probabilities\n    exceedance_probability = (ranks - alpha) / (len(extremes) + 1 - alpha - beta)\n\n    # Calculate return periods\n    return_periods = 1 / exceedance_probability / extremes_rate\n\n    # Copy `extremes` to make the returned DataFrame independent from the original\n    extremes = extremes.copy(deep=True)\n\n    return pd.DataFrame(\n        data={\n            extremes.name: extremes.values,\n            \"exceedance probability\": exceedance_probability,\n            \"return period\": return_periods,\n        },\n        index=extremes.index,\n        dtype=np.float64,\n    )\n</code></pre>"},{"location":"api/models/","title":"Models","text":""},{"location":"api/models/#pyextremes.models.models.get_model","title":"<code>pyextremes.models.models.get_model(model, extremes, distribution, distribution_kwargs=None, **kwargs)</code>","text":"<p>Get distribution fitting model and fit it to given extreme values.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name of model. Supported models:     MLE - Maximum Likelihood Estimate (MLE) model.         Based on 'scipy' package (scipy.stats.rv_continuous.fit).     Emcee - Markov Chain Monte Carlo (MCMC) model.         Based on 'emcee' package by Daniel Foreman-Mackey.</p> required <code>extremes</code> <code>Series</code> <p>Time series of extreme events.</p> required <code>distribution</code> <code>str or rv_continuous</code> <p>Distribution name compatible with scipy.stats or a subclass of scipy.stats.rv_continuous. See https://docs.scipy.org/doc/scipy/reference/stats.html</p> required <code>distribution_kwargs</code> <code>dict</code> <p>Special keyword arguments, passed to the <code>.fit</code> method of the distribution. These keyword arguments represent parameters to be held fixed. Names of parameters to be fixed must have 'f' prefixes. Valid parameters:     - shape(s): 'fc', e.g. fc=0     - location: 'floc', e.g. floc=0     - scale: 'fscale', e.g. fscale=1 By default, no parameters are fixed. See documentation of a specific scipy.stats distribution for names of available parameters.</p> <code>None</code> <code>kwargs</code> <p>Keyword arguments passed to a model .fit method. MLE model:     MLE model takes no additional arguments. Emcee model:     n_walkers : int, optional         The number of walkers in the ensemble (default=100).     n_samples : int, optional         The number of steps to run (default=500).     progress : bool or str, optional         If True, a progress bar will be shown as the sampler progresses.         If a string, will select a specific tqdm progress bar.         Most notable is 'notebook', which shows a progress bar         suitable for Jupyter notebooks.         If False (default), no progress bar will be shown.         This progress bar is a part of the <code>emcee</code> package.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>model</code> <code>MLE or Emcee</code> <p>Distribution fitting model fitted to the <code>extremes</code>.</p> Source code in <code>src/pyextremes/models/models.py</code> <pre><code>def get_model(\n    model: Literal[\"MLE\", \"Emcee\"],\n    extremes: pd.Series,\n    distribution: Union[str, scipy.stats.rv_continuous],\n    distribution_kwargs: Optional[dict] = None,\n    **kwargs,\n) -&gt; Union[MLE, Emcee]:\n    \"\"\"\n    Get distribution fitting model and fit it to given extreme values.\n\n    Parameters\n    ----------\n    model : str\n        Name of model.\n        Supported models:\n            MLE - Maximum Likelihood Estimate (MLE) model.\n                Based on 'scipy' package (scipy.stats.rv_continuous.fit).\n            Emcee - Markov Chain Monte Carlo (MCMC) model.\n                Based on 'emcee' package by Daniel Foreman-Mackey.\n    extremes : pandas.Series\n        Time series of extreme events.\n    distribution : str or scipy.stats.rv_continuous\n        Distribution name compatible with scipy.stats\n        or a subclass of scipy.stats.rv_continuous.\n        See https://docs.scipy.org/doc/scipy/reference/stats.html\n    distribution_kwargs : dict, optional\n        Special keyword arguments, passed to the `.fit` method of the distribution.\n        These keyword arguments represent parameters to be held fixed.\n        Names of parameters to be fixed must have 'f' prefixes. Valid parameters:\n            - shape(s): 'fc', e.g. fc=0\n            - location: 'floc', e.g. floc=0\n            - scale: 'fscale', e.g. fscale=1\n        By default, no parameters are fixed.\n        See documentation of a specific scipy.stats distribution\n        for names of available parameters.\n    kwargs\n        Keyword arguments passed to a model .fit method.\n        MLE model:\n            MLE model takes no additional arguments.\n        Emcee model:\n            n_walkers : int, optional\n                The number of walkers in the ensemble (default=100).\n            n_samples : int, optional\n                The number of steps to run (default=500).\n            progress : bool or str, optional\n                If True, a progress bar will be shown as the sampler progresses.\n                If a string, will select a specific tqdm progress bar.\n                Most notable is 'notebook', which shows a progress bar\n                suitable for Jupyter notebooks.\n                If False (default), no progress bar will be shown.\n                This progress bar is a part of the `emcee` package.\n\n    Returns\n    -------\n    model : MLE or Emcee\n        Distribution fitting model fitted to the `extremes`.\n\n    \"\"\"\n    distribution_model_kwargs = {\n        \"extremes\": extremes,\n        \"distribution\": distribution,\n        \"distribution_kwargs\": distribution_kwargs,\n        **kwargs,\n    }\n\n    if model == \"MLE\":\n        return MLE(**distribution_model_kwargs)\n    if model == \"Emcee\":\n        return Emcee(**distribution_model_kwargs)\n    raise ValueError(\n        f\"invalid value in '{model}' for the 'model' argument, \"\n        f\"available model: 'MLE', 'Emcee'\"\n    )\n</code></pre>"},{"location":"api/models/#pyextremes.models.model_base.AbstractModelBaseClass","title":"<code>pyextremes.models.model_base.AbstractModelBaseClass</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>src/pyextremes/models/model_base.py</code> <pre><code>class AbstractModelBaseClass(abc.ABC):\n    def __init__(\n        self,\n        extremes: pd.Series,\n        distribution: typing.Union[str, scipy.stats.rv_continuous],\n        distribution_kwargs: typing.Optional[dict] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the model.\n\n        Parameters\n        ----------\n        extremes : pandas.Series\n            Time series of extreme events.\n        distribution : str or scipy.stats.rv_continuous\n            Distribution name compatible with scipy.stats\n            or a subclass of scipy.stats.rv_continuous.\n            See https://docs.scipy.org/doc/scipy/reference/stats.html\n        distribution_kwargs : dict, optional\n            Special keyword arguments, passed to the `.fit` method of the distribution.\n            These keyword arguments represent parameters to be held fixed.\n            Names of parameters to be fixed must have 'f' prefixes. Valid parameters:\n                - shape(s): 'fc', e.g. fc=0\n                - location: 'floc', e.g. floc=0\n                - scale: 'fscale', e.g. fscale=1\n            By default, no parameters are fixed.\n            See documentation of a specific scipy.stats distribution\n            for names of available parameters.\n        kwargs\n            Keyword arguments passed to a model .fit method.\n            MLE model:\n                MLE model takes no additional arguments.\n            Emcee model:\n                n_walkers : int, optional\n                    The number of walkers in the ensemble (default=100).\n                n_samples : int, optional\n                    The number of steps to run (default=500).\n                progress : bool or str, optional\n                    If True, a progress bar will be shown as the sampler progresses.\n                    If a string, will select a specific tqdm progress bar.\n                    Most notable is 'notebook', which shows a progress bar\n                    suitable for Jupyter notebooks.\n                    If False (default), no progress bar will be shown.\n                    This progress bar is a part of the `emcee` package.\n\n        \"\"\"\n        self.extremes = extremes\n\n        # Declare extreme value distribution\n        distribution_kwargs = distribution_kwargs or {}\n        self.distribution = Distribution(\n            extremes=self.extremes, distribution=distribution, **distribution_kwargs\n        )\n\n        # Fit the distribution to extremes\n        self._fit_parameters: typing.Optional[dict] = None\n        self._trace: typing.Optional[np.ndarray] = None\n        self.fit(**kwargs)\n\n        # Initialize 'return_value_cache'\n        self.return_value_cache: typing.Dict[tuple, tuple] = {}\n\n    @property\n    @abc.abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"Return model name.\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def fit(self, **kwargs) -&gt; None:\n        \"\"\"\n        Set values for self.fit_parameters and self.trace.\n\n        self.trace is set only for MCMC-like models.\n        self.fit_parameters is a dictionary with {parameter_name: value},\n        e.g. {'c': 0.1, 'loc': -7, 'scale': 0.3}\n        self.trace is a numpy.ndarray with shape of\n        (n_walkers, n_samples, n_free_parameters)\n\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def fit_parameters(self) -&gt; typing.Dict[str, float]:\n        if self._fit_parameters is None:\n            raise AssertionError\n        else:\n            return self._fit_parameters\n\n    @property\n    def trace(self) -&gt; np.ndarray:\n        if self._trace is None:\n            raise TypeError(f\"trace property is not applicable for '{self.name}' model\")\n        else:\n            return self._trace\n\n    @property\n    def loglikelihood(self) -&gt; float:\n        return np.sum(self.logpdf(x=self.extremes.values))\n\n    @property\n    def AIC(self) -&gt; float:\n        \"\"\"\n        Return corrected Akaike Information Criterion (AIC) of the model.\n\n        Smaller AIC value corresponds to better model.\n        This formula scales well for small sample sizes.\n        See https://en.wikipedia.org/wiki/Akaike_information_criterion\n\n        \"\"\"\n        k = self.distribution.number_of_parameters\n        n = len(self.extremes)\n        aic = 2 * k - 2 * self.loglikelihood\n        correction = (2 * k**2 + 2 * k) / (n - k - 1)\n        return aic + correction\n\n    @abc.abstractmethod\n    def get_return_value(\n        self,\n        exceedance_probability: float,\n        alpha: typing.Optional[float] = None,\n        **kwargs,\n    ) -&gt; tuple:\n        \"\"\"\n        Calculate return value and confidence interval bounds.\n\n        Parameters\n        ----------\n        exceedance_probability : array-like\n            Exceedance probability or 1D array of exceedance probabilities.\n            Each exceedance probability must be in the [0, 1) range.\n        alpha : float, optional\n            Width of confidence interval (0, 1).\n            If None (default), return None\n            for upper and lower confidence interval bounds.\n        kwargs\n            Model-specific keyword arguments.\n            If alpha is None, keyword arguments are ignored\n            (error still raised for unrecognized arguments).\n            MLE model:\n                n_samples : int, optional\n                    Number of bootstrap samples used to estimate\n                    confidence interval bounds (default=100).\n            Emcee model:\n                burn_in : int\n                    Burn-in value (number of first steps to discard for each walker).\n\n        Returns\n        -------\n        return_value : array-like\n            Return values.\n        ci_lower : array-like\n            Lower confidence interval bounds.\n        ci_upper : array-like\n            Upper confidence interval bounds.\n\n        \"\"\"\n        raise NotImplementedError\n\n    def _get_prop(self, prop: str, x):\n        return self.distribution.get_prop(\n            prop=prop, x=x, free_parameters=self.fit_parameters\n        )\n\n    def pdf(self, x):\n        return self._get_prop(prop=\"pdf\", x=x)\n\n    def logpdf(self, x):\n        return self._get_prop(prop=\"logpdf\", x=x)\n\n    def cdf(self, x):\n        return self._get_prop(prop=\"cdf\", x=x)\n\n    def ppf(self, x):\n        return self._get_prop(prop=\"ppf\", x=x)\n\n    def isf(self, x):\n        return self._get_prop(prop=\"isf\", x=x)\n</code></pre>"},{"location":"api/models/#pyextremes.models.model_base.AbstractModelBaseClass.AIC","title":"<code>AIC: float</code>  <code>property</code>","text":"<p>Return corrected Akaike Information Criterion (AIC) of the model.</p> <p>Smaller AIC value corresponds to better model. This formula scales well for small sample sizes. See https://en.wikipedia.org/wiki/Akaike_information_criterion</p>"},{"location":"api/models/#pyextremes.models.model_base.AbstractModelBaseClass.name","title":"<code>name: str</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return model name.</p>"},{"location":"api/models/#pyextremes.models.model_base.AbstractModelBaseClass.__init__","title":"<code>__init__(extremes, distribution, distribution_kwargs=None, **kwargs)</code>","text":"<p>Initialize the model.</p> <p>Parameters:</p> Name Type Description Default <code>extremes</code> <code>Series</code> <p>Time series of extreme events.</p> required <code>distribution</code> <code>str or rv_continuous</code> <p>Distribution name compatible with scipy.stats or a subclass of scipy.stats.rv_continuous. See https://docs.scipy.org/doc/scipy/reference/stats.html</p> required <code>distribution_kwargs</code> <code>dict</code> <p>Special keyword arguments, passed to the <code>.fit</code> method of the distribution. These keyword arguments represent parameters to be held fixed. Names of parameters to be fixed must have 'f' prefixes. Valid parameters:     - shape(s): 'fc', e.g. fc=0     - location: 'floc', e.g. floc=0     - scale: 'fscale', e.g. fscale=1 By default, no parameters are fixed. See documentation of a specific scipy.stats distribution for names of available parameters.</p> <code>None</code> <code>kwargs</code> <p>Keyword arguments passed to a model .fit method. MLE model:     MLE model takes no additional arguments. Emcee model:     n_walkers : int, optional         The number of walkers in the ensemble (default=100).     n_samples : int, optional         The number of steps to run (default=500).     progress : bool or str, optional         If True, a progress bar will be shown as the sampler progresses.         If a string, will select a specific tqdm progress bar.         Most notable is 'notebook', which shows a progress bar         suitable for Jupyter notebooks.         If False (default), no progress bar will be shown.         This progress bar is a part of the <code>emcee</code> package.</p> <code>{}</code> Source code in <code>src/pyextremes/models/model_base.py</code> <pre><code>def __init__(\n    self,\n    extremes: pd.Series,\n    distribution: typing.Union[str, scipy.stats.rv_continuous],\n    distribution_kwargs: typing.Optional[dict] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Initialize the model.\n\n    Parameters\n    ----------\n    extremes : pandas.Series\n        Time series of extreme events.\n    distribution : str or scipy.stats.rv_continuous\n        Distribution name compatible with scipy.stats\n        or a subclass of scipy.stats.rv_continuous.\n        See https://docs.scipy.org/doc/scipy/reference/stats.html\n    distribution_kwargs : dict, optional\n        Special keyword arguments, passed to the `.fit` method of the distribution.\n        These keyword arguments represent parameters to be held fixed.\n        Names of parameters to be fixed must have 'f' prefixes. Valid parameters:\n            - shape(s): 'fc', e.g. fc=0\n            - location: 'floc', e.g. floc=0\n            - scale: 'fscale', e.g. fscale=1\n        By default, no parameters are fixed.\n        See documentation of a specific scipy.stats distribution\n        for names of available parameters.\n    kwargs\n        Keyword arguments passed to a model .fit method.\n        MLE model:\n            MLE model takes no additional arguments.\n        Emcee model:\n            n_walkers : int, optional\n                The number of walkers in the ensemble (default=100).\n            n_samples : int, optional\n                The number of steps to run (default=500).\n            progress : bool or str, optional\n                If True, a progress bar will be shown as the sampler progresses.\n                If a string, will select a specific tqdm progress bar.\n                Most notable is 'notebook', which shows a progress bar\n                suitable for Jupyter notebooks.\n                If False (default), no progress bar will be shown.\n                This progress bar is a part of the `emcee` package.\n\n    \"\"\"\n    self.extremes = extremes\n\n    # Declare extreme value distribution\n    distribution_kwargs = distribution_kwargs or {}\n    self.distribution = Distribution(\n        extremes=self.extremes, distribution=distribution, **distribution_kwargs\n    )\n\n    # Fit the distribution to extremes\n    self._fit_parameters: typing.Optional[dict] = None\n    self._trace: typing.Optional[np.ndarray] = None\n    self.fit(**kwargs)\n\n    # Initialize 'return_value_cache'\n    self.return_value_cache: typing.Dict[tuple, tuple] = {}\n</code></pre>"},{"location":"api/models/#pyextremes.models.model_base.AbstractModelBaseClass.fit","title":"<code>fit(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Set values for self.fit_parameters and self.trace.</p> <p>self.trace is set only for MCMC-like models. self.fit_parameters is a dictionary with {parameter_name: value}, e.g. {'c': 0.1, 'loc': -7, 'scale': 0.3} self.trace is a numpy.ndarray with shape of (n_walkers, n_samples, n_free_parameters)</p> Source code in <code>src/pyextremes/models/model_base.py</code> <pre><code>@abc.abstractmethod\ndef fit(self, **kwargs) -&gt; None:\n    \"\"\"\n    Set values for self.fit_parameters and self.trace.\n\n    self.trace is set only for MCMC-like models.\n    self.fit_parameters is a dictionary with {parameter_name: value},\n    e.g. {'c': 0.1, 'loc': -7, 'scale': 0.3}\n    self.trace is a numpy.ndarray with shape of\n    (n_walkers, n_samples, n_free_parameters)\n\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/models/#pyextremes.models.model_base.AbstractModelBaseClass.get_return_value","title":"<code>get_return_value(exceedance_probability, alpha=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Calculate return value and confidence interval bounds.</p> <p>Parameters:</p> Name Type Description Default <code>exceedance_probability</code> <code>array - like</code> <p>Exceedance probability or 1D array of exceedance probabilities. Each exceedance probability must be in the [0, 1) range.</p> required <code>alpha</code> <code>float</code> <p>Width of confidence interval (0, 1). If None (default), return None for upper and lower confidence interval bounds.</p> <code>None</code> <code>kwargs</code> <p>Model-specific keyword arguments. If alpha is None, keyword arguments are ignored (error still raised for unrecognized arguments). MLE model:     n_samples : int, optional         Number of bootstrap samples used to estimate         confidence interval bounds (default=100). Emcee model:     burn_in : int         Burn-in value (number of first steps to discard for each walker).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>return_value</code> <code>array - like</code> <p>Return values.</p> <code>ci_lower</code> <code>array - like</code> <p>Lower confidence interval bounds.</p> <code>ci_upper</code> <code>array - like</code> <p>Upper confidence interval bounds.</p> Source code in <code>src/pyextremes/models/model_base.py</code> <pre><code>@abc.abstractmethod\ndef get_return_value(\n    self,\n    exceedance_probability: float,\n    alpha: typing.Optional[float] = None,\n    **kwargs,\n) -&gt; tuple:\n    \"\"\"\n    Calculate return value and confidence interval bounds.\n\n    Parameters\n    ----------\n    exceedance_probability : array-like\n        Exceedance probability or 1D array of exceedance probabilities.\n        Each exceedance probability must be in the [0, 1) range.\n    alpha : float, optional\n        Width of confidence interval (0, 1).\n        If None (default), return None\n        for upper and lower confidence interval bounds.\n    kwargs\n        Model-specific keyword arguments.\n        If alpha is None, keyword arguments are ignored\n        (error still raised for unrecognized arguments).\n        MLE model:\n            n_samples : int, optional\n                Number of bootstrap samples used to estimate\n                confidence interval bounds (default=100).\n        Emcee model:\n            burn_in : int\n                Burn-in value (number of first steps to discard for each walker).\n\n    Returns\n    -------\n    return_value : array-like\n        Return values.\n    ci_lower : array-like\n        Lower confidence interval bounds.\n    ci_upper : array-like\n        Upper confidence interval bounds.\n\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/models/#pyextremes.models.model_emcee.Emcee","title":"<code>pyextremes.models.model_emcee.Emcee</code>","text":"<p>             Bases: <code>AbstractModelBaseClass</code></p> Source code in <code>src/pyextremes/models/model_emcee.py</code> <pre><code>class Emcee(AbstractModelBaseClass):\n    def __init__(\n        self,\n        extremes: pd.Series,\n        distribution: typing.Union[str, scipy.stats.rv_continuous],\n        distribution_kwargs: typing.Optional[dict] = None,\n        n_walkers: int = 100,\n        n_samples: int = 500,\n        progress: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Markov Chain Monte Carlo (MCMC) model.\n\n        Built around the 'emcee' package by Daniel Foreman-Mackey\n\n        \"\"\"\n        super().__init__(\n            extremes=extremes,\n            distribution=distribution,\n            distribution_kwargs=distribution_kwargs,\n            n_walkers=n_walkers,\n            n_samples=n_samples,\n            progress=progress,\n        )\n        self.n_walkers = n_walkers\n        self.n_samples = n_samples\n\n    @property\n    def name(self) -&gt; str:\n        return \"Emcee\"\n\n    def fit(self, **kwargs) -&gt; None:\n        # Parse kwargs\n        n_walkers: int = kwargs.pop(\"n_walkers\")\n        n_samples: int = kwargs.pop(\"n_samples\")\n        progress: bool = kwargs.pop(\"progress\")\n        if len(kwargs) != 0:\n            raise TypeError(\n                f\"unrecognized arguments passed in: {', '.join(kwargs.keys())}\"\n            )\n\n        # Declare Emcee ensemble sampler\n        sampler = emcee.EnsembleSampler(\n            nwalkers=n_walkers,\n            ndim=self.distribution.number_of_parameters,\n            log_prob_fn=self.distribution.log_probability,\n        )\n\n        # Run the ensemble sampler\n        logger.debug(\n            \"running ensemble sampler with %d walkers and %d samples\",\n            n_walkers,\n            n_samples,\n        )\n        with warnings.catch_warnings():\n            warnings.simplefilter(action=\"ignore\", category=RuntimeWarning)\n            sampler.run_mcmc(\n                initial_state=self.distribution.get_initial_state(n_walkers=n_walkers),\n                nsteps=n_samples,\n                progress=progress,\n            )\n        logger.debug(\n            \"finished run for ensemble sampler with %d walkers and %d samples\",\n            n_walkers,\n            n_samples,\n        )\n\n        # Extract ensemble sampler chain\n        self._trace: np.ndarray = sampler.get_chain().transpose((1, 0, 2))\n\n        # Calculate fit parameters as MAP of distribution parameters\n        kernel = scipy.stats.gaussian_kde(np.vstack(self._trace).transpose())\n\n        def kde_func(x):\n            return -kernel(x)[0]\n\n        fit_parameters = self._trace.mean(axis=(0, 1))\n        solution = scipy.optimize.minimize(\n            kde_func,\n            x0=fit_parameters,\n            method=\"Nelder-Mead\",\n        )\n        if solution.success:\n            fit_parameters = solution.x\n        else:  # pragma: no cover\n            warnings.warn(\n                message=(\n                    \"cannot calculate MAP using Gaussian KDE, \"\n                    \"setting fit parameters as mean\"\n                ),\n                category=RuntimeWarning,\n            )\n        self._fit_parameters = dict(\n            zip(self.distribution.free_parameters, fit_parameters)\n        )\n\n    @property\n    def trace_map(self) -&gt; tuple:\n        return tuple(\n            self.fit_parameters[parameter]\n            for parameter in self.distribution.free_parameters\n        )\n\n    def get_return_value(\n        self, exceedance_probability, alpha: typing.Optional[float] = None, **kwargs\n    ) -&gt; tuple:\n        \"\"\"\n        Calculate return value and confidence interval bounds.\n\n        Parameters\n        ----------\n        exceedance_probability : array-like\n            Exceedance probability or 1D array of exceedance probabilities.\n            Each exceedance probability must be in the [0, 1) range.\n        alpha : float, optional\n            Width of confidence interval (0, 1).\n            If None (default), return None\n            for upper and lower confidence interval bounds.\n        kwargs\n            burn_in : int, optional\n                Burn-in value (number of first steps to discard for each walker).\n                By default it is 0 (no values are discarded).\n\n        Returns\n        -------\n        return_value : array-like\n            Return values.\n        ci_lower : array-like\n            Lower confidence interval bounds.\n        ci_upper : array-like\n            Upper confidence interval bounds.\n\n        \"\"\"\n        # Parse 'kwargs'\n        burn_in = kwargs.pop(\"burn_in\", 0)\n        if len(kwargs) != 0:\n            raise TypeError(\n                f\"unrecognized arguments passed in: {', '.join(kwargs.keys())}\"\n            )\n\n        # Convert 'exceedance_probability' to ndarray\n        exceedance_probability = np.asarray(\n            a=exceedance_probability, dtype=np.float64\n        ).copy()\n        if exceedance_probability.ndim == 0:\n            exceedance_probability = exceedance_probability[np.newaxis]\n        if exceedance_probability.ndim != 1:\n            raise ValueError(\n                f\"invalid shape in {exceedance_probability.shape} \"\n                f\"for the 'exceedance_probability' argument, must be 1D array\"\n            )\n\n        # Calculate return values\n        return_value = np.full(\n            shape=exceedance_probability.shape, fill_value=np.nan, dtype=np.float64\n        )\n        ci_lower = return_value.copy()\n        ci_upper = return_value.copy()\n        for i, ep in enumerate(exceedance_probability):\n            key: typing.Tuple[float, typing.Optional[float], int] = (\n                ep,\n                alpha,\n                burn_in,\n            )\n            try:\n                # Try to fetch pre-calculated values from cache\n                rv, cil, ciu = self.return_value_cache[key]\n                logger.debug(\n                    \"fetched return value for %s from cache as (%s, %s, %s)\",\n                    key,\n                    rv,\n                    cil,\n                    ciu,\n                )\n            except KeyError:\n                # Value not in cache - calculate new return value\n                rv = self.distribution.distribution.isf(\n                    q=ep,\n                    **self.fit_parameters,\n                    **self.distribution._fixed_parameters,\n                )\n\n                # Calculate confidence intervals\n                if alpha is None:\n                    cil = None\n                    ciu = None\n                else:\n                    # Calculate confidence intervals\n                    rv_sample = self.distribution.get_prop(\n                        prop=\"isf\",\n                        x=ep,\n                        free_parameters=np.vstack(self.trace[:, burn_in:, :]),\n                    )\n                    cil, ciu = np.quantile(\n                        a=rv_sample, q=[(1 - alpha) / 2, (1 + alpha) / 2]\n                    )\n\n                # Add calculated return value and intervals to cache\n                self.return_value_cache[key] = (rv, cil, ciu)\n                logger.debug(\n                    \"calculated return value for %s as (%s, %s, %s)\",\n                    key,\n                    rv,\n                    cil,\n                    ciu,\n                )\n\n            return_value[i] = rv\n            ci_lower[i] = cil\n            ci_upper[i] = ciu\n\n        # Return results\n        if len(return_value) == 1:\n            return return_value[0], ci_lower[0], ci_upper[0]\n        else:\n            return return_value, ci_lower, ci_upper\n\n    def __repr__(self) -&gt; str:\n        free_parameters = \", \".join(\n            [\n                f\"{parameter}={self.fit_parameters[parameter]:.3f}\"\n                for parameter in self.distribution.free_parameters\n            ]\n        )\n\n        fixed_parameters = \", \".join(\n            [\n                f\"{key}={value:.3f}\"\n                for key, value in self.distribution.fixed_parameters.items()\n            ]\n        )\n        if fixed_parameters == \"\":\n            fixed_parameters = \"all parameters are free\"\n\n        summary = [\n            \"Emcee model\",\n            \"\",\n            f\"free parameters: {free_parameters}\",\n            f\"fixed parameters: {fixed_parameters}\",\n            f\"AIC: {self.AIC:.3f}\",\n            f\"loglikelihood: {self.loglikelihood:.3f}\",\n            f\"number of walkers: {self.n_walkers:d}\",\n            f\"number of samples: {self.n_samples:d}\",\n            f\"return value cache size: {len(self.return_value_cache):,d}\",\n        ]\n\n        longest_row = max(map(len, summary))\n        summary[1] = \"-\" * longest_row\n        summary.append(summary[1])\n        summary[0] = \" \" * ((longest_row - len(summary[0])) // 2) + summary[0]\n        for i, row in enumerate(summary):\n            summary[i] += \" \" * (longest_row - len(row))\n\n        return \"\\n\".join(summary)\n</code></pre>"},{"location":"api/models/#pyextremes.models.model_emcee.Emcee.__init__","title":"<code>__init__(extremes, distribution, distribution_kwargs=None, n_walkers=100, n_samples=500, progress=False)</code>","text":"<p>Markov Chain Monte Carlo (MCMC) model.</p> <p>Built around the 'emcee' package by Daniel Foreman-Mackey</p> Source code in <code>src/pyextremes/models/model_emcee.py</code> <pre><code>def __init__(\n    self,\n    extremes: pd.Series,\n    distribution: typing.Union[str, scipy.stats.rv_continuous],\n    distribution_kwargs: typing.Optional[dict] = None,\n    n_walkers: int = 100,\n    n_samples: int = 500,\n    progress: bool = False,\n) -&gt; None:\n    \"\"\"\n    Markov Chain Monte Carlo (MCMC) model.\n\n    Built around the 'emcee' package by Daniel Foreman-Mackey\n\n    \"\"\"\n    super().__init__(\n        extremes=extremes,\n        distribution=distribution,\n        distribution_kwargs=distribution_kwargs,\n        n_walkers=n_walkers,\n        n_samples=n_samples,\n        progress=progress,\n    )\n    self.n_walkers = n_walkers\n    self.n_samples = n_samples\n</code></pre>"},{"location":"api/models/#pyextremes.models.model_emcee.Emcee.get_return_value","title":"<code>get_return_value(exceedance_probability, alpha=None, **kwargs)</code>","text":"<p>Calculate return value and confidence interval bounds.</p> <p>Parameters:</p> Name Type Description Default <code>exceedance_probability</code> <code>array - like</code> <p>Exceedance probability or 1D array of exceedance probabilities. Each exceedance probability must be in the [0, 1) range.</p> required <code>alpha</code> <code>float</code> <p>Width of confidence interval (0, 1). If None (default), return None for upper and lower confidence interval bounds.</p> <code>None</code> <code>kwargs</code> <p>burn_in : int, optional     Burn-in value (number of first steps to discard for each walker).     By default it is 0 (no values are discarded).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>return_value</code> <code>array - like</code> <p>Return values.</p> <code>ci_lower</code> <code>array - like</code> <p>Lower confidence interval bounds.</p> <code>ci_upper</code> <code>array - like</code> <p>Upper confidence interval bounds.</p> Source code in <code>src/pyextremes/models/model_emcee.py</code> <pre><code>def get_return_value(\n    self, exceedance_probability, alpha: typing.Optional[float] = None, **kwargs\n) -&gt; tuple:\n    \"\"\"\n    Calculate return value and confidence interval bounds.\n\n    Parameters\n    ----------\n    exceedance_probability : array-like\n        Exceedance probability or 1D array of exceedance probabilities.\n        Each exceedance probability must be in the [0, 1) range.\n    alpha : float, optional\n        Width of confidence interval (0, 1).\n        If None (default), return None\n        for upper and lower confidence interval bounds.\n    kwargs\n        burn_in : int, optional\n            Burn-in value (number of first steps to discard for each walker).\n            By default it is 0 (no values are discarded).\n\n    Returns\n    -------\n    return_value : array-like\n        Return values.\n    ci_lower : array-like\n        Lower confidence interval bounds.\n    ci_upper : array-like\n        Upper confidence interval bounds.\n\n    \"\"\"\n    # Parse 'kwargs'\n    burn_in = kwargs.pop(\"burn_in\", 0)\n    if len(kwargs) != 0:\n        raise TypeError(\n            f\"unrecognized arguments passed in: {', '.join(kwargs.keys())}\"\n        )\n\n    # Convert 'exceedance_probability' to ndarray\n    exceedance_probability = np.asarray(\n        a=exceedance_probability, dtype=np.float64\n    ).copy()\n    if exceedance_probability.ndim == 0:\n        exceedance_probability = exceedance_probability[np.newaxis]\n    if exceedance_probability.ndim != 1:\n        raise ValueError(\n            f\"invalid shape in {exceedance_probability.shape} \"\n            f\"for the 'exceedance_probability' argument, must be 1D array\"\n        )\n\n    # Calculate return values\n    return_value = np.full(\n        shape=exceedance_probability.shape, fill_value=np.nan, dtype=np.float64\n    )\n    ci_lower = return_value.copy()\n    ci_upper = return_value.copy()\n    for i, ep in enumerate(exceedance_probability):\n        key: typing.Tuple[float, typing.Optional[float], int] = (\n            ep,\n            alpha,\n            burn_in,\n        )\n        try:\n            # Try to fetch pre-calculated values from cache\n            rv, cil, ciu = self.return_value_cache[key]\n            logger.debug(\n                \"fetched return value for %s from cache as (%s, %s, %s)\",\n                key,\n                rv,\n                cil,\n                ciu,\n            )\n        except KeyError:\n            # Value not in cache - calculate new return value\n            rv = self.distribution.distribution.isf(\n                q=ep,\n                **self.fit_parameters,\n                **self.distribution._fixed_parameters,\n            )\n\n            # Calculate confidence intervals\n            if alpha is None:\n                cil = None\n                ciu = None\n            else:\n                # Calculate confidence intervals\n                rv_sample = self.distribution.get_prop(\n                    prop=\"isf\",\n                    x=ep,\n                    free_parameters=np.vstack(self.trace[:, burn_in:, :]),\n                )\n                cil, ciu = np.quantile(\n                    a=rv_sample, q=[(1 - alpha) / 2, (1 + alpha) / 2]\n                )\n\n            # Add calculated return value and intervals to cache\n            self.return_value_cache[key] = (rv, cil, ciu)\n            logger.debug(\n                \"calculated return value for %s as (%s, %s, %s)\",\n                key,\n                rv,\n                cil,\n                ciu,\n            )\n\n        return_value[i] = rv\n        ci_lower[i] = cil\n        ci_upper[i] = ciu\n\n    # Return results\n    if len(return_value) == 1:\n        return return_value[0], ci_lower[0], ci_upper[0]\n    else:\n        return return_value, ci_lower, ci_upper\n</code></pre>"},{"location":"api/models/#pyextremes.models.model_mle.MLE","title":"<code>pyextremes.models.model_mle.MLE</code>","text":"<p>             Bases: <code>AbstractModelBaseClass</code></p> Source code in <code>src/pyextremes/models/model_mle.py</code> <pre><code>class MLE(AbstractModelBaseClass):\n    def __init__(\n        self,\n        extremes: pd.Series,\n        distribution: typing.Union[str, scipy.stats.rv_continuous],\n        distribution_kwargs: typing.Optional[dict] = None,\n    ) -&gt; None:\n        \"\"\"\n        Maximum Likelihood Estimate (MLE) model.\n\n        Built around the scipy.stats.rv_continuous.fit method.\n\n        \"\"\"\n        super().__init__(\n            extremes=extremes,\n            distribution=distribution,\n            distribution_kwargs=distribution_kwargs,\n        )\n\n        # Initialize 'fit_parameter_cache' and 'seed_cache'\n        self.fit_parameter_cache: typing.List[typing.Tuple[float, ...]] = []\n        self.seed_cache: typing.Set[int] = set()\n\n    @property\n    def name(self) -&gt; str:\n        return \"MLE\"\n\n    def fit(self, **kwargs) -&gt; None:\n        if len(kwargs) != 0:\n            raise TypeError(\n                f\"unrecognized arguments passed in: {', '.join(kwargs.keys())}\"\n            )\n        self._fit_parameters = self.distribution.mle_parameters\n        logger.debug(\n            \"fit %s distribution with %s parameters\",\n            self.distribution.name,\n            len(self._fit_parameters),\n        )\n\n    def get_return_value(\n        self, exceedance_probability, alpha: typing.Optional[float] = None, **kwargs\n    ) -&gt; tuple:\n        \"\"\"\n        Calculate return value and confidence interval bounds.\n\n        Parameters\n        ----------\n        exceedance_probability : array-like\n            Exceedance probability or 1D array of exceedance probabilities.\n            Each exceedance probability must be in the [0, 1) range.\n        alpha : float, optional\n            Width of confidence interval (0, 1).\n            If None (default), return None\n            for upper and lower confidence interval bounds.\n        kwargs\n            n_samples : int, optional\n                Number of bootstrap samples used to estimate\n                confidence interval bounds (default=100).\n\n        Returns\n        -------\n        return_value : array-like\n            Return values.\n        ci_lower : array-like\n            Lower confidence interval bounds.\n        ci_upper : array-like\n            Upper confidence interval bounds.\n\n        \"\"\"\n        # Parse 'kwargs'\n        n_samples = kwargs.pop(\"n_samples\", 100)\n        if not n_samples &gt; 0:\n            raise ValueError(\n                f\"invalid value in {n_samples} for the 'n_samples' \"\n                f\"argument, must be positive number\"\n            )\n        if len(kwargs) != 0:\n            raise TypeError(\n                f\"unrecognized arguments passed in: {', '.join(kwargs.keys())}\"\n            )\n\n        # Convert 'exceedance_probability' to ndarray\n        exceedance_probability = np.asarray(\n            a=exceedance_probability, dtype=np.float64\n        ).copy()\n        if exceedance_probability.ndim == 0:\n            exceedance_probability = exceedance_probability[np.newaxis]\n        if exceedance_probability.ndim != 1:\n            raise ValueError(\n                f\"invalid shape in {exceedance_probability.shape} \"\n                f\"for the 'exceedance_probability' argument, must be 1D array\"\n            )\n\n        # If cache doesn't have enough values, calculate new fit parameters\n        if alpha is not None:\n            n_extra_fit_parameters = n_samples - len(self.fit_parameter_cache)\n            if n_extra_fit_parameters &gt; 0:\n                self._extend_fit_parameter_cache(n=n_extra_fit_parameters)\n\n        # Calculate return values\n        return_value = np.full(\n            shape=exceedance_probability.shape, fill_value=np.nan, dtype=np.float64\n        )\n        ci_lower = return_value.copy()\n        ci_upper = return_value.copy()\n        for i, ep in enumerate(exceedance_probability):\n            key: typing.Tuple[float, typing.Optional[float], int] = (\n                ep,\n                alpha,\n                n_samples,\n            )\n            try:\n                # Try to fetch pre-calculated values from cache\n                rv, cil, ciu = self.return_value_cache[key]\n                logger.debug(\n                    \"fetched return value for %s from cache as (%s, %s, %s)\",\n                    key,\n                    rv,\n                    cil,\n                    ciu,\n                )\n            except KeyError:\n                # Value not in cache - calculate new return value\n                rv = self.distribution.distribution.isf(\n                    q=ep,\n                    **self.fit_parameters,\n                    **self.distribution._fixed_parameters,\n                )\n\n                # Calculate confidence intervals\n                if alpha is None:\n                    cil = None\n                    ciu = None\n                else:\n                    # Calculate confidence intervals\n                    rv_sample = self.distribution.distribution.isf(\n                        ep, *np.transpose(self.fit_parameter_cache[:n_samples])\n                    )\n                    cil, ciu = np.quantile(\n                        a=rv_sample, q=[(1 - alpha) / 2, (1 + alpha) / 2]\n                    )\n\n                # Add calculated return value and intervals to cache\n                self.return_value_cache[key] = (rv, cil, ciu)\n                logger.debug(\n                    \"calculated return value for %s as (%s, %s, %s)\",\n                    key,\n                    rv,\n                    cil,\n                    ciu,\n                )\n\n            return_value[i] = rv\n            ci_lower[i] = cil\n            ci_upper[i] = ciu\n\n        # Return results\n        if len(return_value) == 1:\n            return return_value[0], ci_lower[0], ci_upper[0]\n        else:\n            return return_value, ci_lower, ci_upper\n\n    def _extend_fit_parameter_cache(self, n: int) -&gt; None:\n        # Prepare local variables used by fit parameter calculator\n        extremes = self.extremes.values\n        distribution = self.distribution.distribution\n        fixed_parameters = self.distribution.fixed_parameters\n\n        min_samples_per_core = 50\n        if n &lt;= min_samples_per_core:\n            # Calculate without multiprocessing\n            logger.debug(\"getting random seed value for fit parameter sampler\")\n            seed = None\n            while seed is None:\n                _seed = np.random.randint(low=0, high=1e6, size=None)\n                if _seed not in self.seed_cache:\n                    seed = _seed\n                    self.seed_cache.add(_seed)\n\n            logger.debug(\n                \"calculating %d additional fit parameters using single core\",\n                n,\n            )\n            new_fit_parameters = get_fit_parameters(\n                params=(\n                    n,\n                    distribution,\n                    extremes,\n                    fixed_parameters,\n                    seed,\n                )\n            )\n        else:\n            # Find number of cores\n            n_cores = min(\n                os.cpu_count() or 2,\n                int(np.ceil(n / min_samples_per_core)),\n            )\n\n            # Calculate number of samples per core\n            min_samples_per_core = int(n / n_cores)\n            core_samples = [min_samples_per_core for _ in range(n_cores)]\n\n            # Distribute remaining samples evenly across cores\n            for i in range(n - sum(core_samples)):\n                core_samples[i] += 1\n\n            # Get unique random seed for each core and add it to `self.seed_cache`\n            logger.debug(\"getting random seed values for each core\")\n            seeds: typing.List[int] = []\n            while len(seeds) &lt; n_cores:\n                seed = np.random.randint(low=0, high=1e6, size=None)\n                if seed not in self.seed_cache:\n                    seeds.append(seed)\n                    self.seed_cache.add(seed)\n\n            # Calculate new fit parameters using processor pool\n            logger.debug(\n                \"calculating %d additional fit parameters using %d cores \"\n                \"having %s samples accordingly\",\n                n,\n                n_cores,\n                core_samples,\n            )\n            with multiprocessing.Pool(processes=n_cores) as pool:\n                new_fit_parameters = list(\n                    itertools.chain(\n                        *pool.map(\n                            get_fit_parameters,\n                            zip(\n                                core_samples,\n                                [distribution for _ in range(n_cores)],\n                                [extremes for _ in range(n_cores)],\n                                [fixed_parameters for _ in range(n_cores)],\n                                seeds,\n                            ),\n                        )\n                    )\n                )\n\n        # Extend fit parameter cache\n        logger.debug(\"extending fit parameter cache with %d new entries\", n)\n        self.fit_parameter_cache.extend(new_fit_parameters)\n        return None\n\n    def __repr__(self) -&gt; str:\n        free_parameters = \", \".join(\n            [\n                f\"{parameter}={self.fit_parameters[parameter]:.3f}\"\n                for parameter in self.distribution.free_parameters\n            ]\n        )\n\n        fixed_parameters = \", \".join(\n            [\n                f\"{key}={value:.3f}\"\n                for key, value in self.distribution.fixed_parameters.items()\n            ]\n        )\n        if fixed_parameters == \"\":\n            fixed_parameters = \"all parameters are free\"\n\n        summary = [\n            \"MLE model\",\n            \"\",\n            f\"free parameters: {free_parameters}\",\n            f\"fixed parameters: {fixed_parameters}\",\n            f\"AIC: {self.AIC:.3f}\",\n            f\"loglikelihood: {self.loglikelihood:.3f}\",\n            f\"return value cache size: {len(self.return_value_cache):,d}\",\n            f\"fit parameter cache size: {len(self.fit_parameter_cache):,d}\",\n        ]\n\n        longest_row = max(map(len, summary))\n        summary[1] = \"-\" * longest_row\n        summary.append(summary[1])\n        summary[0] = \" \" * ((longest_row - len(summary[0])) // 2) + summary[0]\n        for i, row in enumerate(summary):\n            summary[i] += \" \" * (longest_row - len(row))\n\n        return \"\\n\".join(summary)\n</code></pre>"},{"location":"api/models/#pyextremes.models.model_mle.MLE.__init__","title":"<code>__init__(extremes, distribution, distribution_kwargs=None)</code>","text":"<p>Maximum Likelihood Estimate (MLE) model.</p> <p>Built around the scipy.stats.rv_continuous.fit method.</p> Source code in <code>src/pyextremes/models/model_mle.py</code> <pre><code>def __init__(\n    self,\n    extremes: pd.Series,\n    distribution: typing.Union[str, scipy.stats.rv_continuous],\n    distribution_kwargs: typing.Optional[dict] = None,\n) -&gt; None:\n    \"\"\"\n    Maximum Likelihood Estimate (MLE) model.\n\n    Built around the scipy.stats.rv_continuous.fit method.\n\n    \"\"\"\n    super().__init__(\n        extremes=extremes,\n        distribution=distribution,\n        distribution_kwargs=distribution_kwargs,\n    )\n\n    # Initialize 'fit_parameter_cache' and 'seed_cache'\n    self.fit_parameter_cache: typing.List[typing.Tuple[float, ...]] = []\n    self.seed_cache: typing.Set[int] = set()\n</code></pre>"},{"location":"api/models/#pyextremes.models.model_mle.MLE.get_return_value","title":"<code>get_return_value(exceedance_probability, alpha=None, **kwargs)</code>","text":"<p>Calculate return value and confidence interval bounds.</p> <p>Parameters:</p> Name Type Description Default <code>exceedance_probability</code> <code>array - like</code> <p>Exceedance probability or 1D array of exceedance probabilities. Each exceedance probability must be in the [0, 1) range.</p> required <code>alpha</code> <code>float</code> <p>Width of confidence interval (0, 1). If None (default), return None for upper and lower confidence interval bounds.</p> <code>None</code> <code>kwargs</code> <p>n_samples : int, optional     Number of bootstrap samples used to estimate     confidence interval bounds (default=100).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>return_value</code> <code>array - like</code> <p>Return values.</p> <code>ci_lower</code> <code>array - like</code> <p>Lower confidence interval bounds.</p> <code>ci_upper</code> <code>array - like</code> <p>Upper confidence interval bounds.</p> Source code in <code>src/pyextremes/models/model_mle.py</code> <pre><code>def get_return_value(\n    self, exceedance_probability, alpha: typing.Optional[float] = None, **kwargs\n) -&gt; tuple:\n    \"\"\"\n    Calculate return value and confidence interval bounds.\n\n    Parameters\n    ----------\n    exceedance_probability : array-like\n        Exceedance probability or 1D array of exceedance probabilities.\n        Each exceedance probability must be in the [0, 1) range.\n    alpha : float, optional\n        Width of confidence interval (0, 1).\n        If None (default), return None\n        for upper and lower confidence interval bounds.\n    kwargs\n        n_samples : int, optional\n            Number of bootstrap samples used to estimate\n            confidence interval bounds (default=100).\n\n    Returns\n    -------\n    return_value : array-like\n        Return values.\n    ci_lower : array-like\n        Lower confidence interval bounds.\n    ci_upper : array-like\n        Upper confidence interval bounds.\n\n    \"\"\"\n    # Parse 'kwargs'\n    n_samples = kwargs.pop(\"n_samples\", 100)\n    if not n_samples &gt; 0:\n        raise ValueError(\n            f\"invalid value in {n_samples} for the 'n_samples' \"\n            f\"argument, must be positive number\"\n        )\n    if len(kwargs) != 0:\n        raise TypeError(\n            f\"unrecognized arguments passed in: {', '.join(kwargs.keys())}\"\n        )\n\n    # Convert 'exceedance_probability' to ndarray\n    exceedance_probability = np.asarray(\n        a=exceedance_probability, dtype=np.float64\n    ).copy()\n    if exceedance_probability.ndim == 0:\n        exceedance_probability = exceedance_probability[np.newaxis]\n    if exceedance_probability.ndim != 1:\n        raise ValueError(\n            f\"invalid shape in {exceedance_probability.shape} \"\n            f\"for the 'exceedance_probability' argument, must be 1D array\"\n        )\n\n    # If cache doesn't have enough values, calculate new fit parameters\n    if alpha is not None:\n        n_extra_fit_parameters = n_samples - len(self.fit_parameter_cache)\n        if n_extra_fit_parameters &gt; 0:\n            self._extend_fit_parameter_cache(n=n_extra_fit_parameters)\n\n    # Calculate return values\n    return_value = np.full(\n        shape=exceedance_probability.shape, fill_value=np.nan, dtype=np.float64\n    )\n    ci_lower = return_value.copy()\n    ci_upper = return_value.copy()\n    for i, ep in enumerate(exceedance_probability):\n        key: typing.Tuple[float, typing.Optional[float], int] = (\n            ep,\n            alpha,\n            n_samples,\n        )\n        try:\n            # Try to fetch pre-calculated values from cache\n            rv, cil, ciu = self.return_value_cache[key]\n            logger.debug(\n                \"fetched return value for %s from cache as (%s, %s, %s)\",\n                key,\n                rv,\n                cil,\n                ciu,\n            )\n        except KeyError:\n            # Value not in cache - calculate new return value\n            rv = self.distribution.distribution.isf(\n                q=ep,\n                **self.fit_parameters,\n                **self.distribution._fixed_parameters,\n            )\n\n            # Calculate confidence intervals\n            if alpha is None:\n                cil = None\n                ciu = None\n            else:\n                # Calculate confidence intervals\n                rv_sample = self.distribution.distribution.isf(\n                    ep, *np.transpose(self.fit_parameter_cache[:n_samples])\n                )\n                cil, ciu = np.quantile(\n                    a=rv_sample, q=[(1 - alpha) / 2, (1 + alpha) / 2]\n                )\n\n            # Add calculated return value and intervals to cache\n            self.return_value_cache[key] = (rv, cil, ciu)\n            logger.debug(\n                \"calculated return value for %s as (%s, %s, %s)\",\n                key,\n                rv,\n                cil,\n                ciu,\n            )\n\n        return_value[i] = rv\n        ci_lower[i] = cil\n        ci_upper[i] = ciu\n\n    # Return results\n    if len(return_value) == 1:\n        return return_value[0], ci_lower[0], ci_upper[0]\n    else:\n        return return_value, ci_lower, ci_upper\n</code></pre>"},{"location":"api/plotting/","title":"Plotting","text":""},{"location":"api/plotting/#pyextremes.plotting.extremes.plot_extremes","title":"<code>pyextremes.plotting.extremes.plot_extremes(ts, extremes, extremes_method, extremes_type=None, block_size=None, threshold=None, r=None, figsize=(8, 5), ax=None)</code>","text":"<p>Plot extreme events.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Series</code> <p>Time series from which <code>extremes</code> were extracted.</p> required <code>extremes</code> <code>Series</code> <p>Time series of extreme events.</p> required <code>extremes_method</code> <code>str</code> <p>Extreme value extraction method. Supported values:     BM - Block Maxima     POT - Peaks Over Threshold</p> required <code>extremes_type</code> <code>str</code> <p>Type of <code>extremes</code>, used only if <code>extremes_method</code> is 'POT' and <code>threshold</code> is not provided.     high - extreme high values     low - get low values</p> <code>None</code> <code>block_size</code> <code>str or Timedelta</code> <p>Block size, used only if <code>extremes_method</code> is 'BM'. If None (default), then calculated as median distance between extreme events.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Threshold, used only if <code>extremes_method</code> is 'POT'. If None (default), then is inferred from <code>extremes</code> as minimum if <code>extremes_type</code> is 'high' or maximum if <code>extremes_type</code> is 'low'.</p> <code>None</code> <code>r</code> <code>pandas.Timedelta or value convertible to timedelta</code> <p>Duration of window used to decluster the exceedances. See pandas.to_timedelta for more information. Used to show clusters. If None (default) then clusters are not shown. Clusters are shown only if both <code>threshold</code> and <code>r</code> were provided.</p> <code>None</code> <code>figsize</code> <code>tuple</code> <p>Figure size in inches in format (width, height). By default it is (8, 5).</p> <code>(8, 5)</code> <code>ax</code> <code>Axes</code> <p>Axes onto which extremes plot is drawn. If None (default), a new figure and axes objects are created.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>figure</code> <code>Figure</code> <p>Figure object.</p> <code>axes</code> <code>Axes</code> <p>Axes object.</p> Source code in <code>src/pyextremes/plotting/extremes.py</code> <pre><code>def plot_extremes(\n    ts: pd.Series,\n    extremes: pd.Series,\n    extremes_method: Literal[\"BM\", \"POT\"],\n    extremes_type: Optional[Literal[\"high\", \"low\"]] = None,\n    block_size: Optional[Union[str, pd.Timedelta]] = None,\n    threshold: Optional[float] = None,\n    r: Optional[Union[pd.Timedelta, Any]] = None,\n    figsize: Tuple[float, float] = (8, 5),\n    ax: Optional[plt.Axes] = None,\n) -&gt; Tuple[plt.Figure, plt.Axes]:\n    \"\"\"\n    Plot extreme events.\n\n    Parameters\n    ----------\n    ts : pandas.Series\n        Time series from which `extremes` were extracted.\n    extremes : pandas.Series\n        Time series of extreme events.\n    extremes_method : str\n        Extreme value extraction method.\n        Supported values:\n            BM - Block Maxima\n            POT - Peaks Over Threshold\n    extremes_type : str, optional\n        Type of `extremes`, used only if `extremes_method` is 'POT'\n        and `threshold` is not provided.\n            high - extreme high values\n            low - get low values\n    block_size : str or pandas.Timedelta, optional\n        Block size, used only if `extremes_method` is 'BM'.\n        If None (default), then calculated as median distance between extreme events.\n    threshold : float, optional\n        Threshold, used only if `extremes_method` is 'POT'.\n        If None (default), then is inferred from `extremes` as\n        minimum if `extremes_type` is 'high' or maximum if `extremes_type` is 'low'.\n    r : pandas.Timedelta or value convertible to timedelta, optional\n        Duration of window used to decluster the exceedances.\n        See pandas.to_timedelta for more information.\n        Used to show clusters. If None (default) then clusters are not shown.\n        Clusters are shown only if both `threshold` and `r` were provided.\n    figsize : tuple, optional\n        Figure size in inches in format (width, height).\n        By default it is (8, 5).\n    ax : matplotlib.axes._axes.Axes, optional\n        Axes onto which extremes plot is drawn.\n        If None (default), a new figure and axes objects are created.\n\n    Returns\n    -------\n    figure : matplotlib.figure.Figure\n        Figure object.\n    axes : matplotlib.axes._axes.Axes\n        Axes object.\n\n    \"\"\"\n    if extremes_method not in [\"BM\", \"POT\"]:\n        raise ValueError(\n            f\"invalid value in '{extremes_method}' for the 'extremes_method' argument\"\n        )\n\n    if extremes_type not in [\"high\", \"low\"]:\n        raise ValueError(\n            f\"invalid value in '{extremes_type}' for the 'extremes_type' argument\"\n        )\n\n    with plt.rc_context(rc=pyextremes_rc):\n        # Create figure\n        if ax is None:\n            fig, ax = plt.subplots(figsize=figsize, dpi=96)\n        else:\n            try:\n                fig = ax.get_figure()\n            except AttributeError as _error:\n                raise TypeError(\n                    f\"invalid type in {type(ax)} for the 'ax' argument, \"\n                    f\"must be matplotlib Axes object\"\n                ) from _error\n\n        # Configure axes\n        ax.grid(False)\n\n        # Plot signal time series\n        ax.plot(ts.index, ts.values, ls=\"-\", color=\"#5199FF\", lw=0.25, zorder=10)\n\n        # Plot extreme events\n        ax.scatter(\n            extremes.index,\n            extremes.values,\n            s=20,\n            lw=0.5,\n            edgecolor=\"w\",\n            facecolor=\"#F85C50\",\n            zorder=20,\n        )\n\n        # Label the axes\n        ax.set_xlabel(extremes.index.name or \"date-time\")\n        ax.set_ylabel(extremes.name or \"Extreme value\")\n\n        if extremes_method == \"BM\":\n            # Infer 'block_size'\n            if block_size is None:\n                # Calculate 'block_size' as median of distances between extremes\n                block_size = pd.to_timedelta(\n                    np.quantile(\n                        np.diff(extremes.index),\n                        0.5,\n                    )\n                )\n            else:\n                if not isinstance(block_size, pd.Timedelta):\n                    if isinstance(block_size, str):\n                        block_size = pd.to_timedelta(block_size)\n                    else:\n                        raise TypeError(\n                            f\"invalid type in {type(block_size)} \"\n                            f\"for the 'block_size' argument\"\n                        )\n\n            # Plot block boundaries\n            block_left_boundary = ts.index[0]\n            while block_left_boundary &lt; extremes.index.max() + block_size:\n                ax.axvline(\n                    block_left_boundary, ls=\"--\", lw=0.5, color=\"#D1D3D4\", zorder=5\n                )\n                block_left_boundary += block_size\n\n        else:\n            if threshold is None:\n                if extremes_type is None:\n                    raise TypeError(\n                        \"'extremes_type' argument must be provided \"\n                        \"for 'extremes_method' being 'POT' \"\n                        \"when 'threshold' is not provided\"\n                    )\n                if extremes_type == \"high\":\n                    threshold = extremes.min()\n                else:\n                    threshold = extremes.max()\n            else:\n                if r is not None:\n                    # Plot clusters (only if both threshold and r are provided)\n                    if extremes_type == \"high\":\n                        exceedances = ts.loc[ts.values &gt; threshold]\n                    else:\n                        exceedances = ts.loc[ts.values &lt; threshold]\n                    for cluster in _generate_clusters(exceedances=exceedances, r=r):\n                        _plot_cluster(ax=ax, cluster=cluster)\n\n            # Plot threshold line\n            ax.axhline(threshold, ls=\"--\", lw=1, color=\"#FF756B\", zorder=15)\n\n        fig.autofmt_xdate()\n\n        return fig, ax\n</code></pre>"},{"location":"api/plotting/#pyextremes.plotting.return_values.plot_return_values","title":"<code>pyextremes.plotting.return_values.plot_return_values(observed_return_values, modeled_return_values, ax=None, figsize=(8, 5))</code>","text":"<p>Plot return values and confidence intervals for given return periods.</p> <p>Parameters:</p> Name Type Description Default <code>observed_return_values</code> <code>DataFrame</code> <p>DataFrame with observed return values. First column must have extreme values. Must have 'return period' column.</p> required <code>modeled_return_values</code> <code>DataFrame</code> <p>DataFrame with modeled return values. Index has return periods. Must have the following columns: 'return value', 'lower ci', 'upper ci'.</p> required <code>ax</code> <code>Axes</code> <p>Axes onto which the return value plot is drawn. If None (default), a new figure and axes objects are created.</p> <code>None</code> <code>figsize</code> <code>tuple</code> <p>Figure size in inches in format (width, height). By default it is (8, 5).</p> <code>(8, 5)</code> <p>Returns:</p> Name Type Description <code>figure</code> <code>Figure</code> <p>Figure object.</p> <code>axes</code> <code>Axes</code> <p>Axes object.</p> Source code in <code>src/pyextremes/plotting/return_values.py</code> <pre><code>def plot_return_values(\n    observed_return_values: pd.DataFrame,\n    modeled_return_values: pd.DataFrame,\n    ax: Optional[plt.Axes] = None,\n    figsize: Tuple[float, float] = (8, 5),\n) -&gt; Tuple[plt.Figure, plt.Axes]:\n    \"\"\"\n    Plot return values and confidence intervals for given return periods.\n\n    Parameters\n    ----------\n    observed_return_values : pandas.DataFrame\n        DataFrame with observed return values.\n        First column must have extreme values.\n        Must have 'return period' column.\n    modeled_return_values : pandas.DataFrame\n        DataFrame with modeled return values.\n        Index has return periods.\n        Must have the following columns: 'return value', 'lower ci', 'upper ci'.\n    ax : matplotlib.axes._axes.Axes, optional\n        Axes onto which the return value plot is drawn.\n        If None (default), a new figure and axes objects are created.\n    figsize : tuple, optional\n        Figure size in inches in format (width, height).\n        By default it is (8, 5).\n\n    Returns\n    -------\n    figure : matplotlib.figure.Figure\n        Figure object.\n    axes : matplotlib.axes._axes.Axes\n        Axes object.\n\n    \"\"\"\n    # Validate the 'observed_return_values' argument\n    if (\n        len(observed_return_values.columns) &lt; 2\n        or observed_return_values.columns[0] == \"return period\"\n        or \"return period\" not in observed_return_values.columns\n    ):\n        raise ValueError(\n            f\"'observed_return_values' argument \"\n            f\"has invalid columns in {observed_return_values.columns}, \"\n            f\"must have at least two columns and a 'return period' column \"\n            f\"which is not the first column\"\n        )\n\n    # Validate the 'modeled_return_values' argument\n    if len(modeled_return_values.columns) &lt; 3 or any(\n        col not in modeled_return_values.columns\n        for col in [\"return value\", \"lower ci\", \"upper ci\"]\n    ):\n        raise ValueError(\n            f\"'modeled_return_values' argument \"\n            f\"has invalid columns in {modeled_return_values.columns}, \"\n            f\"must have at least three columns and include columns: \"\n            f\"'return value', 'lower ci', 'upper ci'\"\n        )\n\n    with plt.rc_context(rc=pyextremes_rc):\n        # Create figure\n        if ax is None:\n            fig, ax = plt.subplots(figsize=figsize, dpi=96)\n        else:\n            try:\n                fig = ax.figure\n            except AttributeError as _error:\n                raise TypeError(\n                    f\"invalid type in {type(ax)} for the 'ax' argument, \"\n                    f\"must be matplotlib Axes object\"\n                ) from _error\n\n        # Configure axes\n        ax.semilogx()\n        ax.grid(True, which=\"both\")\n        ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:,.0f}\"))\n\n        # Plot modeled confidence intervals\n        for col in [\"lower ci\", \"upper ci\"]:\n            ax.plot(\n                modeled_return_values.index.values,\n                modeled_return_values.loc[:, col].values,\n                color=\"#5199FF\",\n                lw=1,\n                ls=\"--\",\n                zorder=15,\n            )\n        ax.fill_between(\n            modeled_return_values.index.values,\n            modeled_return_values.loc[:, \"lower ci\"].values,\n            modeled_return_values.loc[:, \"upper ci\"].values,\n            facecolor=\"#5199FF\",\n            edgecolor=\"None\",\n            alpha=0.25,\n            zorder=10,\n        )\n\n        # Plot observed extreme values\n        ax.scatter(\n            observed_return_values.loc[:, \"return period\"].values,\n            observed_return_values.loc[:, observed_return_values.columns[0]].values,\n            marker=\"o\",\n            s=20,\n            lw=1,\n            facecolor=\"k\",\n            edgecolor=\"w\",\n            zorder=20,\n        )\n\n        # Plot modeled return values\n        ax.plot(\n            modeled_return_values.index.values,\n            modeled_return_values.loc[:, \"return value\"].values,\n            color=\"#F85C50\",\n            lw=2,\n            ls=\"-\",\n            zorder=25,\n        )\n\n        # Label axes\n        ax.set_xlabel(\"Return period\")\n        ax.set_ylabel(observed_return_values.columns[0])\n\n        return fig, ax\n</code></pre>"},{"location":"api/plotting/#pyextremes.plotting.probability_plots.plot_probability","title":"<code>pyextremes.plotting.probability_plots.plot_probability(observed, theoretical, ax=None, figsize=(8, 8))</code>","text":"<p>Plot a probability plot (QQ or PP).</p> <p>Parameters:</p> Name Type Description Default <code>observed</code> <code>ndarray</code> <p>Observed values.</p> required <code>theoretical</code> <code>ndarray</code> <p>Theoretical values.</p> required <code>ax</code> <code>Axes</code> <p>Axes onto which the probability plot is drawn. If None (default), a new figure and axes objects are created.</p> <code>None</code> <code>figsize</code> <code>tuple</code> <p>Figure size in inches in format (width, height). By default it is (8, 8).</p> <code>(8, 8)</code> <p>Returns:</p> Name Type Description <code>figure</code> <code>Figure</code> <p>Figure object.</p> <code>axes</code> <code>Axes</code> <p>Axes object.</p> Source code in <code>src/pyextremes/plotting/probability_plots.py</code> <pre><code>def plot_probability(\n    observed: np.ndarray,\n    theoretical: np.ndarray,\n    ax: Optional[plt.Axes] = None,\n    figsize: Tuple[float, float] = (8, 8),\n) -&gt; Tuple[plt.Figure, plt.Axes]:\n    \"\"\"\n    Plot a probability plot (QQ or PP).\n\n    Parameters\n    ----------\n    observed : numpy.ndarray\n        Observed values.\n    theoretical : numpy.ndarray\n        Theoretical values.\n    ax : matplotlib.axes._axes.Axes, optional\n        Axes onto which the probability plot is drawn.\n        If None (default), a new figure and axes objects are created.\n    figsize : tuple, optional\n        Figure size in inches in format (width, height).\n        By default it is (8, 8).\n\n    Returns\n    -------\n    figure : matplotlib.figure.Figure\n        Figure object.\n    axes : matplotlib.axes._axes.Axes\n        Axes object.\n\n    \"\"\"\n    with plt.rc_context(rc=pyextremes_rc):\n        # Create figure\n        if ax is None:\n            fig, ax = plt.subplots(figsize=figsize, dpi=96)\n        else:\n            try:\n                fig = ax.figure\n            except AttributeError as _error:\n                raise TypeError(\n                    f\"invalid type in {type(ax)} for the 'ax' argument, \"\n                    f\"must be matplotlib Axes object\"\n                ) from _error\n\n        # Configure axes\n        ax.grid(False)\n\n        # Plot scatter of observed and theoretical probabilities\n        ax.scatter(\n            theoretical,\n            observed,\n            marker=\"o\",\n            s=20,\n            lw=0.75,\n            facecolor=\"k\",\n            edgecolor=\"w\",\n            zorder=10,\n        )\n\n        # Plot a diagonal perfect-fit line\n        min_value = min([min(ax.get_xlim()), min(ax.get_ylim())])\n        max_value = max([max(ax.get_xlim()), max(ax.get_ylim())])\n        ax.plot(\n            [min_value, max_value],\n            [min_value, max_value],\n            color=\"#5199FF\",\n            lw=1,\n            ls=\"--\",\n            zorder=5,\n        )\n\n        # Label axes\n        ax.set_xlabel(\"Theoretical\")\n        ax.set_ylabel(\"Observed\")\n\n        # Calculate Pearson R statistic and show it in the figure\n        pearsonr, p_value = scipy.stats.pearsonr(theoretical, observed)\n        axes_range = max_value - min_value\n        ax.text(\n            x=min_value + 0.05 * axes_range,\n            y=max_value - 0.05 * axes_range,\n            s=f\"$R^2={pearsonr:.3f}$\\n$p={p_value:.3f}$\",\n            horizontalalignment=\"left\",\n            verticalalignment=\"top\",\n        )\n\n        # Set axes limits\n        ax.set_xlim(min_value, max_value)\n        ax.set_ylim(min_value, max_value)\n\n        return fig, ax\n</code></pre>"},{"location":"api/plotting/#pyextremes.plotting.mcmc.plot_trace","title":"<code>pyextremes.plotting.mcmc.plot_trace(trace, trace_map=None, burn_in=0, labels=None, figsize=None)</code>","text":"<p>Plot a trace plot for a given MCMC sampler trace.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>ndarray</code> <p>Array with MCMC sampler trace. Has a shape of (n_walkers, n_samples, n_parameters).</p> required <code>trace_map</code> <code>tuple</code> <p>Tuple with maximum aposteriori estimate of distribution parameters. If provided, MAP values are plotted as orange lines on top of the trace. If None (default) then MAP estimates are not plotted.</p> <code>None</code> <code>burn_in</code> <code>int</code> <p>Burn-in value (number of first steps to discard for each walker). By default it is 0 (no values are discarded).</p> <code>0</code> <code>labels</code> <code>list of strings</code> <p>Sequence of strings with parameter names, used to label axes. If None (default), then axes are labeled sequentially.</p> <code>None</code> <code>figsize</code> <code>tuple</code> <p>Figure size in inches. If None (default), then figure size is calculated automatically as 8 by 2 times number of parameters (<code>trace.shape[2]</code>).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>figure</code> <code>Figure</code> <p>Figure object.</p> <code>axes</code> <code>list</code> <p>List with <code>trace.shape[2]</code> Axes objects.</p> Source code in <code>src/pyextremes/plotting/mcmc.py</code> <pre><code>def plot_trace(\n    trace: np.ndarray,\n    trace_map: Optional[tuple] = None,\n    burn_in: int = 0,\n    labels: List[str] = None,\n    figsize: Optional[Tuple[float, float]] = None,\n) -&gt; Tuple[plt.Figure, list]:\n    \"\"\"\n    Plot a trace plot for a given MCMC sampler trace.\n\n    Parameters\n    ----------\n    trace : numpy.ndarray\n        Array with MCMC sampler trace.\n        Has a shape of (n_walkers, n_samples, n_parameters).\n    trace_map : tuple, optional\n        Tuple with maximum aposteriori estimate of distribution parameters.\n        If provided, MAP values are plotted as orange lines on top of the trace.\n        If None (default) then MAP estimates are not plotted.\n    burn_in : int, optional\n        Burn-in value (number of first steps to discard for each walker).\n        By default it is 0 (no values are discarded).\n    labels : list of strings, optional\n        Sequence of strings with parameter names, used to label axes.\n        If None (default), then axes are labeled sequentially.\n    figsize : tuple, optional\n        Figure size in inches.\n        If None (default), then figure size is calculated automatically\n        as 8 by 2 times number of parameters (`trace.shape[2]`).\n\n    Returns\n    -------\n    figure : matplotlib.figure.Figure\n        Figure object.\n    axes : list\n        List with `trace.shape[2]` Axes objects.\n\n    \"\"\"\n    # Parse the 'burn_in' argument\n    if not isinstance(burn_in, int):\n        raise TypeError(\n            f\"invalid type in {type(burn_in)} for the 'burn_in' argument, \"\n            f\"must be integer\"\n        )\n    if burn_in &lt; 0:\n        raise ValueError(\n            f\"invalid value in {burn_in} for the 'burn_in' argument, \"\n            f\"must be a positive integer\"\n        )\n    if burn_in &gt;= trace.shape[1]:\n        raise ValueError(\n            f\"'burn_in' value of {burn_in} exceeds number of samples {trace.shape[1]}\"\n        )\n\n    # Calculate figure size\n    n_parameters = trace.shape[2]\n    figsize = figsize or (8, 2 * n_parameters)\n\n    with plt.rc_context(rc=pyextremes_rc):\n        # Create figure\n        fig = plt.figure(figsize=figsize, dpi=96)\n\n        # Create gridspec\n        gs = matplotlib.gridspec.GridSpec(\n            nrows=n_parameters,\n            ncols=1,\n            wspace=0.1,\n            hspace=0.1,\n            width_ratios=[1],\n            height_ratios=np.full(shape=n_parameters, fill_value=1),\n        )\n\n        # Create and configure axes\n        axes = [fig.add_subplot(gs[i, 0]) for i in range(n_parameters)]\n        if labels is None:\n            labels = [f\"Parameter {i}\" for i in range(n_parameters)]\n        for i, ax in enumerate(axes):\n            ax.grid(False)\n            ax.set_ylabel(labels[i])\n            if i == n_parameters - 1:\n                ax.set_xlabel(\"Sample number\")\n                ax.xaxis.set_major_formatter(\n                    plt.FuncFormatter(lambda x, _: f\"{x:,.0f}\"),\n                )\n            else:\n                ax.tick_params(axis=\"x\", which=\"both\", labelbottom=False)\n\n        # Plot the trace plots\n        lines = np.full(\n            shape=(trace.shape[0], trace.shape[1] - burn_in, 2),\n            fill_value=np.nan,\n            dtype=np.float64,\n        )\n        lines[:, :, 0] = np.arange(burn_in + 1, trace.shape[1] + 1, 1)\n        for i, ax in enumerate(axes):\n            lines[:, :, 1] = trace[:, burn_in:, i]\n            line_collection = LineCollection(\n                segments=lines.copy(),\n                color=\"#231F20\",\n                lw=0.1,\n                zorder=5,\n            )\n            ax.add_collection(line_collection)\n            if trace_map is not None:\n                ax.axhline(trace_map[i], color=\"#F85C50\", lw=2, ls=\"--\", zorder=10)\n            ax.autoscale(enable=True, axis=\"both\", tight=False)\n\n        # Align y-labels\n        fig.align_ylabels(axs=axes)\n\n        return fig, axes\n</code></pre>"},{"location":"api/plotting/#pyextremes.plotting.mcmc.plot_corner","title":"<code>pyextremes.plotting.mcmc.plot_corner(trace, trace_map=None, burn_in=0, labels=None, levels=None, figsize=(8, 8))</code>","text":"<p>Plot a corner plot for a given MCMC sampler trace.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>ndarray</code> <p>Array with MCMC sampler trace. Has a shape of (n_walkers, n_samples, n_parameters).</p> required <code>trace_map</code> <code>tuple</code> <p>Tuple with maximum aposteriori estimate of distribution parameters. If provided, MAP values are plotted as orange lines. If None (default) then MAP estimates are not plotted.</p> <code>None</code> <code>burn_in</code> <code>int</code> <p>Burn-in value (number of first steps to discard for each walker). By default it is 0 (no values are discarded).</p> <code>0</code> <code>labels</code> <code>array - like</code> <p>Sequence of strings with parameter names, used to label axes. If None (default), then axes are labeled sequentially.</p> <code>None</code> <code>levels</code> <code>int</code> <p>Number of Gaussian KDE contours to plot. If None (default), then not shown.</p> <code>None</code> <code>figsize</code> <code>tuple</code> <p>Figure size in inches. By default it is (8, 8).</p> <code>(8, 8)</code> <p>Returns:</p> Name Type Description <code>figure</code> <code>Figure</code> <p>Figure object.</p> <code>axes</code> <code>list</code> <p>2D list with Axes objects of size N by N, where N is <code>trace.shape[2]</code>. Empty slots are represented by None. Axes are ordered from left to right top to bottom.</p> Source code in <code>src/pyextremes/plotting/mcmc.py</code> <pre><code>def plot_corner(\n    trace: np.ndarray,\n    trace_map: Optional[tuple] = None,\n    burn_in: int = 0,\n    labels: List[str] = None,\n    levels: Optional[int] = None,\n    figsize: Tuple[float, float] = (8, 8),\n) -&gt; Tuple[plt.Figure, list]:\n    \"\"\"\n    Plot a corner plot for a given MCMC sampler trace.\n\n    Parameters\n    ----------\n    trace : numpy.ndarray\n        Array with MCMC sampler trace.\n        Has a shape of (n_walkers, n_samples, n_parameters).\n    trace_map : tuple, optional\n        Tuple with maximum aposteriori estimate of distribution parameters.\n        If provided, MAP values are plotted as orange lines.\n        If None (default) then MAP estimates are not plotted.\n    burn_in : int, optional\n        Burn-in value (number of first steps to discard for each walker).\n        By default it is 0 (no values are discarded).\n    labels : array-like, optional\n        Sequence of strings with parameter names, used to label axes.\n        If None (default), then axes are labeled sequentially.\n    levels : int, optional\n        Number of Gaussian KDE contours to plot.\n        If None (default), then not shown.\n    figsize : tuple, optional\n        Figure size in inches. By default it is (8, 8).\n\n    Returns\n    -------\n    figure : matplotlib.figure.Figure\n        Figure object.\n    axes : list\n        2D list with Axes objects of size N by N, where N is `trace.shape[2]`.\n        Empty slots are represented by None. Axes are ordered from left to right\n        top to bottom.\n\n    \"\"\"\n    # Parse the 'burn_in' argument\n    if not isinstance(burn_in, int):\n        raise TypeError(\n            f\"invalid type in {type(burn_in)} for the 'burn_in' argument, \"\n            f\"must be integer\"\n        )\n    if burn_in &lt; 0:\n        raise ValueError(\n            f\"invalid value in {burn_in} for the 'burn_in' argument, \"\n            f\"must be a positive integer\"\n        )\n    if burn_in &gt;= trace.shape[1]:\n        raise ValueError(\n            f\"'burn_in' value of {burn_in} exceeds number of samples {trace.shape[1]}\"\n        )\n\n    n_parameters = trace.shape[2]\n\n    with plt.rc_context(rc=pyextremes_rc):\n        # Create figure\n        fig = plt.figure(figsize=figsize, dpi=96)\n\n        # Create gridspec\n        gs = matplotlib.gridspec.GridSpec(\n            nrows=n_parameters,\n            ncols=n_parameters,\n            wspace=0.1,\n            hspace=0.1,\n            width_ratios=[1] * n_parameters,\n            height_ratios=[1] * n_parameters,\n        )\n\n        # Create and configure axes\n        axes = [[None] * n_parameters for _ in range(n_parameters)]\n        for i in range(n_parameters):\n            for j in range(n_parameters):\n                # Create axes only for axes at or left of the main diagonal\n                if i &gt;= j:\n                    ax = fig.add_subplot(gs[i, j])\n                    ax.grid(False)\n                    axes[i][j] = ax\n\n                    if i == n_parameters - 1:\n                        # Set x-axis labels for axes located in the first row\n                        if labels is None:\n                            ax.set_xlabel(f\"Parameter {j}\")\n                        else:\n                            ax.set_xlabel(labels[j])\n                    else:\n                        # Remove x-axis ticks for axes located above the bottom row\n                        ax.tick_params(\n                            axis=\"x\", which=\"both\", labelbottom=False, length=0\n                        )\n\n                    if j == 0:\n                        # Set y-axis label for axes located in the first column\n                        if labels is None:\n                            ax.set_ylabel(f\"Parameter {i}\")\n                        else:\n                            ax.set_ylabel(labels[i])\n\n                    if j != 0 or i == j == 0:\n                        # Remove y-axis ticks for axes located right of the first column\n                        # and for the first axes along the main diagonal\n                        ax.tick_params(\n                            axis=\"y\", which=\"both\", labelleft=False, length=0\n                        )\n\n                    if i == j:\n                        # Plot histogram\n                        parameter_samples = trace[:, burn_in:, i].flatten()\n                        ax.hist(\n                            parameter_samples,\n                            bins=np.histogram_bin_edges(\n                                a=parameter_samples, bins=\"auto\"\n                            ),\n                            density=True,\n                            histtype=\"step\",\n                            edgecolor=\"#231F20\",\n                            lw=0.5,\n                            zorder=5,\n                        )\n                        if trace_map is not None:\n                            ax.axvline(\n                                trace_map[i], color=\"#F85C50\", lw=1, ls=\"--\", zorder=10\n                            )\n                    else:\n                        # Plot scatter plot\n                        parameter_i = trace[:, burn_in:, i].flatten()\n                        parameter_j = trace[:, burn_in:, j].flatten()\n                        ax.scatter(\n                            parameter_j,\n                            parameter_i,\n                            marker=\"o\",\n                            s=2,\n                            alpha=0.1,\n                            facecolor=\"#231F20\",\n                            edgecolor=\"None\",\n                            lw=0,\n                            zorder=5,\n                        )\n\n                        # Plot trace map lines\n                        if trace_map is not None:\n                            ax.axvline(\n                                trace_map[j], color=\"#F85C50\", lw=1, ls=\"--\", zorder=15\n                            )\n                            ax.axhline(\n                                trace_map[i], color=\"#F85C50\", lw=1, ls=\"--\", zorder=15\n                            )\n\n                        # Plot Gaussian KDE contour\n                        if levels is not None:\n                            kernel = scipy.stats.gaussian_kde(\n                                np.vstack([parameter_j, parameter_i])\n                            )\n                            xx, yy = np.meshgrid(\n                                np.linspace(parameter_j.min(), parameter_j.max(), 100),\n                                np.linspace(parameter_i.min(), parameter_i.max(), 100),\n                            )\n                            zz = np.reshape(\n                                kernel(np.vstack([xx.ravel(), yy.ravel()])).transpose(),\n                                xx.shape,\n                            )\n                            ax.contour(\n                                xx,\n                                yy,\n                                zz,\n                                colors=\"w\",\n                                levels=levels,\n                                linewidths=1,\n                                linestyles=\"-\",\n                                zorder=10,\n                            )\n\n                    # Set axes limits\n                    parameter_i = trace[:, burn_in:, i].flatten()\n                    parameter_j = trace[:, burn_in:, j].flatten()\n                    xlim = (parameter_j.min(), parameter_j.max())\n                    ylim = (parameter_i.min(), parameter_i.max())\n                    ax.set_xlim(xlim)\n                    if i != j:\n                        ax.set_ylim(ylim)\n\n        # Align labels\n        fig.align_labels()\n\n        return fig, axes\n</code></pre>"},{"location":"user-guide/1-read-first/","title":"Read First","text":""},{"location":"user-guide/1-read-first/#disclaimer","title":"Disclaimer","text":"<p>pyextremes was created to make the process of running EVA simpler and faster. While the project is built with reasonable defaults which produce good results in most cases, one should not rely on the model as a source of ground truth. It is always the responsibility of the user to understand the subject of analysis and to properly interpret the model outputs.</p> <p>Example</p> <p>A 1000-year wave height of 100 meters is not physical and is an artifact of the underlying statistical model. One should always be mindful of the actual process being modeled and remember that the model gives a proabilistic estimate of extreme values under the assumption that the selected model (e.g. GEVD or GPD) correctly describes the underlying process (in this example, water waves).</p>"},{"location":"user-guide/1-read-first/#tutorial-structure","title":"Tutorial Structure","text":"<p>Each tutorial section covers a particular area of EVA, such as extreme value extraction, fitting a model, or summarizing and visualizing analysis results. pyextremes was built in a modular fashion where each of these components is implemented independently and can be used on its own. In order to make life easier a helper class <code>EVA</code> was created (located in <code>pyextremes.eva.EVA</code>) which chains these components together to streamline the most common types of EVA workflows and to reduce the amount of code a user needs to write when performing analysis.</p> <p>When possible, sections of this tutorial present two alternative ways to perform the same action: via <code>EVA</code> and via low-level functions which are used by <code>EVA</code> behind the scenes.</p>"},{"location":"user-guide/10-selecting-distribution/","title":"10 selecting distribution","text":"<p>GEVD and GPD, degeneracy, model comparison (AIC, likelihood ratio).</p>"},{"location":"user-guide/11-return-values/","title":"11 return values","text":"<p>Discrete return values and summary.</p>"},{"location":"user-guide/13-visualization/","title":"13 visualization","text":"<p>Diagnostic and its components.</p>"},{"location":"user-guide/14-goodness-of-fit/","title":"14 goodness of fit","text":"<p>Kolmogorov-Smirnov and other (WIP).</p>"},{"location":"user-guide/2-extreme-value-types/","title":"Extreme Value Types","text":"<p>Traditionally EVA recognizes two types of extreme values:</p> <ul> <li>Block maxima (BM)</li> <li>Peaks over threshold (POT)</li> </ul> <p>The BM and POT extreme values are used to apply a statistical model (GEVD or GPD accordingly) to allow for estimation of extreme events of an arbitrary probability of exceedance (return period). Both of these extreme value types represent a valid path of performing EVA and neither of these is generally better than another.</p> <p>Info</p> <p>GEVD and GPD models used for BM and POT extreme value types accordingly have a common theoretical basis and should be treated as complementary models, not as either/or. More information about why only the GEVD and GPD models are recommended to be used is provided in subsequent sections of this tutorial.</p>"},{"location":"user-guide/2-extreme-value-types/#block-maxima","title":"Block Maxima","text":"<p>The Block Maxima extreme values are extracted by selecting a block size (typically 1 year, also the default in pyextremes), then partitioning time series of your signal into equal consequtive blocks of this size, and for each block getting single maximum value (thus, block maxima). The resulting BM time series are then used to model extreme value behavior using the GEVD model family. See figure below illustrating this concept:</p>"},{"location":"user-guide/2-extreme-value-types/#peaks-over-threshold","title":"Peaks Over Threshold","text":"<p>The Peaks Over Threshold extreme values are extracted by choosing a threshold and selecting values higher or lower (depends on which extreme process is analyzed) than the threshold value. The selected values are then declustered by specifying minimum distance between adjacent clusters (e.g. 24 hours, which means that the model assumes that clusters of exceedances separater by this distiance or larger are independent). Selection of threshold and declustering distance is not a trivial task and has a strong effect on the EVA results. pyextremes provides a series of tools which help with threshold selection, these are described further in this tutorial. See figure below illustrating POT extremes:</p>"},{"location":"user-guide/2-extreme-value-types/#which-method-to-use","title":"Which Method to Use","text":"<p>One of the most important concepts of EVA is convergenece. What this means practically is that different models and approaches (as long as they are correctly applied) should be converging to the same answer (within reasonable confidence limits). Because of this, we cannot say that one method is better than another. Therefore, for a thorough analysis, user is advised to investigate both routes to make sure that the final answer of the analysis provides a robust estimate.</p> <p>A general rule of thumb, which is based on years of author's (subjective) experience, when performing EVA is to do the following:</p> <ul> <li>Use BM with a reasonable block size to avoid capturing seasonality   (read more in the next section) to get the first estimates and to see how the extremes   behave. Generally, BM is a \"simpler\" and more stable model which requires very little   input from the user.</li> <li>Use POT with a reasonable threshold and declustering parameters   (read more in the next section) to investigate how stable the model is in the region   of target exceedance probabilities (return periods)   and to gain more confidence in your results.</li> </ul> <p>Attention</p> <p>User is strongly discouraged from \"playing\" with the model parameters until a desired answer is achieved. EVA is not about getting a perfect estimate and a lack of a definitive answer is an answer in itself - it signifies that underlying process may be not random (e.g. seasonality or some trends were not removed prior to performing EVA), that the model is poorly behaved for the data, or that there is simply not enough data to provide an accurate answer for a given probability. This is typically reflected by wide confidence intervals showing that the answer has high uncertainty. For example, your 100-year wind speed may be 50 knots (seems reasonable, right?) but the 95% confidence interval shows 10 to 120 knot uncertainty range - this clearly indicates that, while your answer happened to appear reasonable, the model is telling you that you cannot make any good faith judgement about the 100-year extreme event when using this data and methodology.</p>"},{"location":"user-guide/3-block-maxima/","title":"Block Maxima","text":"<p>Block Maxima or Minima (BM) extreme values are extracted from time series by partitioning it into blocks (segments) of equal duration (e.g. 1 year) and locating maximum or minimum values within each block. Block maxima extreme values asymptotically follow the Generalized Extreme Value Distribution family, according to the Fisher\u2013Tippett\u2013Gnedenko theorem. This theorem demonstrates that the GEVD family is the only possible limit for the block maxima extreme values.</p>"},{"location":"user-guide/3-block-maxima/#extracting-extremes","title":"Extracting Extremes","text":"<p>As outlined in the Read First section of this documentation, there are multiple ways the same thing can be achieved in <code>pyextremes</code>. The BM extraction function can be accessed via:</p> <ul> <li><code>pyextremes.extremes.block_maxima.get_extremes_block_maxima</code> - the lowest level</li> <li><code>pyextremes.get_extremes</code> - general-purpose extreme value extraction function</li> <li><code>pyextremes.EVA.get_extremes</code> - helper-class   (extreme values are not returned by this function, but instead are set   on the <code>EVA</code> instance in the <code>.extremes</code> attribute)</li> </ul> <p>The simplest way to extract extreme values using BM method is to use the default parameters of the <code>get_extremes</code> function:</p> StandaloneUsing EVA <pre><code>from pyextremes import get_extremes\nfrom pyextremes.plotting import plot_extremes\n\nextremes = get_extremes(data, \"BM\")\nplot_extremes(\n    ts=data,\n    extremes=extremes,\n    extremes_method=\"BM\",\n    extremes_type=\"high\",\n    block_size=\"365.2425D\",\n)\n</code></pre> <pre><code>from pyextremes import EVA\n\nmodel = EVA(data=data)\nmodel.get_extremes(\"BM\")\nmodel.plot_extremes()\n</code></pre> Note <p>You can get the <code>data</code> variable referenced above by running the following code:</p> <pre><code>data = pd.read_csv(\n    \"battery_wl.csv\",\n    index_col=0,\n    parse_dates=True,\n).squeeze()\ndata = (\n    data\n    .sort_index(ascending=True)\n    .astype(float)\n    .dropna()\n    .loc[pd.to_datetime(\"1980\"):pd.to_datetime(\"1995\")]\n)\ndata = (\n  data - (data.index.array - pd.to_datetime(\"1992\"))\n) / pd.to_timedelta(\"365.2425D\") * 2.87e-3\n</code></pre> <p><code>\"battery_wl.csv\"</code> can be downloaded here.</p> <p>All figures shown in this tutorial section were generated using this jupyter notebook.</p> <p>The <code>get_extremes</code> function uses the following parameters:</p> <ul> <li>ts - time series (<code>pandas.Series</code>) from which the extreme values are extracted</li> <li>method - extreme value extraction method: <code>\"BM\"</code> for Block Maxima   and <code>\"POT\"</code> for Peaks Over Threshold.</li> <li>extremes_type - extreme value type:   <code>\"high\"</code> for maxima (default) and <code>\"low\"</code> for minima</li> </ul> <p>The following paramters are used only when <code>method=\"BM\"</code>:</p> <ul> <li>block_size - block size, by default <code>\"365.2425D\"</code>.   Internally is converted using the <code>pandas.to_timedelta</code> function.</li> <li>errors - specifies what to do when a block is empty (has no values).   <code>\"raise\"</code> (default) raises error, <code>\"ignore\"</code> skips such blocks   (not recommended), and <code>\"coerce\"</code> sets values for such blocks as average   of extreme values in other blocks.</li> <li>min_last_block - minimum data availability ratio (0 to 1)   in the last block. If the last block is shorter than this ration   (e.g. 0.25 corresponds to 3 months for a block size of 1 year) then it is not used   to get extreme values. This argument is useful to avoid situations when the last   block is very small. By default this is <code>None</code>, which means that last   block is always used.</li> </ul> <p>If we specify all of these parameters then the function would look as:</p> <pre><code>get_extremes(\n    ts=data,\n    method=\"BM\",\n    extremes_type=\"high\",\n    block_size=\"365.2425D\",\n    errors=\"raise\",\n    min_last_block=None,\n)\n</code></pre>"},{"location":"user-guide/3-block-maxima/#selecting-block-size","title":"Selecting Block Size","text":"<p>Like with most choices in statistics, selection of block size involves making a trade-off between bias and variance: blocks that are too small mean that approximation by the limit model (GEVD) is likely to be poor, leading to bias in estimation and extrapolation; large blocks generate few block maxima/minima, leading to large estimation variance. Pragmatic considerations often lead to the adoption of blocks of length one year. (Coles, 2004)</p> <p>An important thing to consider is also the physical nature of investigated signal. Many meteorological events (e.g. snowfall, rain, waves) are seasonal and, therefore, selection of block sizes smaller than 1-year would result in significant bias due to blocks no longer being equivalent (e.g. summer blocks are nearly guaranteed to have no snow).</p> <p>We can specify different block size using the <code>block_size</code> argument. Using the same data as above but with a block size of 2 years we get:</p> StandaloneUsing EVA <pre><code>extremes = get_extremes(\n    ts=data,\n    method=\"BM\",\n    block_size=pd.to_timedelta(\"365.2425D\") * 2,\n)\nplot_extremes(\n    ts=data,\n    extremes=extremes,\n    extremes_method=\"BM\",\n    extremes_type=\"high\",\n    block_size=pd.to_timedelta(\"365.2425D\") * 2,\n)\n</code></pre> <pre><code>model = EVA(data=data)\nmodel.get_extremes(\"BM\", block_size=pd.to_timedelta(\"365.2425D\") * 2)\nmodel.plot_extremes()\n</code></pre>"},{"location":"user-guide/3-block-maxima/#block-minima","title":"Block Minima","text":"<p>Block minima is fully equivalent to block maxima in the way it is extracted. Block minima can be extracted by setting the <code>extremes_type</code> argument to <code>\"low\"</code>:</p> StandaloneUsing EVA <pre><code>extremes = get_extremes(\n    ts=data,\n    method=\"BM\",\n    extremes_type=\"low\",\n)\nplot_extremes(\n    ts=data,\n    extremes=extremes,\n    extremes_method=\"BM\",\n    extremes_type=\"high\",\n    block_size=\"365.2425D\",\n)\n</code></pre> <pre><code>model = EVA(data=data)\nmodel.get_extremes(\"BM\", extremes_type=\"low\")\nmodel.plot_extremes()\n</code></pre> <p>Tip</p> <p>The <code>pyextremes.EVA</code> class works identically for both maxima and minima series and properly reflects (rotates) the data to fit statistical distributions. This is true as long as the <code>extremes_type</code> argument is correctly specified.</p> <p>Warning</p> <p>When analyzing block minima be mindful of your data being censored. An example of this would be water level time series - water levels cannot go below the seabed and will, therefore, be censored by the seabed elevation. Such series would no longer follow the GEVD and any results of such analysis would be unreliable.</p>"},{"location":"user-guide/4-peaks-over-threshold/","title":"Peaks Over Threshold","text":"<p>Peaks Over Threshold (POT) extreme values are extracted from time series by first generating a time series of exceedances by selecting values above (or below for <code>extremes_type=\"low\"</code>) a certain threshold and then declustering the exceedance time series by identifying clusters separated by a given time period and then selecting only the highest (lowest) values within each cluster. Declustering is performed in order to ensure that these values are IID (independent and identically distributed) which is required for the corresponding limit distribution to be applicable. The POT extreme values asymptotically follow the Generalized Pareto Distribution famliy, according to the Pickands\u2013Balkema\u2013De Haan theorem.</p>"},{"location":"user-guide/4-peaks-over-threshold/#extracting-extremes","title":"Extracting Extremes","text":"<p>As outlined in the Read First section of this documentation, there are multiple ways the same thing can be achieved in <code>pyextremes</code>. The POT extraction function can be accessed via:</p> <ul> <li><code>pyextremes.extremes.peaks_over_threshold.get_extremes_peaks_over_threshold</code> - the lowest level</li> <li><code>pyextremes.get_extremes</code> - general-purpose extreme value extraction function</li> <li><code>pyextremes.EVA.get_extremes</code> - helper-class   (extreme values are not returned by this function, but instead are set   on the <code>EVA</code> instance in the <code>.extremes</code> attribute)</li> </ul> <p>The simplest way to extract extreme values using BM method is to use the default parameters of the <code>get_extremes</code> function:</p> StandaloneUsing EVA <pre><code>from pyextremes import get_extremes\nfrom pyextremes.plotting import plot_extremes\n\nextremes = get_extremes(data, \"POT\", threshold=0.5, r=\"12h\")\nplot_extremes(\n    ts=data,\n    extremes=extremes,\n    extremes_method=\"POT\",\n    extremes_type=\"high\",\n    threshold=0.5,\n    r=\"12h\",\n)\n</code></pre> <pre><code>from pyextremes import EVA\n\nmodel = EVA(data=data)\nmodel.get_extremes(\"POT\", threshold=0.5, r=\"12h\")\nmodel.plot_extremes(show_clusters=True)\n</code></pre> Note <p>You can get the <code>data</code> variable referenced above by running the following code:</p> <pre><code>data = pd.read_csv(\n    \"battery_wl.csv\",\n    index_col=0,\n    parse_dates=True,\n).squeeze()\ndata = (\n    data\n    .sort_index(ascending=True)\n    .astype(float)\n    .dropna()\n    .loc[pd.to_datetime(\"1980/01/01\"):pd.to_datetime(\"1980/01/20\")]\n)\ndata = (\n  data - (data.index.array - pd.to_datetime(\"1992\"))\n) / pd.to_timedelta(\"365.2425D\") * 2.87e-3\n</code></pre> <p><code>\"battery_wl.csv\"</code> can be downloaded here.</p> <p>All figures shown in this tutorial section were generated using this jupyter notebook.</p> <p>The <code>get_extremes</code> function uses the following parameters:</p> <ul> <li>ts - time series (<code>pandas.Series</code>) from which the extreme values are extracted</li> <li>method - extreme value extraction method: <code>\"BM\"</code> for Block Maxima   and <code>\"POT\"</code> for Peaks Over Threshold.</li> <li>extremes_type - extreme value type:   <code>\"high\"</code> for above threshold (default)   and <code>\"low\"</code> for below threshold.</li> </ul> <p>The following paramters are used only when <code>method=\"POT\"</code>:</p> <ul> <li>threshold - threshold value.</li> <li>r - minimum time distance (window duration) between adjacent clusters. Used   to decluster exceedances by locating clusters where all exceedances are separated   by distances no more than <code>r</code> and then locating maximum or minimum   (depends on <code>extremes_type</code>) values within each cluster.   By default <code>r=\"24h\"</code> (24 hours).</li> </ul> <p>If we specify all of these parameters then the function would look as:</p> <pre><code>get_extremes(\n    ts=data,\n    method=\"POT\",\n    extremes_type=\"high\",\n    threshold=0.5,\n    r=\"12h\",\n)\n</code></pre>"},{"location":"user-guide/4-peaks-over-threshold/#declustering","title":"Declustering","text":"<p>As described earlier, declustering is controlled using the <code>r</code> argument. The goal of declustering is to ensure that all extreme values are IID, which is a requirement for the GPD model to be valid. Shown below is an example of extremes extracted from the same data and using the same threshold as above, but with a larger <code>r</code> value:</p> StandaloneUsing EVA <pre><code>extremes = get_extremes(data, \"POT\", threshold=0.5, r=\"24h\")\nplot_extremes(\n    ts=data,\n    extremes=extremes,\n    extremes_method=\"POT\",\n    extremes_type=\"high\",\n    threshold=0.5,\n    r=\"24h\",\n)\n</code></pre> <pre><code>model = EVA(data=data)\nmodel.get_extremes(\"POT\", threshold=0.5, r=\"24h\")\nmodel.plot_extremes(show_clusters=True)\n</code></pre> <p>Tip</p> <p>Declustering value of <code>r=\"24h\"</code> was selected as a default value because <code>pyextremes</code> was developed when working with meteorological phenomena - namely, storms. Extreme storm surge and waves are generally caused by a storm event which generally doesn't exceed 12-24 hours and, because of this, the assumption of 24-hour inter-cluster distance results in a reasonably good separation of independent storm events.</p> <p>User is advised to select this parameter based on the properties of studied phenomena. For example, extreme precipitation events in some regions of the world can last for more than several days and, because of this, the default value of 24 hours would not be adequate for such analysis.</p>"},{"location":"user-guide/4-peaks-over-threshold/#peaks-below-threshold","title":"Peaks Below Threshold","text":"<p>A special case of Peaks Over Threshold is when instead of selecting values above the threshold we select values below it. Such values can be extracted by setting the <code>extremes_type</code> argument to <code>\"low\"</code>:</p> StandaloneUsing EVA <pre><code>extremes = get_extremes(\n    data,\n    \"POT\",\n    threshold=0.5,\n    r=\"24h\",\n    extremes_type=\"low\",\n)\nplot_extremes(\n    ts=data,\n    extremes=extremes,\n    extremes_method=\"POT\",\n    extremes_type=\"low\",\n    threshold=0.5,\n    r=\"24h\",\n)\n</code></pre> <pre><code>model = EVA(data=data)\nmodel.get_extremes(\"POT\", threshold=0.5, r=\"24h\", extremes_type=\"low\")\nmodel.plot_extremes(show_clusters=True)\n</code></pre> <p>Tip</p> <p>The <code>pyextremes.EVA</code> class works identically for both peaks over and below threshold series and properly reflects (rotates) the data to fit statistical distributions. This is true as long as the <code>extremes_type</code> argument is correctly specified.</p> <p>Warning</p> <p>When analyzing POT with <code>extremes_type=\"low\"</code> be mindful of your data being censored. An example of this would be water level time series - water levels cannot go below the seabed and will, therefore, be censored by the seabed elevation. Such series would no longer follow the GPD and any results of such analysis would be unreliable.</p>"},{"location":"user-guide/5-threshold-selection/","title":"Threshold Selection","text":"<p>Selection of the threshold value is a very important step because it has the strongest effect on the results of EVA. The core idea of threshold selection is the same as when selecting block size in the Block Maxima approach - it is a trade-off between bias and variance. Larger threshold values produce few extreme values and lead to large variance in result (confidence bounds), while smaller threshold values generate a sample which poorly approximates the GPD model. The opposite is true when performing EVA for extreme low values (<code>extremes_type=\"low\"</code>).</p> <p>The key goal of threshold selection can, therefore, be formulated as follows:</p> <p>Goal of threshold selection</p> <p>Select the smallest threshold value among those which produce extreme values following the limit exceedance model (Generalized Pareto Distribution family).</p> <p>Warning</p> <p>Threshold selection is probably the hardest part of Extreme Value Analysis when analyzing extreme values obtained using the Peaks Over Threshold method. It involves a great deal of subjective judgement and should be performed in conjunction with other methods, such as Block Maxima + GEVD, to gain more confidence in the validty of obtained results.</p>"},{"location":"user-guide/5-threshold-selection/#mean-residual-life","title":"Mean Residual Life","text":"<p>Mean residual life plot plots average excess value over given threshold for a series of thresholds. The idea is that the mean residual life plot should be approximately linear above a threshold for which the Generalized Pareto Distribution model is valid.</p> <pre><code>from pyextremes import plot_mean_residual_life\n\nplot_mean_residual_life(data)\n</code></pre> Note <p>You can get the <code>data</code> variable referenced above by running the following code:</p> <pre><code>data = pd.read_csv(\n    \"battery_wl.csv\",\n    index_col=0,\n    parse_dates=True,\n).squeeze()\ndata = (\n    data\n    .sort_index(ascending=True)\n    .astype(float)\n    .dropna()\n    .loc[pd.to_datetime(\"1925\"):]\n)\ndata = (\n  data - (data.index.array - pd.to_datetime(\"1992\"))\n) / pd.to_timedelta(\"365.2425D\") * 2.87e-3\n</code></pre> <p><code>\"battery_wl.csv\"</code> can be downloaded here.</p> <p>All figures shown in this tutorial section were generated using this jupyter notebook.</p> <p>As seen in the figure above, exceedance values are approximately linear between threshold values of 1.2 and 1.8. This provides a range of threshold values which can be further investigated using other methods.</p> <p>The <code>plot_mean_residual_life</code> function uses the following parameters:</p> <ul> <li>ts - time series (<code>pandas.Series</code>) from which the extreme values are extracted</li> <li>thresholds - array of threshold for which the plot is displayed. By default   100 equally-spaced thresholds between 90th (10th if <code>extremes_type=\"high\"</code>)   percentile and 10th largest (smallest if <code>extremes_type=\"low\"</code>)   value in the series.</li> <li>extremes_type - extreme value type:   <code>\"high\"</code> for above threshold (default)   and <code>\"low\"</code> for below threshold.</li> <li>alpha - confidence interval width in the range (0, 1), by default it is 0.95.   If None, then confidence interval is not shown.</li> <li>ax - matplotlib Axes object. If provided, then the plot is drawn on this axes.   If None (default), new figure and axes are created</li> <li>figsize - figure size in inches in format (width, height).   By default it is (8, 5).</li> </ul> <p>Note</p> <p>In author's (subjective) opinion this is the least useful technique among those listed in this section because mean residual life plots are very hard to interpret.</p>"},{"location":"user-guide/5-threshold-selection/#parameter-stability","title":"Parameter Stability","text":"<p>Parameter stability plot shows how shape and modified scale parameters of the Generalized Pareto Distribution change over a range of threshold values. The idea is that these parameters should be stable (vary by small amount) within a range of valid thresholds.</p> <pre><code>from pyextremes import plot_parameter_stability\n\nplot_parameter_stability(data)\n</code></pre> <p>As seen in the figure above, these parameters appear to stabilize around threshold value of 1.2 with subsequent values having higher variance due to smaller number of exceedances.</p> <p>The <code>plot_parameter_stability</code> function uses the following parameters:</p> <ul> <li>ts - time series (<code>pandas.Series</code>) from which the extreme values are extracted</li> <li>thresholds - array of threshold for which the plot is displayed. By default   100 equally-spaced thresholds between 90th (10th if <code>extremes_type=\"high\"</code>)   percentile and 10th largest (smallest if <code>extremes_type=\"low\"</code>)   value in the series.</li> <li>r - minimum time distance (window duration) between adjacent clusters. Used   to decluster exceedances by locating clusters where all exceedances are separated   by distances no more than <code>r</code> and then locating maximum or minimum   (depends on <code>extremes_type</code>) values within each cluster.   By default <code>r=\"24h\"</code> (24 hours).</li> <li>extremes_type - extreme value type:   <code>\"high\"</code> for above threshold (default)   and <code>\"low\"</code> for below threshold.</li> <li>alpha - confidence interval width in the range (0, 1), by default it is 0.95.   If None, then confidence interval is not shown.</li> <li>n_samples - number of bootstrap samples used to estimate confidence   interval bounds (default=100). Ignored if <code>alpha</code> is None.</li> <li>axes - tuple with matplotlib Axes (ax_shape, ax_scale) for shape and scale values.   If None (default), new figure and axes are created.</li> <li>figsize - figure size in inches in format (width, height).   By default it is (8, 5).</li> <li>progress - if True, shows tqdm progress bar. By default False.   Requires <code>tqdm</code> package.</li> </ul>"},{"location":"user-guide/5-threshold-selection/#return-value-stability","title":"Return Value Stability","text":"<p>An extension of the previous technique is to investigate stability of a target return value with a pre-defined return period over a range of thresholds. This technique provides a more intuitive metric of model stability. Let's plot it for the range of thresholds identified earlier:</p> <pre><code>from pyextremes import plot_return_value_stability\n\nplot_return_value_stability(\n    data,\n    return_period=100,\n    thresholds=np.linspace(1.2, 1.8, 20),\n    alpha=0.95,\n)\n</code></pre> <p>As seen in the figure above, the model is very stable for threshold values above 1.4.</p> <p>The <code>plot_return_value_stability</code> function uses the following parameters:</p> <ul> <li>ts - time series (<code>pandas.Series</code>) from which the extreme values are extracted</li> <li>return_period - return period given as a multiple of <code>return_period_size</code>.</li> <li>return_period_size - size of return period. Same as the <code>r</code> argument.   By default this is 1 year.</li> <li>thresholds - array of threshold for which the plot is displayed. By default   100 equally-spaced thresholds between 90th (10th if <code>extremes_type=\"high\"</code>)   percentile and 10th largest (smallest if <code>extremes_type=\"low\"</code>)   value in the series.</li> <li>r - minimum time distance (window duration) between adjacent clusters. Used   to decluster exceedances by locating clusters where all exceedances are separated   by distances no more than <code>r</code> and then locating maximum or minimum   (depends on <code>extremes_type</code>) values within each cluster.   By default <code>r=\"24h\"</code> (24 hours).</li> <li>extremes_type - extreme value type:   <code>\"high\"</code> for above threshold (default)   and <code>\"low\"</code> for below threshold.</li> <li>distributions - list of distributions for which the plot is produced.   By default these are \"genpareto\" and \"expon\".   A distribution must be either a name of distribution from <code>scipy.stats</code>   or a subclass of scipy.stats.rv_continuous.   See scipy.stats documentation</li> <li>alpha - confidence interval width in the range (0, 1), by default it is 0.95.   If None, then confidence interval is not shown.</li> <li>n_samples - number of bootstrap samples used to estimate confidence   interval bounds (default=100). Ignored if <code>alpha</code> is None.</li> <li>ax - matplotlib Axes object. If provided, then the plot is drawn on this axes.   If None (default), new figure and axes are created</li> <li>figsize - figure size in inches in format (width, height).   By default it is (8, 5).</li> <li>progress - if True, shows tqdm progress bar. By default False.   Requires <code>tqdm</code> package.</li> </ul> <p>Warning</p> <p>This is the most dangerous threshold selection technique presented in this section. It can be abused by selecting a threshold value which gives a desired result. Results of such analysis would be biased and invalid. Analysts should honestly present results of their analysis and high variance in answer should be considered a valuable result as well - it indicates that available data can be used to obtain reliable results and that there is high uncertainty in the analyzed process.</p>"},{"location":"user-guide/5-threshold-selection/#putting-it-all-together","title":"Putting it all Together","text":"<p><code>pyextremes</code> provides a convenience function to put all of the above together. It also adds an additional plot - AIC curve indicating relative model performance. The AIC curve should not be used as a threshold selection tool because it will always have the same logarithmic shape. Instead, it should guide the user as to which model (e.g. GEVD or Exponential) should be preferred for a given threshold.</p> <pre><code>from pyextremes import plot_threshold_stability\n\nplot_threshold_stability(\n    data,\n    return_period=100,\n    thresholds=np.linspace(1.2, 1.8, 20),\n)\n</code></pre> <p>Based on the figures shown earlier, one may conclude that the valid threshold may lie between 1.4 and 1.6. A decision was made to select threshold value of 1.5.</p> <p>The <code>plot_threshold_stability</code> function uses the following parameters:</p> <ul> <li>ts - time series (<code>pandas.Series</code>) from which the extreme values are extracted</li> <li>return_period - return period given as a multiple of <code>return_period_size</code>.</li> <li>return_period_size - size of return period. Same as the <code>r</code> argument.   By default this is 1 year.</li> <li>thresholds - array of threshold for which the plot is displayed. By default   100 equally-spaced thresholds between 90th (10th if <code>extremes_type=\"high\"</code>)   percentile and 10th largest (smallest if <code>extremes_type=\"low\"</code>)   value in the series.</li> <li>r - minimum time distance (window duration) between adjacent clusters. Used   to decluster exceedances by locating clusters where all exceedances are separated   by distances no more than <code>r</code> and then locating maximum or minimum   (depends on <code>extremes_type</code>) values within each cluster.   By default <code>r=\"24h\"</code> (24 hours).</li> <li>extremes_type - extreme value type:   <code>\"high\"</code> for above threshold (default)   and <code>\"low\"</code> for below threshold.</li> <li>distributions - list of distributions for which the plot is produced.   By default these are \"genpareto\" and \"expon\".   A distribution must be either a name of distribution from <code>scipy.stats</code>   or a subclass of scipy.stats.rv_continuous.   See scipy.stats documentation</li> <li>alpha - confidence interval width in the range (0, 1), by default it is 0.95.   If None, then confidence interval is not shown.</li> <li>n_samples - number of bootstrap samples used to estimate confidence   interval bounds (default=100). Ignored if <code>alpha</code> is None.</li> <li>ax - matplotlib Axes object. If provided, then the plot is drawn on this axes.   If None (default), new figure and axes are created</li> <li>figsize - figure size in inches in format (width, height).   By default it is (8, 5).</li> <li>progress - if True, shows tqdm progress bar. By default False.   Requires <code>tqdm</code> package.</li> </ul> <p>Results of selecting the threshold value 1.5 are shown below:</p>"},{"location":"user-guide/6-return-periods/","title":"Estimating Return Periods","text":"<p>This section demonstrates how empirical probabilities (return periods) can be obtained for extreme values extracted using methods described in earlier sections.</p>"},{"location":"user-guide/6-return-periods/#what-is-return-period","title":"What is Return Period","text":"<p>Return period indicates duration of time (typically years) which corresponds to a probability that a given value (e.g. wind speed) would be exceeded at least once within a year. This probability is called probability of exceedance and is related to return periods as <code>1/p</code> where <code>p</code> is return period.</p> <p>Coles (2001, p.49)</p> <p>In common terminology, \\(z_{p}\\) is the return level associated with the return period \\(1/p\\), since to a reasonable degree of accuracy, the level \\(z_{p}\\) is expected to be exceeded on average once every \\(1/p\\) years. More precisely, \\(z_{p}\\) is exceeded by the annual maximum in any particular year with probability \\(p\\).</p> <p>Return periods are often incorrectly interpreted in the professional communities as \"100-year event is an event which happens only once in 100 years\", which may lead to inaccurate assessment of risks. A more holistic way of looking at this is to consider a time period within which a risk is evaluated. For example, a 100-year event with probability of exceedance in any given year of 1% would have a probability of ~39.5% to be exceeded at least once within 50 years - this is calculated using this formula:</p> \\[1 - (1 - p) ^ n\\] <p>Where \\(n\\) is number of return period blocks within a time period (50 for 50 years with retun period block of size 1 year) and \\(p\\) is 1% (100-year event).</p>"},{"location":"user-guide/6-return-periods/#empirical-return-periods","title":"Empirical Return Periods","text":"<p>Empirical return periods are assigned to observed extreme values using an empirical rule where extreme values are ordered and ranked from the most extreme (1) to the least extreme (n), then exceedance probabilities are calculated (see the following sub-section), and return periods are calculated as multiples of a given <code>return_period_size</code> (typically 1 year).</p>"},{"location":"user-guide/6-return-periods/#probability-of-exceedance","title":"Probability of Exceedance","text":"<p>Extreme events extracted using BM or POT methods are assigned exceedance probabilities using the following formula:</p> \\[P = \\frac{r - \\alpha}{n + 1 - \\alpha - \\beta}\\] <p>where:</p> <ul> <li>r - rank of extreme value (1 to n). In <code>pyextremes</code> rank is calculated using   <code>scipy.stats.rankdata</code> with <code>method=\"average\"</code>, which means that extreme   events of the same magnitude are assigned average of ranks these values would be   assigned otherwise if ranked sequentially. For example, array of <code>[1, 2, 3, 3, 4]</code>   would have ranks of <code>[5, 4, 2.5, 2.5, 1]</code>.</li> <li>n - number of extreme values.</li> <li>\\(\\alpha\\) and \\(\\beta\\) - empricial plotting position parameters (see further below).</li> </ul> <p>In this context \\(P\\) corresponds to a probability of exceedance of a value with rank r in a any given time period with duration \\(t/n\\) where \\(t\\) is total duration of series from which the extreme values were drawn and \\(n\\) is number of extreme events. If we measure time in years and we use Block Maxima with block size of 1 year, then the formula \\(t/n\\) becomes 1 by definition and the return period in years can be calculated as \\(1/P\\). For general rule read this tutorial section further.</p>"},{"location":"user-guide/6-return-periods/#plotting-positions","title":"Plotting Positions","text":"<p>Plotting positions are sets of empirical coefficients defining how extreme values are assigned probabilities, which are subsequently used to plot extreme values on the probability plots.</p> <p>Warning</p> <p>Plotting positions have nothing to do with modeling extreme event statistics in modern EVA. Historically, in time before computers became widespread, EVA was performed by plotting extreme events on probability paper (with axes scaled logarithmically and according to a specific plotting position) with the idea that a return value curve for a given model (e.g. GEVD) would be a straight line drawn through these points using a pen and a ruler.</p> <p>Modern EVA fits models to data by maximimizng likelihood function via methods such as MLE or MCMC (read more in other sections). This is only feasible due to the use of computers and would be prohibitively expensive to do manually. Plotting positions are presently used only to show extreme values on return value plots and to perform some goodness-of-fit tests (e.g. P-P or Q-Q plots).</p> <p>TL;DR: plotting positions are NOT used to fit models.</p> <p>An example of plotting positions used in <code>pyextremes</code> is the diagnostic plot where observed extreme values (black dots) are superimposed against the theoretical estimates (by fitting a distribution) as seen in the return value, Q-Q, and P-P plots.</p>"},{"location":"user-guide/6-return-periods/#return-period","title":"Return Period","text":"<p>Return periods are calculated from the exceedance probabilities using the following formula:</p> \\[R = 1 / P / \\lambda\\] <p>where:</p> <ul> <li>R - return period as multiple of <code>return_period_size</code> (by default 1 year).</li> <li>P - exceedance probability calculated earlier.</li> <li>\\(\\lambda\\) - rate of extreme events (average number of extreme events per   <code>return_period_size</code>). Calculated as:<ul> <li>\\(\\lambda\\) = <code>return_period_size</code> / <code>block_size</code> for Block Maxima</li> <li>\\(\\lambda = \\frac{n}{t / return\\_period\\_size}\\) for Peaks Over Threshold,   where \\(n\\) is number of extreme events and \\(t\\) is total duration of series   from which the extreme values were drawn</li> </ul> </li> </ul> <p>The resulting return period R is, therefore, a real number representing a multiple of <code>return_period_size</code>.</p> <p>Example</p> <p>We have 2 years of data and, using <code>block_size</code> of 30 days (~1 month), we extract 24 extreme events using the Block Maxima method. We then rank the values from 1 to 24 as outlined above and, using the Weibull plotting position (\\(\\alpha=0\\) and \\(\\beta=0\\)), for the most extreme value (rank 1) we get exceedance probability \\(P\\) of 1/25 or 0.04.</p> <p>Let's say we would like to get return period of the most extreme value (rank 1) in years (<code>return_period_size</code> of 1 year). First, we calculate extreme value rate \\(\\lambda\\) as <code>return_period_size</code> / <code>block_size</code>, which gives us 12 (approximately since we used 30 days for <code>block_size</code>). Now we can use the return period formula above directly as \\(R = 1 / 0.04 / 12 = 2.08\\) years.</p>"},{"location":"user-guide/6-return-periods/#estimating-return-periods","title":"Estimating Return Periods","text":"<p><code>pyextremes</code> estimates empirical return periods for many plotting functions and goodness-of-fit tests behind the scenes using the Weibull plotting position. Return periods can be calculated using the <code>get_return_periods</code> function (shown only for Block Maxima; Peaks Over Threshold works identically with the only difference being the <code>block_size</code> argument):</p> weibull (default)mediancunnanegringorten <pre><code>from pyextremes import get_extremes, get_return_periods\n\nextremes = get_extremes(\n    ts=data,\n    method=\"BM\",\n    block_size=\"365.2425D\",\n)\nreturn_periods = get_return_periods(\n    ts=data,\n    extremes=extremes,\n    extremes_method=\"BM\",\n    extremes_type=\"high\",\n    block_size=\"365.2425D\",\n    return_period_size=\"365.2425D\",\n    plotting_position=\"weibull\",\n)\nreturn_periods.sort_values(\"return period\", ascending=False).head()\n</code></pre> Date-Time (GMT) Water Elevation [m NAVD88] exceedance probability return period 2012-10-30 01:00:00 3.357218 0.010526 95.000000 1960-09-12 18:00:00 2.295832 0.021053 47.500000 1992-12-11 14:00:00 2.108284 0.031579 31.666667 1953-11-07 12:00:00 2.101487 0.042105 23.750000 1950-11-25 14:00:00 2.012957 0.052632 19.000000 <pre><code>from pyextremes import get_extremes, get_return_periods\n\nextremes = get_extremes(\n    ts=data,\n    method=\"BM\",\n    block_size=\"365.2425D\",\n)\nreturn_periods = get_return_periods(\n    ts=data,\n    extremes=extremes,\n    extremes_method=\"BM\",\n    extremes_type=\"high\",\n    block_size=\"365.2425D\",\n    return_period_size=\"365.2425D\",\n    plotting_position=\"median\",\n)\nreturn_periods.sort_values(\"return period\", ascending=False).head()\n</code></pre> Date-Time (GMT) Water Elevation [m NAVD88] exceedance probability return period 2012-10-30 01:00:00 3.357218 0.007233 138.263736 1960-09-12 18:00:00 2.295832 0.017830 56.086181 1992-12-11 14:00:00 2.108284 0.028427 35.178006 1953-11-07 12:00:00 2.101487 0.039024 25.625255 1950-11-25 14:00:00 2.012957 0.049621 20.152696 <pre><code>from pyextremes import get_extremes, get_return_periods\n\nextremes = get_extremes(\n    ts=data,\n    method=\"BM\",\n    block_size=\"365.2425D\",\n)\nreturn_periods = get_return_periods(\n    ts=data,\n    extremes=extremes,\n    extremes_method=\"BM\",\n    extremes_type=\"high\",\n    block_size=\"365.2425D\",\n    return_period_size=\"365.2425D\",\n    plotting_position=\"cunnane\",\n)\nreturn_periods.sort_values(\"return period\", ascending=False).head()\n</code></pre> Date-Time (GMT) Water Elevation [m NAVD88] exceedance probability return period 2012-10-30 01:00:00 3.357218 0.006369 157.000000 1960-09-12 18:00:00 2.295832 0.016985 58.875000 1992-12-11 14:00:00 2.108284 0.027601 36.230769 1953-11-07 12:00:00 2.101487 0.038217 26.166667 1950-11-25 14:00:00 2.012957 0.048832 20.478261 <pre><code>from pyextremes import get_extremes, get_return_periods\n\nextremes = get_extremes(\n    ts=data,\n    method=\"BM\",\n    block_size=\"365.2425D\",\n)\nreturn_periods = get_return_periods(\n    ts=data,\n    extremes=extremes,\n    extremes_method=\"BM\",\n    extremes_type=\"high\",\n    block_size=\"365.2425D\",\n    return_period_size=\"365.2425D\",\n    plotting_position=\"gringorten\",\n)\nreturn_periods.sort_values(\"return period\", ascending=False).head()\n</code></pre> Date-Time (GMT) Water Elevation [m NAVD88] exceedance probability return period 2012-10-30 01:00:00 3.357218 0.005950 168.071429 1960-09-12 18:00:00 2.295832 0.016575 60.333333 1992-12-11 14:00:00 2.108284 0.027199 36.765625 1953-11-07 12:00:00 2.101487 0.037824 26.438202 1950-11-25 14:00:00 2.012957 0.048449 20.640351 <p>The <code>get_return_periods</code> function uses the following parameters:</p> <ul> <li>ts - time series (<code>pandas.Series</code>) from which the extreme values are extracted</li> <li>extremes - time series of extreme values.</li> <li>extremes_method - extreme value extraction method, must be <code>\"BM\"</code> or   <code>\"POT\"</code>.</li> <li>extremes_type - extreme value type:   <code>\"high\"</code> for above threshold (default)   and <code>\"low\"</code> for below threshold.</li> <li>return_period_size - size of return period. Same as the <code>r</code> argument.   By default this is 1 year.</li> <li>plotting_position : plotting position name, case-insensitive.   Supported plotting positions: ecdf, hazen, weibull (default), tukey, blom, median,   cunnane, gringorten, beard.</li> </ul> <p>The following paramters are used only when <code>extremes_method=\"BM\"</code>:</p> <ul> <li>block_size - block size, by default <code>\"365.2425D\"</code>.   Internally is converted using the <code>pandas.to_timedelta</code> function.   If not provided, then it is calculated as median distance between extreme values.</li> </ul> Note <p>You can get the <code>data</code> variable referenced above by running the following code:</p> <pre><code>data = pd.read_csv(\n    \"battery_wl.csv\",\n    index_col=0,\n    parse_dates=True,\n).squeeze()\ndata = (\n    data\n    .sort_index(ascending=True)\n    .astype(float)\n    .dropna()\n    .loc[pd.to_datetime(\"1925\"):]\n)\ndata = (\n  data - (data.index.array - pd.to_datetime(\"1992\"))\n) / pd.to_timedelta(\"365.2425D\") * 2.87e-3\n</code></pre> <p><code>\"battery_wl.csv\"</code> can be downloaded here.</p> <p>All figures shown in this tutorial section were generated using this jupyter notebook.</p>"},{"location":"user-guide/7-models/","title":"7 models","text":"<p>Means MLE and MCMC, not GEVD and GPD.</p>"},{"location":"user-guide/9-mcmc/","title":"9 mcmc","text":"<p>MAP instead of MLE.</p>"},{"location":"user-guide-advanced/1-distributions/","title":"1 distributions","text":"<p><code>Distribution</code> class and how it works.</p>"},{"location":"user-guide-advanced/2-extreme-value-transformation/","title":"2 extreme value transformation","text":"<p>Block minima and peaks-below-threshold.</p>"}]}